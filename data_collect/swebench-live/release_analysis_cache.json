{
  "AUTOMATIC1111/stable-diffusion-webui#v1.10.1": {
    "tag_name": "v1.10.1",
    "repo_name": "AUTOMATIC1111/stable-diffusion-webui",
    "new_features": [],
    "improvements": [],
    "bug_fixes": [
      {
        "feature_type": "bug_fix",
        "description": "fix image upscale on cpu",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16275"
        ]
      }
    ],
    "other_changes": [],
    "processed_body": "## 1.10.1\r\n\r\n### Bug Fixes:\r\n* fix image upscale on cpu ([#16275](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16275))",
    "analyzed_at": "2025-11-05 23:54:23"
  },
  "AUTOMATIC1111/stable-diffusion-webui#v1.10.0": {
    "tag_name": "v1.10.0",
    "repo_name": "AUTOMATIC1111/stable-diffusion-webui",
    "new_features": [
      {
        "feature_type": "new_feature",
        "description": "Stable Diffusion 3 support",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16030",
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16164",
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16212"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "New scheduler: Align Your Steps",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15751"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "New scheduler: KL Optimal",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15608"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "New scheduler: Normal",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16149"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "New scheduler: DDIM",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16149"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "New scheduler: Simple",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16142"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "New scheduler: Beta",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16235"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "New sampler: DDIM CFG++",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16035"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Option to skip CFG on early steps",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15607"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add --models-dir option",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15742"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Allow mobile users to open context menu by using two fingers press",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15682"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Infotext: add Lora name as TI hashes for bundled Textual Inversion",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15679"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "More extension tag filtering options",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15627"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "When saving AVIF, use JPEG's quality setting",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15610"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add filename pattern: [basename]",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15978"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add option to enable clip skip for clip L on SDXL",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15992"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Option to prevent screen sleep during generation",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16001"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "ToggleLivePreview button in image viewer",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16065"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Option to disable save button log.csv",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16242"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add process_before_every_sampling hook",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15984"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Return HTTP 400 instead of 404 on invalid sampler error",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16140"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Added option to run torch profiler for image generation",
        "pr_links": []
      },
      {
        "feature_type": "new_feature",
        "description": "img2img batch upload method",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15817"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add AVIF MIME type support to mimetype definitions",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15739"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "add ids to the resize tabs in img2img",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16218"
        ]
      }
    ],
    "improvements": [
      {
        "feature_type": "improvement",
        "description": "Check model's hash after downloading it to prevent corrupted downloads",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15602"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Remove ui flashing on reloading and fast scrolling",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16153"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Performance: use_checkpoint = False",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15803"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Performance: Replace einops.rearrange with torch native ops",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15804"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Performance: Precompute is_sdxl_inpaint flag",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15806"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Performance: Prevent unnecessary extra networks bias backup",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15816"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Performance: Add --precision half option to avoid casting during inference",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15820"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Performance: LDM optimization patches",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15824"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Performance: Keep sigmas on CPU",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15823"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Check for nans in unet only once, after all steps have been completed",
        "pr_links": []
      },
      {
        "feature_type": "improvement",
        "description": "ReloadUI backgroundColor --background-fill-primary",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15864"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "XYZ override rework",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15836"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "scroll extensions table on overflow",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15830"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Update imageviewer.js",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15730"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "no-referrer",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15641"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "shlex.join launch args in console log",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16170"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Robust sysinfo",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16173"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "do not send image size on paste inpaint",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16180"
        ]
      }
    ],
    "bug_fixes": [
      {
        "feature_type": "bug_fix",
        "description": "Fix for grids without comprehensive infotexts",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15958"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "feat: lora partial update precede full update",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15943"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix bug where file extension had an extra '.' under some circumstances",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15893"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix corrupt model initial load loop",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15600"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Allow old sampler names in API",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15656"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "more old sampler scheduler compatibility",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15681"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Hypertile xyz",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15831"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "XYZ CSV skipinitialspace",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15832"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "fix soft inpainting on mps and xpu, torch_utils.float64",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15815"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "fix extension update when not on main branch",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15797"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "update pickle safe filenames",
        "pr_links": []
      },
      {
        "feature_type": "bug_fix",
        "description": "use relative path for webui-assets css",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15757"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "When creating a virtual environment, upgrade pip in webui.bat/webui.sh",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15750"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix AttributeError",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15738"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "use script_path for webui root in launch_utils",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15705"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "fix extra batch mode P Transparency",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15664"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "use gradio theme colors in css",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15680"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix dragging text within prompt input",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15657"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Add correct mimetype for .mjs files",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15654"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "QOL Items - handle metadata issues more cleanly for SD models, Loras and embeddings",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15632"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "replace wsl-open with wslpath and explorer.exe",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15968"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix SDXL Inpaint",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15976"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "multi size grid",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15988"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "fix Replace preview",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16118"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Possible fix of wrong scale in weight decomposition",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16151"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Ensure use of python from venv on Mac and Linux",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16116"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Prioritize python3.10 over python3 if both are available on Linux and Mac (with fallback)",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16092"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "stoping generation extras",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16085"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix SD2 loading",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16078",
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16079"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "fix infotext Lora hashes for hires fix different lora",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16062"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix sampler scheduler autocorrection warning",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16054"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "fix ui flashing on reloading and fast scrolling",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16153"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "fix upscale logic",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16239"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "[bug] do not break progressbar on non-job actions (add wrap_gradio_call_no_job)",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16202"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "fix OSError: cannot write mode P as JPEG",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16194"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix noisy DS_Store files for MacOS",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16166"
        ]
      }
    ],
    "other_changes": [
      {
        "feature_type": "other",
        "description": "fix changelog #15883 -> #15882",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15907"
        ]
      },
      {
        "feature_type": "other",
        "description": "Use different torch versions for Intel and ARM Macs",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15851"
        ]
      },
      {
        "feature_type": "other",
        "description": "chore: sync v1.8.0 packages according to changelog",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15783"
        ]
      },
      {
        "feature_type": "other",
        "description": ".gitignore trace.json",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15980"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump spandrel to 0.3.4",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16144"
        ]
      },
      {
        "feature_type": "other",
        "description": "Defunct --max-batch-count",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16119"
        ]
      },
      {
        "feature_type": "other",
        "description": "docs: update bug_report.yml",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16102"
        ]
      },
      {
        "feature_type": "other",
        "description": "Maintaining Project Compatibility for Python 3.9 Users Without Upgrade Requirements",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16088",
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16169",
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16192"
        ]
      },
      {
        "feature_type": "other",
        "description": "Update torch for ARM Macs to 2.3.1",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16059"
        ]
      },
      {
        "feature_type": "other",
        "description": "remove deprecated setting dont_fix_second_order_samplers_schedule",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16061"
        ]
      },
      {
        "feature_type": "other",
        "description": "chore: fix typos",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16060"
        ]
      },
      {
        "feature_type": "other",
        "description": "activate venv .bat",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16231"
        ]
      },
      {
        "feature_type": "other",
        "description": "update installation guide linux",
        "pr_links": [
          "https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16178"
        ]
      }
    ],
    "processed_body": "### Features:\r\n* A lot of performance improvements (see below in Performance section)\r\n* Stable Diffusion 3 support ([#16030](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16030), [#16164](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16164), [#16212](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16212))\r\n  * Recommended Euler sampler; DDIM and other timestamp samplers currently not supported\r\n  * T5 text model is disabled by default, enable it in settings\r\n* New schedulers:\r\n  * Align Your Steps ([#15751](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15751))\r\n  * KL Optimal ([#15608](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15608))\r\n  * Normal ([#16149](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16149))\r\n  * DDIM ([#16149](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16149))\r\n  * Simple ([#16142](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16142))\r\n  * Beta ([#16235](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16235))\r\n* New sampler: DDIM CFG++ ([#16035](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16035))\r\n\r\n### Minor:\r\n* Option to skip CFG on early steps ([#15607](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15607))\r\n* Add --models-dir option ([#15742](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15742))\r\n* Allow mobile users to open context menu by using two fingers press ([#15682](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15682))\r\n* Infotext: add Lora name as TI hashes for bundled Textual Inversion ([#15679](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15679))\r\n* Check model's hash after downloading it to prevent corruped downloads ([#15602](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15602))\r\n* More extension tag filtering options ([#15627](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15627))\r\n* When saving AVIF, use JPEG's quality setting ([#15610](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15610))\r\n* Add filename pattern: `[basename]` ([#15978](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15978))\r\n* Add option to enable clip skip for clip L on SDXL ([#15992](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15992))\r\n* Option to prevent screen sleep during generation ([#16001](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16001))\r\n* ToggleLivePriview button in image viewer ([#16065](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16065))\r\n* Remove ui flashing on reloading and fast scrollong ([#16153](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16153))\r\n* option to disable save button log.csv ([#16242](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16242))\r\n\r\n### Extensions and API:\r\n* Add process_before_every_sampling hook ([#15984](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15984))\r\n* Return HTTP 400 instead of 404 on invalid sampler error ([#16140](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16140))\r\n\r\n### Performance:\r\n* [Performance 1/6] use_checkpoint = False ([#15803](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15803))\r\n* [Performance 2/6] Replace einops.rearrange with torch native ops ([#15804](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15804))\r\n* [Performance 4/6] Precompute is_sdxl_inpaint flag ([#15806](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15806))\r\n* [Performance 5/6] Prevent unnecessary extra networks bias backup ([#15816](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15816))\r\n* [Performance 6/6] Add --precision half option to avoid casting during inference ([#15820](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15820))\r\n* [Performance] LDM optimization patches ([#15824](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15824))\r\n* [Performance] Keep sigmas on CPU ([#15823](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15823))\r\n* Check for nans in unet only once, after all steps have been completed\r\n* Added pption to run torch profiler for image generation\r\n\r\n### Bug Fixes:\r\n* Fix for grids without comprehensive infotexts ([#15958](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15958))\r\n* feat: lora partial update precede full update ([#15943](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15943))\r\n* Fix bug where file extension had an extra '.' under some circumstances ([#15893](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15893))\r\n* Fix corrupt model initial load loop ([#15600](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15600))\r\n* Allow old sampler names in API ([#15656](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15656))\r\n* more old sampler scheduler compatibility ([#15681](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15681))\r\n* Fix Hypertile xyz ([#15831](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15831))\r\n* XYZ CSV skipinitialspace ([#15832](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15832))\r\n* fix soft inpainting on mps and xpu, torch_utils.float64 ([#15815](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15815))\r\n* fix extention update when not on main branch ([#15797](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15797))\r\n* update pickle safe filenames\r\n* use relative path for webui-assets css ([#15757](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15757))\r\n* When creating a virtual environment, upgrade pip in webui.bat/webui.sh ([#15750](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15750))\r\n* Fix AttributeError ([#15738](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15738))\r\n* use script_path for webui root in launch_utils ([#15705](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15705))\r\n* fix extra batch mode P Transparency ([#15664](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15664))\r\n* use gradio theme colors in css ([#15680](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15680))\r\n* Fix dragging text within prompt input ([#15657](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15657))\r\n* Add correct mimetype for .mjs files ([#15654](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15654))\r\n* QOL Items - handle metadata issues more cleanly for SD models, Loras and embeddings ([#15632](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15632))\r\n* replace wsl-open with wslpath and explorer.exe ([#15968](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15968))\r\n* Fix SDXL Inpaint ([#15976](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15976))\r\n* multi size grid ([#15988](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15988))\r\n* fix Replace preview ([#16118](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16118))\r\n* Possible fix of wrong scale in weight decomposition ([#16151](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16151))\r\n* Ensure use of python from venv on Mac and Linux ([#16116](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16116))\r\n* Prioritize python3.10 over python3 if both are available on Linux and Mac (with fallback) ([#16092](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16092))\r\n* stoping generation extras ([#16085](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16085))\r\n* Fix SD2 loading ([#16078](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16078), [#16079](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16079))\r\n* fix infotext Lora hashes for hires fix different lora ([#16062](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16062))\r\n* Fix sampler scheduler autocorrection warning ([#16054](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16054))\r\n* fix ui flashing on reloading and fast scrollong ([#16153](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16153))\r\n* fix upscale logic ([#16239](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16239))\r\n* [bug] do not break progressbar on non-job actions (add wrap_gradio_call_no_job) ([#16202](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16202))\r\n* fix OSError: cannot write mode P as JPEG ([#16194](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16194))\r\n\r\n### Other:\r\n* fix changelog #15883 -> #15882 ([#15907](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15907))\r\n* ReloadUI backgroundColor --background-fill-primary ([#15864](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15864))\r\n* Use different torch versions for Intel and ARM Macs ([#15851](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15851))\r\n* XYZ override rework ([#15836](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15836))\r\n* scroll extensions table on overflow ([#15830](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15830))\r\n* img2img batch upload method ([#15817](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15817))\r\n* chore: sync v1.8.0 packages according to changelog ([#15783](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15783))\r\n* Add AVIF MIME type support to mimetype definitions ([#15739](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15739))\r\n* Update imageviewer.js ([#15730](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15730))\r\n* no-referrer ([#15641](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15641))\r\n* .gitignore trace.json ([#15980](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15980))\r\n* Bump spandrel to 0.3.4 ([#16144](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16144))\r\n* Defunct --max-batch-count ([#16119](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16119))\r\n* docs: update bug_report.yml ([#16102](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16102))\r\n* Maintaining Project Compatibility for Python 3.9 Users Without Upgrade Requirements. ([#16088](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16088), [#16169](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16169), [#16192](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16192))\r\n* Update torch for ARM Macs to 2.3.1 ([#16059](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16059))\r\n* remove deprecated setting dont_fix_second_order_samplers_schedule ([#16061](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16061))\r\n* chore: fix typos ([#16060](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16060))\r\n* shlex.join launch args in console log ([#16170](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16170))\r\n* activate venv .bat ([#16231](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16231))\r\n* add ids to the resize tabs in img2img ([#16218](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16218))\r\n* update installation guide linux ([#16178](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16178))\r\n* Robust sysinfo ([#16173](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16173))\r\n* do not send image size on paste inpaint ([#16180](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16180))\r\n* Fix noisy DS_Store files for MacOS ([#16166](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16166))\r\n",
    "analyzed_at": "2025-11-05 23:55:48"
  },
  "huggingface/transformers#v4.57.1": {
    "tag_name": "v4.57.1",
    "repo_name": "huggingface/transformers",
    "new_features": [],
    "improvements": [],
    "bug_fixes": [
      {
        "feature_type": "bug_fix",
        "description": "Fix an issue with an optional dependency (optax) that resulted in parsing errors with poetry",
        "pr_links": []
      },
      {
        "feature_type": "bug_fix",
        "description": "Remove offload_state_dict from kwargs",
        "pr_links": []
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix bnb fsdp loading for pre-quantized checkpoint",
        "pr_links": [
          "https://github.com/huggingface/transformers/pull/41415"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix tests fsdp",
        "pr_links": [
          "https://github.com/huggingface/transformers/pull/41422"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix trainer for py3.9",
        "pr_links": [
          "https://github.com/huggingface/transformers/pull/41359"
        ]
      }
    ],
    "other_changes": [],
    "processed_body": "This patch most notably fixes an issue with an optional dependency (`optax`), which resulted in parsing errors with `poetry`. It contains the following fixes:\r\n\r\n- [fix optax dep issue](https://github.com/huggingface/transformers/commit/0645c9ec3188e000aecf5060e2cdabcc156bb794)\r\n- [remove offload_state_dict from kwargs](https://github.com/huggingface/transformers/commit/a92b1e8a45e1863b95c5e2caa12f5597aee80279)\r\n- Fix bnb fsdp loading for pre-quantized checkpoint (#41415)\r\n- Fix tests fsdp (#41422)\r\n- Fix trainer for py3.9 (#41359)",
    "analyzed_at": "2025-11-05 23:55:55"
  },
  "huggingface/transformers#v4.57.0": {
    "tag_name": "v4.57.0",
    "repo_name": "huggingface/transformers",
    "new_features": [],
    "improvements": [],
    "bug_fixes": [],
    "other_changes": [],
    "processed_body": "## New model additions\r\n\r\n### Qwen3 Next\r\n\r\n<img width=\"1200\" height=\"511\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3abad6c4-5650-412d-a831-f8a30a5d962e\" />\r\n\r\nThe Qwen3-Next series represents the Qwen team's next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency. \r\nThe series introduces a suite of architectural innovations designed to maximize performance while minimizing computational cost:\r\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling.  \r\n- **High-Sparsity MoE**: Achieves an extreme low activation ratio as 1:50 in MoE layers — drastically reducing FLOPs per token while preserving model capacity.\r\n- **Multi-Token Prediction(MTP)**: Boosts pretraining model performance, and accelerates inference.\r\n- **Other Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, **Gated Attention**, and other stabilizing enhancements for robust training.  \r\n\r\nBuilt on this architecture, they trained and open-sourced Qwen3-Next-80B-A3B — 80B total parameters, only 3B active — achieving extreme sparsity and efficiency.\r\n\r\nDespite its ultra-efficiency, it outperforms Qwen3-32B on downstream tasks — while requiring **less than 1/10 of the training cost**. \r\nMoreover, it delivers over **10x higher inference throughput** than Qwen3-32B when handling contexts longer than 32K tokens.\r\n\r\nFor more details, please visit their blog [Qwen3-Next](qwen3_next) ([blog post](https://qwenlm.github.io/blog/qwen3_next/)).\r\n\r\n* Adding Support for Qwen3-Next  by @bozheng-hit in #40771\r\n\r\n### Vault Gemma\r\n\r\n<img width=\"1282\" height=\"392\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9412905b-4083-4994-9000-aa0dbf97eb6f\" />\r\n\r\n[VaultGemma](https://services.google.com/fh/files/blogs/vaultgemma_tech_report.pdf) is a text-only decoder model derived from [Gemma 2](https://huggingface.co/docs/transformers/en/model_doc/gemma2), notably it drops the norms after the Attention and MLP blocks, and uses full attention for all layers instead of alternating between full attention and local sliding attention. VaultGemma is available as a pretrained model with 1B parameters that uses a 1024 token sequence length.\r\n\r\nVaultGemma was trained from scratch with sequence-level differential privacy (DP). Its training data includes the same mixture as the [Gemma 2 models](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315), consisting of a number of documents of varying lengths. Additionally, it is trained using [DP stochastic gradient descent (DP-SGD)](https://arxiv.org/abs/1607.00133) and provides a (ε ≤ 2.0, δ ≤ 1.1e-10)-sequence-level DP guarantee, where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, the privacy unit of the guarantee is for the sequences after sampling and packing of the mixture.\r\n\r\n* add: differential privacy research model  by @RyanMullins in #40851\r\n\r\n### Qwen3 VL\r\n\r\n<img width=\"3544\" height=\"1886\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5afa70cb-506e-4d56-baa3-30e7522ac653\" />\r\n\r\n[Qwen3-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model series, encompassing both dense and MoE variants, as well as Instruct and Thinking versions. \r\n\r\nBuilding upon its predecessors, Qwen3-VL delivers significant improvements in visual understanding while maintaining strong pure text capabilities. Key architectural advancements include: enhanced MRope with interleaved layout for better spatial-temporal modeling, DeepStack integration to effectively leverage multi-level features from the Vision Transformer (ViT), and improved video understanding through text-based time alignment—evolving from T-RoPE to text timestamp alignment for more precise temporal grounding. \r\n\r\nThese innovations collectively enable Qwen3-VL to achieve superior performance in complex multimodal tasks.\r\n\r\n* Adding Support for Qwen3-VL Series  by @JJJYmmm in #40795\r\n\r\n### Longcat Flash\r\n\r\n<img width=\"763\" height=\"468\" alt=\"image\" src=\"https://github.com/user-attachments/assets/289d33e0-6c71-458d-ae07-b7d454ac2adf\" />\r\n\r\nThe LongCatFlash model was proposed in [LongCat-Flash Technical Report](https://huggingface.co/papers/2509.01322) by the Meituan LongCat Team. LongCat-Flash is a 560B parameter Mixture-of-Experts (MoE) model that activates 18.6B-31.3B parameters dynamically (average ~27B). The model features a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and advanced reasoning capabilities.\r\n\r\nThe abstract from the paper is the following:\r\n\r\n*We present LongCat-Flash, a 560 billion parameter Mixture-of-Experts (MoE) language model featuring a dynamic computation mechanism that activates 18.6B-31.3B parameters based on context (average ~27B). The model incorporates a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and demonstrates strong performance across multiple benchmarks including 89.71% accuracy on MMLU and exceptional agentic tool use capabilities.*\r\n\r\nTips:\r\n\r\n- LongCat-Flash uses a unique shortcut-connected MoE architecture that enables faster inference compared to traditional MoE models\r\n- The model supports up to 128k context length for long-form tasks\r\n- Dynamic parameter activation makes it computationally efficient while maintaining high performance\r\n- Best suited for applications requiring strong reasoning, coding, and tool-calling capabilities\r\n- The MoE architecture includes zero experts (nn.Identity modules) which act as skip connections, allowing tokens to bypass expert computation when appropriate\r\n\r\n* Add LongCat-Flash  by @molbap in #40730\r\n\r\n### Flex Olmo\r\n\r\n<img width=\"700\" height=\"414\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7b92ee0f-5f5a-459c-ad4d-e01b5c10202e\" />\r\n\r\n[FlexOlmo](https://huggingface.co/papers/2507.07024) is a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. \r\n\r\nYou can find all the original FlexOlmo checkpoints under the [FlexOlmo](https://huggingface.co/collections/allenai/flexolmo-68471177a386b6e20a54c55f) collection.\r\n\r\n* Add FlexOlmo model  by @2015aroras in #40921\r\n\r\n### LFM2 VL\r\n\r\n<img width=\"2300\" height=\"1400\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ef0605cd-9512-458c-915a-62316e14d90c\" />\r\n\r\n[LFM2-VL](https://www.liquid.ai/blog/lfm2-vl-efficient-vision-language-models) first series of vision-language foundation models developed by [Liquid AI](https://liquid.ai/). These multimodal models are designed for low-latency and device-aware deployment. LFM2-VL extends the LFM2 family of open-weight Liquid Foundation Models (LFMs) into the vision-language space, supporting both text and image inputs with variable resolutions.\r\n\r\n#### Architecture\r\n\r\nLFM2-VL consists of three main components: a language model backbone, a vision encoder, and a multimodal projector. LFM2-VL builds upon the LFM2 backbone, inheriting from either LFM2-1.2B (for LFM2-VL-1.6B) or LFM2-350M (for LFM2-VL-450M). For the vision tower, LFM2-VL uses SigLIP2 NaFlex encoders to convert input images into token sequences. Two variants are implemented:\r\n* Shape-optimized (400M) for more fine-grained vision capabilities for LFM2-VL-1.6B\r\n* Base (86M) for fast image processing for LFM2-VL-450M\r\n\r\nThe encoder processes images at their native resolution up to 512×512 pixels, efficiently handling smaller images without upscaling and supporting non-standard aspect ratios without distortion. Larger images are split into non-overlapping square patches of 512×512 each, preserving detail. In LFM2-VL-1.6B, the model also receives a thumbnail (a small, downscaled version of the original image capturing the overall scene) to enhance global context understanding and alignment. Special tokens mark each patch’s position and indicate the thumbnail’s start. The multimodal connector is a 2-layer MLP connector with pixel unshuffle to reduce image token count. \r\n\r\n* Add new model LFM2-VL  by @zucchini-nlp in #40624\r\n\r\n### BLT\r\n\r\n<img width=\"1448\" height=\"1062\" alt=\"image\" src=\"https://github.com/user-attachments/assets/af1fbb09-082c-4331-9217-357adb506cbf\" />\r\n\r\nThe BLT model was proposed in [Byte Latent Transformer: Patches Scale Better Than Tokens](<https://arxiv.org/pdf/2412.09871>) by Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li1, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman†, Srinivasan Iyer.\r\nBLT is a byte-level LLM that achieves tokenization-level performance through entropy-based dynamic patching.\r\n\r\nThe abstract from the paper is the following:\r\n\r\n*We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference\r\nefficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating\r\nmore compute and model capacity where increased data complexity demands it. We present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.*\r\n\r\n#### Usage Tips:\r\n\r\n- **Dual Model Architecture**: BLT consists of two separate trained models:\r\n  - **Patcher (Entropy Model)**: A smaller transformer model that predicts byte-level entropy to determine patch boundaries and segment input.\r\n  - **Main Transformer Model**: The primary model that processes the patches through a Local Encoder, Global Transformer, and Local Decoder.\r\n\r\n- **Dynamic Patching**: The model uses entropy-based dynamic patching where:\r\n  - High-entropy regions (complex data) get shorter patches with more computational attention\r\n  - Low-entropy regions (predictable data) get longer patches for efficiency\r\n  - This allows the model to allocate compute resources where they're most needed\r\n\r\n- **Local Encoder**: Processes byte sequences with cross-attention to patch embeddings\r\n- **Global Transformer**: Processes patch-level representations with full attention across patches\r\n- **Local Decoder**: Generates output with cross-attention back to the original byte sequence\r\n\r\n- **Byte-Level Tokenizer**: Unlike traditional tokenizers that use learned vocabularies, BLT's tokenizer simply converts text to UTF-8 bytes and maps each byte to a token ID. There is no need for a vocabulary.\r\n\r\n* blt wip  by @itazap in #38579\r\n\r\n### Qwen3 Omni MoE\r\n\r\n<img width=\"14084\" height=\"7429\" alt=\"image\" src=\"https://github.com/user-attachments/assets/20d46a43-15f2-42bf-9703-9575f5ca4430\" />\r\n\r\nThe [Qwen2.5-Omni](https://qwenlm.github.io/blog/qwen2.5-omni/) model is a unified multiple modalities model proposed in [Qwen2.5-Omni Technical Report](https://huggingface.co/papers/2503.20215) from Qwen team, Alibaba Group.\r\n\r\n#### Notes\r\n\r\n- Use [`Qwen2_5OmniForConditionalGeneration`] to generate audio and text output. To generate only one output type, use [`Qwen2_5OmniThinkerForConditionalGeneration`] for text-only and [`Qwen2_5OmniTalkersForConditionalGeneration`] for audio-only outputs.\r\n- Audio generation with [`Qwen2_5OmniForConditionalGeneration`] supports only single batch size at the moment.\r\n- In case out out-of-memory errors hwen working with video input, decrease `processor.max_pixels`. By default the maximum is set to a very arge value and high resolution visuals will not be resized, unless resolution exceeds `processor.max_pixels`.\r\n- The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\r\n\r\n* Adding support for Qwen3Omni  by @BakerBunker in #41025\r\n\r\n### Parakeet\r\n\r\n<img width=\"1431\" height=\"527\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e831f451-9be3-4b5c-a222-b833a50ceb2a\" />\r\n\r\nParakeet models, [introduced by NVIDIA NeMo](https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/), are models that combine a [Fast Conformer](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html#fast-conformer) encoder with connectionist temporal classification (CTC), recurrent neural network transducer (RNNT) or token and duration transducer (TDT) decoder for automatic speech recognition.\r\n\r\n**Model Architecture**\r\n- **Fast Conformer Encoder**: A linearly scalable Conformer architecture that processes mel-spectrogram features and reduces sequence length through subsampling. This is more efficient version of the Conformer Encoder found in [FastSpeech2Conformer](./fastspeech2_conformer.md) (see [`ParakeetEncoder`] for the encoder implementation and details).\r\n- [**ParakeetForCTC**](#parakeetforctc): a Fast Conformer Encoder + a CTC decoder\r\n    - **CTC Decoder**: Simple but effective decoder consisting of:\r\n        - 1D convolution projection from encoder hidden size to vocabulary size (for optimal NeMo compatibility).\r\n        - CTC loss computation for training.\r\n        - Greedy CTC decoding for inference.\r\n\r\n* Add Parakeet  by @nithinraok in #39062\r\n\r\n### EdgeTAM\r\n\r\n<img width=\"949\" height=\"537\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5ca4e73d-5aa9-487d-96e1-92d4f2f4739f\" />\r\n\r\nThe EdgeTAM model was proposed in [EdgeTAM: On-Device Track Anything Model](https://huggingface.co/papers/2501.07256) Chong Zhou, Chenchen Zhu, Yunyang Xiong, Saksham Suri, Fanyi Xiao, Lemeng Wu, Raghuraman Krishnamoorthi, Bo Dai, Chen Change Loy, Vikas Chandra, Bilge Soran.\r\n\r\nEdgeTAM is an efficient adaptation of SAM 2 that introduces a 2D Spatial Perceiver architecture to optimize memory attention mechanisms for real-time video segmentation on mobile devices.\r\n\r\n* Add EdgeTAM  by @yonigozlan in #39800\r\n\r\n### OLMO3\r\n\r\nMore details to come soon :eyes:\r\n\r\n* Add Olmo3 model  by @2015aroras in #40778\r\n\r\n## Continuous batching\r\n\r\nWe are introducing Continuous Batching (CB) in this release, we consider it a stable feature. The main use case for CB is batched generation, which makes it very efficient in the context of GRPO training or evaluation. Thanks to CB, researchers or model developers are now free to use transformers in these contexts without having to spin up an additional inference engine. \r\n\r\nCB currently supports both full attention and sliding window attention: this means that the vast majority of models are supported, like llama, gemma3, gpt-oss. \r\n\r\nCB is also integrated with transformers serve, which means that you can deploy transformers as an OpenAI-compatible HTTP server.\r\nHere is a small snippet on how to use it:\r\n\r\n```python\r\nimport datasets\r\nimport torch\r\n\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nfrom transformers.generation import GenerationConfig\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"Qwen/Qwen3-4B-Instruct-2507\", dtype=torch.bfloat16, _attn_implementation=\"sdpa_paged\", device_map=\"auto\"\r\n)\r\nmodel.generation_config.max_new_tokens = 32\r\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", padding_side=\"left\")\r\ndataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\r\ntokenized_datasets = dataset.map(lambda x: tokenizer(x[\"question\"]), batched=True)\r\nsimple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\r\n\r\nbatch_outputs = model.generate_batch(inputs=simple_batch_inputs)\r\nfor request in batch_outputs:\r\n    print(tokenizer.decode(batch_outputs[request].generated_tokens))\r\n\"\"\"\r\n Let's break down the problem step by step:\r\n\r\n1. **Total eggs laid per day**:  \r\n   Janet’s ducks lay **16 eggs per day**\r\n Let's break down the problem step by step:\r\n\r\n1. **Blue fiber**: The robe takes **2 bolts** of blue fiber.\r\n2. **White fiber\r\n To determine Josh's profit from flipping the house, let's go step by step.\r\n\r\n---\r\n\r\n### Step 1: Initial cost of the house\r\nJosh buys the\r\n To find the total distance James runs in a week, we can break down the problem step by step:\r\n\r\n1. **Sprints per session**: James runs \r\n To determine how many cups of feed Wendi needs to give her chickens in the final meal of the day, let's go step by step.\r\n\"\"\"\r\n```\r\n\r\n## Breaking changes\r\n\r\n* 🚨 Remove Group Beam Search decoding strategy  by @manueldeprada in #40495\r\n* 🚨 Remove Constrained Beam Search decoding strategy  by @manueldeprada in #40518\r\n* 🚨 Allow `check_model_inputs` in core VLMs  by @zucchini-nlp in #40342\r\n* 🔴 Update Glm4V to use config values  by @zucchini-nlp in #40712\r\n* 🚨 Fix Inconsistant `input_feature` length and `attention_mask` length in `WhisperFeatureExtractor`  by @BakerBunker in #39221\r\n* ⚠️ 🔴 Add ministral model  by @manueldeprada in #40247\r\n* 🔴 Move variable output controls to `_prepare_generation_config `  by @manueldeprada in #40715\r\n* 🔴Make `center_crop` fast equivalent to slow  by @yonigozlan in #40856\r\n\r\n## Bugfixes and improvements\r\n\r\n* Fix collated reports upload filename  by @ivarflakstad in #40556\r\n* pin `pytest-rerunfailures<16.0`  by @ydshieh in #40561\r\n* remove the redundant non maintained jieba and use rjieba instead  by @divyanshsinghvi in #40383\r\n* Set `test_all_params_have_gradient=False` for `DeepseekV2ModelTest`  by @ydshieh in #40566\r\n* processor tests - use dummy videos  by @zucchini-nlp in #40537\r\n* [qwen-vl] fix position ids  by @zucchini-nlp in #40490\r\n* Fix `test_eager_matches_sdpa_inference` not run for `CLIP`  by @ydshieh in #40581\r\n* Fix CircleCI step passes in the case of pytest worker crash at test collection time  by @ydshieh in #40552\r\n* Allow `remi-or` to `run-slow`  by @ydshieh in #40590\r\n* Fix llava image processor  by @zucchini-nlp in #40588\r\n* Update `get_*_features` methods + update doc snippets  by @qubvel in #40555\r\n* Fix custom generate relative imports  by @manueldeprada in #40480\r\n* Support batch size > 1 image-text inference  by @hiyouga in #36682\r\n* Fix typos  by @cyyever in #40585\r\n* Skip `TvpImageProcessingTest::test_slow_fast_equivalence`  by @ydshieh in #40593\r\n* Fix inexistent imports  by @cyyever in #40580\r\n* Add Copilot instructions  by @Rocketknight1 in #40432\r\n* Fix `siglip` flaky `test_eager_matches_sdpa_inference`  by @ydshieh in #40584\r\n* Fix for missing default values in encoder decoder   by @remi-or in #40517\r\n* Fix quite a lot of FA tests  by @Cyrilvallez in #40548\r\n* [`Tests`] Fixup duplicated mrope logic  by @vasqu in #40592\r\n* Reduce more test data fetch  by @ydshieh in #40595\r\n* Pin torchcodec to 0.5 in AMD docker  by @remi-or in #40598\r\n* Multiple fixes to FA tests in AMD  by @remi-or in #40498\r\n* Disable cache for `TokenizerTesterMixin` temporarily  by @ydshieh in #40611\r\n* fix: continuous batching in `transformers serve`  by @McPatate in #40479\r\n* Fix processor chat template  by @zucchini-nlp in #40613\r\n* Avoid `too many request` caused by `AutoModelTest::test_dynamic_saving_from_local_repo`  by @ydshieh in #40614\r\n* Fix flaky `JambaModelTest.test_load_balancing_loss`  by @ydshieh in #40617\r\n* Add collated reports job to Nvidia CI  by @ahadnagy in #40470\r\n* Remove unnecessary pillow version check  by @cyyever in #40604\r\n* Fix invalid typing  by @cyyever in #40612\r\n* Enable more ruff UP rules  by @cyyever in #40579\r\n* Support TF32 flag for MUSA backend  by @fmo-mt in #33187\r\n* Remove random flag  by @Cyrilvallez in #40629\r\n* 🌐 [i18n-KO] Translated `deepseek_v3.md` to Korean   by @ssum21 in #39649\r\n* Fix `too many requests` in `TestMistralCommonTokenizer`  by @ydshieh in #40623\r\n* fix: gas for gemma fixed  by @yevvonlim in #40591\r\n* [auto-model] propagate kwargs  by @zucchini-nlp in #40491\r\n* [CP] Add attention_mask to the buffer when the mask is causal   by @kashif in #40619\r\n* Fix: PIL image load in Processing utils apply_chat_template  by @abdokaseb in #40622\r\n* Skip `test_prompt_lookup_decoding_matches_greedy_search` for `voxtral`  by @ydshieh in #40643\r\n* add DeepseekV3ForTokenClassification  by @bzantium in #40641\r\n* fix MetaCLIP 2 wrong link & wrong model names in the docstrings  by @voidism in #40565\r\n* Remove TF/Flax examples  by @Rocketknight1 in #40654\r\n* Mark `LongformerModelTest::test_attention_outputs` as flaky  by @ydshieh in #40655\r\n* fix pipeline dtype  by @jiqing-feng in #40638\r\n* feat(serving): add healthcheck  by @McPatate in #40653\r\n* Fix Metaclip modular conversion  by @Rocketknight1 in #40660\r\n* Avoid attention_mask copy in qwen2.5  by @cyyever in #40658\r\n* Allow custom args in `custom_generate` Callables and unify generation args structure  by @manueldeprada in #40586\r\n* Update `check_determinism` inside `test_determinism`  by @ydshieh in #40661\r\n* Skip `test_fast_is_faster_than_slow` for `Owlv2ImageProcessingTest`  by @ydshieh in #40663\r\n* Fix warning for output_attentions=True  by @qubvel in #40597\r\n* Skip `test_prompt_lookup_decoding_matches_greedy_search` for `qwen2_audio`  by @ydshieh in #40664\r\n* Remove overwritten `GitModelTest::test_beam_search_generate`  by @ydshieh in #40666\r\n* refactor: use `tolist` instead of list comprehension calling `.item()`  by @McPatate in #40646\r\n* Benchmarking V2: framework impl  by @ahadnagy in #40486\r\n* Even more test data cached  by @ydshieh in #40636\r\n* Skip more fast v.s slow image processor tests  by @ydshieh in #40675\r\n* Avoid night torch CI not run because of irrelevant docker image failing to build   by @ydshieh in #40677\r\n* Mark `Aimv2ModelTest::test_eager_matches_sdpa_inference_04_fp16_pad_right_sdpa_kernels` as flaky  by @ydshieh in #40683\r\n* CircleCI docker images cleanup / update / fix  by @ydshieh in #40681\r\n* Add sequence classification support for small Gemma 3 text models  by @abdokaseb in #40562\r\n* Add codebook_dim attribute to DacVectorQuantize for DacResidualVectorQuantize.from_latents()  by @flavioialongo in #40665\r\n* fix broken offline mode when loading tokenizer from hub  by @winglian in #40669\r\n* Load a tiny video to make CI faster  by @zucchini-nlp in #40684\r\n* Final test data cache - inside CI docker images  by @ydshieh in #40689\r\n* add: embedding model  by @RyanMullins in #40694\r\n* feat: support request cancellation  by @McPatate in #40599\r\n* Fixing bug in Voxtral when merging text and audio embeddings  by @rcogill in #40671\r\n* Change docker image to preview for the MI355 CI  by @ahadnagy in #40693\r\n* Fix backward compatibility with accelerate in Trainer  by @qgallouedec in #40668\r\n* Fix self.dropout_p is not defined for SamAttention/Sam2Attention  by @yonigozlan in #40667\r\n* [Glm4.5V] fix vLLM support  by @zucchini-nlp in #40696\r\n* Fix broken Llama4 accuracy in MoE part  by @nvpohanh in #40609\r\n* Avoid `T5GemmaModelTest::test_eager_matches_sdpa_inference` being flaky  by @ydshieh in #40702\r\n* Align assisted generate for unified signature in decoding methods  by @manueldeprada in #40657\r\n* Fetch one missing test data  by @ydshieh in #40703\r\n* Add Fast Image Processor for ImageGPT  by @agamjots05 in #39592\r\n* Fetch more test data with `hf_hub_download`  by @ydshieh in #40710\r\n* feat(serve): add healthcheck test  by @McPatate in #40697\r\n* Fix parent classes of ProcessingKwargs  by @cyyever in #40676\r\n* [tests] fix blip2 edge case  by @gante in #40699\r\n* [moduar] Add missing `self` in post-process methods  by @framonmar7 in #40711\r\n* [onnx] use logical `or` for grounding dino mask  by @lmarshall12 in #40625\r\n* Fix parent classes of AllKwargsForChatTemplate  by @cyyever in #40685\r\n* Fix arguments  by @cyyever in #40605\r\n* [serve] re-enable tests  by @gante in #40717\r\n* [tests] remove overwrites of removed test  by @gante in #40720\r\n* Add Optional typing  by @cyyever in #40686\r\n* [`Gemma Embedding`] Fix SWA  by @vasqu in #40700\r\n* Keypoint matching docs  by @merveenoyan in #40541\r\n* Skip `VitMatteImageProcessingTest::test_fast_is_faster_than_slow`  by @ydshieh in #40713\r\n* refactor(serve): move `request_id` to headers  by @McPatate in #40722\r\n* [Continous Batching] fix do_Sample=True in continuous batching  by @kashif in #40692\r\n* Fix order of mask functions when using `and/or_mask_function`  by @Cyrilvallez in #40753\r\n* Fix np array typing  by @cyyever in #40741\r\n* Set accepts_loss_kwargs to False for ConvNext(|V2)ForImageClassification  by @clinty in #40746\r\n* Add BF16 support check for MUSA backend  by @fmo-mt in #40576\r\n* remove gemmas eager training warning  by @August-murr in #40744\r\n* remove FSDP prefix when using save_pretrained with FSDP2  by @winglian in #40207\r\n* feat: err when unsupported attn impl is set w/ `--continuous_batching`  by @McPatate in #40618\r\n* docs: add continuous batching to serving  by @McPatate in #40758\r\n* Remove unnecessary tildes from documentation  by @st81 in #40748\r\n* Fix more typos  by @cyyever in #40627\r\n* Fix inconsistency in SeamlessM4T and SeamlessM4Tv2 docs  by @clinty in #39364\r\n* Fix `continue_final_message` in `apply_chat_template` to prevent substring matching issues  by @abdokaseb in #40732\r\n* 🌐 [i18n-KO] Translated 'xclip.md' to Korean  by @ssum21 in #39594\r\n* Fix Bark failing tests  by @ebezzam in #39478\r\n* Add EfficientLoFTRImageProcessorFast for GPU-accelerated image processing  by @LawJarp-A in #40215\r\n* Fix: swanlab `public.cloud.experiment_url` api error  by @Zeyi-Lin in #40763\r\n* [generate] `PromptLookupCandidateGenerator` won't generate forbidden tokens  by @gante in #40726\r\n* Support sliding window in CB  by @remi-or in #40688\r\n* [deprecations] Remove generate-related deprecations up to v4.56  by @gante in #40729\r\n* rm src/transformers/convert_pytorch_checkpoint_to_tf2.py  by @gante in #40718\r\n* [tests] update `test_past_key_values_format` and delete overwrites  by @gante in #40701\r\n* [RoPE] run RoPE tests when the model uses RoPE  by @gante in #40630\r\n* Fix crash when executing MambaCache sample code  by @torotoki in #40557\r\n* [pipeline] ASR pipeline kwargs are forwared to `generate`  by @gante in #40375\r\n* [docs] CPU install  by @stevhliu in #40631\r\n* Adding Support for Qwen3-Next  by @bozheng-hit in #40771\r\n* Fix gpt-oss router_indices in EP  by @jiqing-feng in #40545\r\n* Remove reference of video_load_backend and video_fps for processor  by @cyyever in #40719\r\n* [processors] Unbloating simple processors  by @zucchini-nlp in #40377\r\n* Enable ruff on benchmark and scripts  by @cyyever in #40634\r\n* Fix doc for PerceptionLMForConditionalGeneration forward.  by @shuminghu in #40733\r\n* Fix typos in tests and util  by @cyyever in #40780\r\n* Fix invalid PipelineParallel member  by @cyyever in #40789\r\n* Use functools.cached_property  by @cyyever in #40607\r\n* Read config pattern for Qwen3Next  by @Cyrilvallez in #40792\r\n* Fix dotted model names  by @August-murr in #40745\r\n* Fix the issue that csm model cannot work with pipeline mode.  by @yuanwu2017 in #39349\r\n* Move num_items_in_batch to correct device before accelerator.gather  by @ssharpe42 in #40773\r\n* Remove use_ipex option from Trainer  by @cyyever in #40784\r\n* fix_image_processing_fast_for_glm4v  by @lambertwjh in #40483\r\n* [Docs] Add missing class documentation for optimizer_schedules  by @jijihuny in #31870,  #23010) \r\n* Fix DeepSpeed mixed precision precedence over Accelerate defaults  by @notkisk in #39856\r\n* feature: Add robust token counting with padding exclusion   by @PrathmeshAdsod in #40416\r\n* Fix edge case for tokenize  by @wangzhen0518 in #36277) \r\n* Fix config dtype parsing for Emu3 edge case  by @Isotr0py in #40766\r\n* Align torch implementation of Gated DeltaNet in Qwen3-Next with fla library.  by @bozheng-hit in #40807\r\n* Fix typos in src  by @cyyever in #40782\r\n* add general hub test for Fast Image Processors in test_image_processing_utils  by @namgyu-youn in #40086\r\n* Push generation config along with checkpoints  by @qgallouedec in #40804\r\n* [`Jetmoe`] Fix RoPE  by @vasqu in #40819\r\n* 🌐 [i18n-KO] Translated clipseg.md to Korean  by @HyunZ118 in #39903\r\n* Improve torch_dtype checks  by @cyyever in #40808\r\n* Add VideoProcessors to auto-backend requirements  by @Cyrilvallez in #40843\r\n* Adds Causal Conv 1D kernel for mamba models  by @MekkCyber in #40765\r\n* Update no split modules in T5Gemma model  by @npuichigo in #40810\r\n* Replace image classification loss functions to `self.loss_function`  by @qubvel in #40764\r\n* Fix the misalignment between the l2norm in GDN of Qwen3-Next and the implementation in the FLA library.  by @bozheng-hit in #40842\r\n* Fixes for continuous batching  by @remi-or in #40828\r\n* [tests] re-enable aria fast tests  by @gante in #40846\r\n* [SAM2] Fix inconsistent results with original implementation with input boxes  by @yonigozlan in #40800\r\n* [Sam2Video] Fix video inference with batched boxes and add test  by @yonigozlan in #40797\r\n* add: differential privacy research model  by @RyanMullins in #40851\r\n* [test] Fix test_eager_matches_sdpa incorrectly skipped  by @eustlb in #40852\r\n* [tests] move generative tests away from `test_modeling_common.py`  by @gante in #40854\r\n* [generate] Always use decoder config to init cache  by @gante in #40772\r\n* Use checkpoint in auto_class_docstring  by @cyyever in #40844\r\n* Fix TrainingArguments.parallelism_config NameError with accelerate<1.10.1  by @albertvillanova in #40818\r\n* Redirect MI355 CI results to dummy dataset  by @ahadnagy in #40862\r\n* [Bug fix #40813] Fix base_model_tp_plan of Starcoder2 model.  by @greg-kwasniewski1 in #40814\r\n* [docstrings / type hints] Update outdated annotations for `past_key_values`   by @gante in #40803\r\n* fix florence kwargs   by @SunMarc in #40826\r\n* fix: XIELU act parameters not being casted to correct dtype  by @NanoCode012 in #40812\r\n* Update model tags and integration references in bug report  by @ArthurZucker in #40881\r\n* [Qwen3 Next] Use numerically stable `rsqrt`  by @thalahors in #40848\r\n* Adding Support for Qwen3-VL Series  by @JJJYmmm in #40795\r\n* [`VaultGemma`] Update expectations in integration tests  by @vasqu in #40855\r\n* Fix modular consistency  by @Cyrilvallez in #40883\r\n* Clarify passing is_causal in sdpa_attention_paged_forward  by @cyyever in #40838\r\n* Use torch.expm1 and torch.log1p for better numerical results  by @cyyever in #40860\r\n* Add Fast PromptDepthAnything Processor  by @SamuelBarryCS in #40602\r\n* Fix deta loading & dataclass  by @Cyrilvallez in #40878\r\n* Remove dict branch of attention_mask in sdpa_attention_paged_forward  by @cyyever in #40882\r\n* 🌐 [i18n-KO] Translated smolvlm.md to Korean  by @HyunZ118 in #40414\r\n* 🌐 [i18n-KO] Translated `imageprocessor.md` to Korean  by @HyunZ118 in #39557\r\n* [generate] remove docs of a feature that no longer exists  by @gante in #40895\r\n* Make debugging failing tests (check and update expect output values) easier 🔥   by @ydshieh in #40727\r\n* Fixing the call to kernelize  by @MekkCyber in #40628\r\n* Fix getter  regression  by @molbap in #40824\r\n* Fix flaky `Gemma3nAudioFeatureExtractionTest::test_dither`  by @ydshieh in #40902\r\n* [cache] Merge static sliding and static chunked layer  by @Cyrilvallez in #40893\r\n* Harmonize CacheLayer names  by @Cyrilvallez in #40892\r\n* [cache] Only use scalars in `get_mask_sizes`  by @Cyrilvallez in #40907\r\n* Set seed for `Glm4vIntegrationTest`  by @ydshieh in #40905\r\n* Add Olmo3 model  by @2015aroras in #40778\r\n* remove dummy EncodingFast  by @cyyever in #40864\r\n* Improve module name handling for local custom code  by @XuehaiPan in #40809\r\n* Remove `runner_map`  by @ydshieh in #40880\r\n* disable `test_fast_is_faster_than_slow`  by @ydshieh in #40909\r\n* [gemma3] `Gemma3ForConditionalGeneration` compatible with assisted generation  by @gante in #40791\r\n* [generate] misc fixes  by @gante in #40906\r\n* Fix dtype in Paligemma  by @zucchini-nlp in #40912\r\n* [Docs] Adding documentation of MXFP4 Quantization  by @ariG23498 in #40885\r\n* Processor load with multi-processing  by @zucchini-nlp in #40786\r\n* [Llama4] Remove `image_sizes` arg and deprecate `vision_feature_layer`  by @yaswanth19 in #40832\r\n* Fix #40067: Add dedicated UMT5 support to GGUF loader (config, tokenizer, test)  by @akshay-babbar in #40218\r\n* [torchao safetensors] renaming get_state_dict function  by @liangel-02 in #40774\r\n* Adding activation kernels  by @MekkCyber in #40890\r\n* Minor fix for #40727  by @ydshieh in #40929\r\n* Add support for Florence-2 training  by @ducviet00 in #40914\r\n* Add LongCat-Flash  by @molbap in #40730\r\n* [DOC] Add missing dates in model cards  by @yonigozlan in #40922\r\n* [models] remove unused `import torch.utils.checkpoint`   by @gante in #40934\r\n* Intel CPU dockerfile  by @jiqing-feng in #40806\r\n* docs(i18n): Correct the descriptive text in the README_zh-hans.md  by @lilin-1 in #40941\r\n* Fix trainer tests  by @SunMarc in #40823\r\n* Fix `Glm4vMoeIntegrationTest`  by @ydshieh in #40930\r\n* Raise error instead of warning when using meta device in from_pretrained  by @Cyrilvallez in #40942\r\n* Consistent naming for images kwargs  by @zucchini-nlp in #40834\r\n* Remove nested import logic for torchvision  by @yonigozlan in #40940\r\n* Fix `Glm4vModelTest::test_eager_matches_fa2_generate`  by @ydshieh in #40947\r\n* Update expected values for some `test_speculative_generation`  by @ydshieh in #40949\r\n* Standardize audio embedding function name for audio multimodal models  by @jackzhxng in #40919\r\n* Add FlexOlmo model  by @2015aroras in #40921\r\n* Don't list dropout in eager_paged_attention_forward  by @cyyever in #40924\r\n\r\n## Significant community contributions\r\n\r\nThe following contributors have made significant changes to the library over the last release:\r\n\r\n* @hiyouga\r\n    * Support batch size > 1 image-text inference (#36682)\r\n* @cyyever\r\n    * Fix typos (#40585)\r\n    * Fix inexistent imports (#40580)\r\n    * Remove unnecessary pillow version check (#40604)\r\n    * Fix invalid typing (#40612)\r\n    * Enable more ruff UP rules (#40579)\r\n    * Avoid attention_mask copy in qwen2.5 (#40658)\r\n    * Fix parent classes of ProcessingKwargs (#40676)\r\n    * Fix parent classes of AllKwargsForChatTemplate (#40685)\r\n    * Fix arguments (#40605)\r\n    * Add Optional typing (#40686)\r\n    * Fix np array typing (#40741)\r\n    * Fix more typos (#40627)\r\n    * Remove reference of video_load_backend and video_fps for processor (#40719)\r\n    * Enable ruff on benchmark and scripts (#40634)\r\n    * Fix typos in tests and util (#40780)\r\n    * Fix invalid PipelineParallel member (#40789)\r\n    * Use functools.cached_property (#40607)\r\n    * Remove use_ipex option from Trainer (#40784)\r\n    * Fix typos in src (#40782)\r\n    * Improve torch_dtype checks (#40808)\r\n    * Use checkpoint in auto_class_docstring (#40844)\r\n    * Clarify passing is_causal in sdpa_attention_paged_forward (#40838)\r\n    * Use torch.expm1 and torch.log1p for better numerical results (#40860)\r\n    * Remove dict branch of attention_mask in sdpa_attention_paged_forward (#40882)\r\n    * remove dummy EncodingFast (#40864)\r\n    * Don't list dropout in eager_paged_attention_forward (#40924)\r\n    * Benchmarking V2: framework impl (#40486)\r\n    * Change docker image to preview for the MI355 CI (#40693)\r\n    * Redirect MI355 CI results to dummy dataset (#40862)\r\n* @voidism\r\n    * fix MetaCLIP 2 wrong link & wrong model names in the docstrings (#40565)\r\n* @RyanMullins\r\n    * add: embedding model (#40694)\r\n    * add: differential privacy research model (#40851)\r\n* @LawJarp-A\r\n    * Add EfficientLoFTRImageProcessorFast for GPU-accelerated image processing (#40215)\r\n* @bozheng-hit\r\n    * Adding Support for Qwen3-Next (#40771)\r\n    * Align torch implementation of Gated DeltaNet in Qwen3-Next with fla library. (#40807)\r\n    * Fix the misalignment between the l2norm in GDN of Qwen3-Next and the implementation in the FLA library. (#40842)\r\n* @wangzhen0518\r\n    * Fix edge case for tokenize (#36277) (#36555)\r\n* @HyunZ118\r\n    * 🌐 [i18n-KO] Translated clipseg.md to Korean (#39903)\r\n    * 🌐 [i18n-KO] Translated smolvlm.md to Korean (#40414)\r\n    * 🌐 [i18n-KO] Translated `imageprocessor.md` to Korean (#39557)\r\n* @JJJYmmm\r\n    * Adding Support for Qwen3-VL Series (#40795)\r\n* @SamuelBarryCS\r\n    * Add Fast PromptDepthAnything Processor (#40602)\r\n* @2015aroras\r\n    * Add Olmo3 model (#40778)\r\n    * Add FlexOlmo model (#40921)",
    "analyzed_at": "2025-11-05 23:58:10"
  },
  "huggingface/transformers#v4.56.2": {
    "tag_name": "v4.56.2",
    "repo_name": "huggingface/transformers",
    "new_features": [],
    "improvements": [
      {
        "feature_type": "improvement",
        "description": "Processor load with multi-processing",
        "pr_links": [
          "https://github.com/huggingface/transformers/pull/40786"
        ]
      }
    ],
    "bug_fixes": [
      {
        "feature_type": "bug_fix",
        "description": "[Jetmoe] Fix RoPE",
        "pr_links": [
          "https://github.com/huggingface/transformers/pull/40819"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix getter regression",
        "pr_links": [
          "https://github.com/huggingface/transformers/pull/40824"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix config dtype parsing for Emu3 edge case",
        "pr_links": [
          "https://github.com/huggingface/transformers/pull/40766"
        ]
      }
    ],
    "other_changes": [],
    "processed_body": "- Processor load with multi-processing (#40786)\r\n- [Jetmoe] Fix RoPE (#40819)\r\n- Fix getter regression (#40824)\r\n- Fix config dtype parsing for Emu3 edge case (#40766)",
    "analyzed_at": "2025-11-05 23:58:16"
  },
  "pytorch/pytorch#v2.9.0": {
    "tag_name": "v2.9.0",
    "repo_name": "pytorch/pytorch",
    "new_features": [],
    "improvements": [],
    "bug_fixes": [],
    "other_changes": [],
    "processed_body": "# PyTorch 2.9.0 Release Notes\r\n- [Highlights](#highlights)\r\n- [Backwards Incompatible Changes](#backwards-incompatible-changes)\r\n- [Deprecations](#deprecations)\r\n- [New Features](#new-features)\r\n- [Improvements](#improvements)\r\n- [Bug fixes](#bug-fixes)\r\n- [Performance](#performance)\r\n- [Documentation](#documentation)\r\n- [Developers](#developers)\r\n- [Security](#security)\r\n\r\n\r\n# Highlights\r\n\r\n<table>\r\n  <tr>\r\n   <td><strong>Unstable (API-Unstable)</strong></td>\r\n  </tr>\r\n  <tr>\r\n   <td>Updates to the stable libtorch ABI for third-party C++/CUDA extensions</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Symmetric memory that enables easy programming of multi-GPU kernels</td>\r\n  </tr>\r\n  <tr>\r\n   <td>The ability to arbitrarily toggle error or resume on graph breaks in torch.compile</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Expanded wheel variant support  to include ROCm,  XPU and CUDA 13</td>\r\n  </tr>\r\n  <tr>\r\n   <td>FlexAttention enablement on Intel GPUs</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Flash decoding optimization based on FlexAttention on X86 CPU</td>\r\n  </tr>\r\n  <tr>\r\n   <td>ARM Platform improvements and optimizations</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Enablement of Linux aarch64 binary wheel builds across all supported CUDA versions</td>\r\n  </tr>\r\n</table>\r\n\r\nFor more details about these highlighted features, you can look at the [release blogpost](https://pytorch.org/blog/pytorch-2-9/). Below are the full release notes for this release.\r\n\r\n\r\n# Backwards Incompatible Changes\r\n\r\n## Min supported Python version is now 3.10 (#162310)\r\n\r\nThe minimum version of Python required for PyTorch 2.9.0 is 3.10. We also have 3.14 and 3.14t available as preview with this release.\r\n\r\n## Undefined behavior when an output of a custom operator shares storage with an input\r\n\r\nThis is a reminder that outputs of PyTorch custom operators (that are registered using the `torch.library` or `TORCH_LIBRARY` APIs) are not allowed to return Tensors that share storage with input tensors. The violation of this condition leads to undefined behavior: sometimes the result will be correct, sometimes it will be garbage.\r\n\r\nAfter [#163227](https://github.com/pytorch/pytorch/pull/163227), custom operators that violated this condition that previously returned correct results under `torch.compile` may now return silently incorrect results under `torch.compile`. Because this is changing the behavior of undefined behavior, we do not consider this to be a bug, but we are still documenting it in this section as a \"potentially unexpected behavior change\".\r\n\r\nThis is one of the conditions checked for by [`torch.library.opcheck`](https://docs.pytorch.org/docs/stable/library.html#testing-custom-ops) and is mentioned in [The Custom Operators Manual](https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit?tab=t.0#bookmark=id.4c0um7xkba6e)\r\n\r\n### More details\r\n\r\nOutputs of PyTorch custom operators are not allowed to return Tensors that share storage with input tensors\r\n\r\nFor example, the following two custom operators are not valid custom operators:\r\n\r\n```py\r\n@torch.library.custom_op(\"mylib::foo\", mutates_args=())\r\ndef foo(x: torch.Tensor) -> torch.Tensor:\r\n    # the result of `foo` must not directly be an input to foo.\r\n    return x\r\n\r\n@torch.library.custom_op(\"mylib::bar\", mutates_args=())\r\ndef bar(x: torch.Tensor) -> torch.Tensor:\r\n    # the result of bar must not be a view of an input of bar\r\n    return x.view(-1)\r\n```\r\nThe easiest workaround is to add an extra `.clone()` to the outputs:\r\n```py\r\n@torch.library.custom_op(\"mylib::foo\", mutates_args=())\r\ndef foo(x: torch.Tensor) -> torch.Tensor:\r\n    return x.clone()\r\n\r\n@torch.library.custom_op(\"mylib::bar\", mutates_args=())\r\ndef bar(x: torch.Tensor) -> torch.Tensor:\r\n    return x.view(-1).clone()\r\n```\r\n\r\nA common way to get into this situation is for a user to want to create a custom operator that sometimes mutates the input in-place and sometimes returns a new Tensor, like in the following example.\r\n\r\n```py\r\n@torch.library.custom_op(\"mylib::baz\", mutates_args=[\"x\"])\r\ndef baz(x: torch.Tensor) -> torch.Tensor:\r\n    if inplace:\r\n        x.sin_()\r\n        return x\r\n    else:\r\n        return x.sin()\r\n```\r\nThis dynamism is not supported and leads to undefined behavior. The workaround is to split the custom operator into two custom operators, one that always mutates the input in-place, and another that always returns a new Tensor.\r\n```py\r\n@torch.library.custom_op(\"mylib::baz_outplace\", mutates_args=())\r\ndef baz_outplace(x: torch.Tensor) -> torch.Tensor:\r\n    return x.sin()\r\n\r\n@torch.library.custom_op(\"mylib::baz_inplace\", mutates_args=[\"x\"])\r\ndef baz_inplace(x: torch.Tensor) -> torch.Tensor:\r\n    x.sin_()\r\n\r\ndef baz(x):\r\n    if inplace:\r\n        baz_inplace(x)\r\n        return x\r\n    else:\r\n        return baz_outplace(x)\r\n```\r\n\r\n## Build metal kernels of MacOS-14+ and remove all pre-MacOS-14 specific logic, requires MacOS-14+ going forward (#159733, #159912)\r\n\r\nPyTorch MPS is only supported on MacOS-14 or later. If you need to use MPS on MacOS Ventura, please avoid updating to Python-3.9 or above\r\n\r\n## Upgrade to DLPack 1.0 (#145000)\r\n\r\nThis upgrade is doing the same BC-breaking changes as the DLPack release. Objects in `torch.utils.dlpack` have been updated to reflect these changes, such as `DLDeviceType`.\r\n\r\nSee the PR for details on the exact changes and how to update your code.\r\n\r\n## Raise appropriate errors in `torch.cat` (#158249)\r\n\r\n`torch.cat` now raises `ValueError`, `IndexError` or `TypeError` where appropriate instead of the generic `RuntimeError`. If you code was catching these errors, you can update to catch the new error type.\r\n\r\n\r\n## Default to `dynamo=True` for ONNX exporter (#159646, #162726)\r\n\r\nPreviously `torch.onnx.export(...)` used the legacy TorchScript exporter if no arguments were provied. The ONNX exporter now uses the newer `torch.export.export` pipeline by default (`dynamo=True`). This change improves graph fidelity and future-proofs exports, but may surface graph capture errors that were previously masked or handled differently.\r\n\r\nPreviously in torch 2.8.0:\r\n\r\n```python\r\n# API calls the legacy exporter with dynamo=False\r\ntorch.onnx.export(...)\r\n```\r\n\r\nNow in torch 2.9.0:\r\n\r\n```python\r\n# To preserve the original behavior\r\ntorch.onnx.export(..., dynamo=False)\r\n\r\n# Export onnx model through torch.export.export\r\ntorch.onnx.export(...)\r\n```\r\n\r\nRecommendation: first try the new default; only fall back if you hit blocking issues and report them upstream.\r\nLong term solution: fix the root cause instead of relying on fallback or TorchScript exporter.\r\n\r\n## Switch off runtime asserts by default in Export in favor of a shape guards function (#160111, #161178, #161794)\r\n\r\n\r\nTo enable runtime asserts, use `export(..., prefer_deferred_runtime_asserts_over_guards=True)`. Also kills the `allow_complex_guards_as_runtime_asserts` flag, merging it into the former option.\r\n\r\n\r\nAdditionally, `exported_program.module()` will generate a call to a `_guards_fn` submodule that will run additional checks on inputs. Users who do not want this behavior can either remove this call in the graph, or do `exported_program.module(check_guards=False)` to avoid the generation.\r\n\r\n## Set default opset to 20 in ONNX (#158802)\r\n\r\nOpset 20 enables newer operator definitions. If your tooling or downstream runtime only supports opset 18, pin it explicitly. For the latest ONNX operators, you can experiment with opset 23.\r\n\r\nPreviously in torch 2.8.0:\r\n\r\n```python\r\n# opset_version=18\r\ntorch.onnx.export(...)\r\n```\r\n\r\nNow in torch 2.9.0:\r\n\r\n```python\r\n# To preserve the original behavior\r\ntorch.onnx.export(..., opset_version=18)\r\n\r\n# New: opset_version=20\r\ntorch.onnx.export(...)\r\n\r\n# Use the latest supported opset: opset_version=23\r\ntorch.onnx.export(..., opset_version=23)\r\n```\r\n\r\n## Drop `draft_export` in exporter API (#161454, #162225)\r\n\r\nRemove implicit draft tracing from the default exporter path, achieving clearer behaviour and faster failures.\r\nThe expensive `torch.export.draft_export` diagnostic path is no longer auto-invoked (which could take hours on large models). You can still opt in for deep diagnostics:\r\n\r\nPreviously in torch 2.8.0:\r\n\r\n```bash\r\n# If both torch.export.export(..., strict=False) and\r\n# torch.export.export(..., strict=True) fail to capture\r\n# the model graph, torch.export.draft_export(...) will be triggered,\r\n# and uses real tensor to trace/export the model.\r\n#\r\n# Inside export_to_onnx.py:\r\n#  ... torch.onnx.export(..., dynamo=True)\r\npython export_to_onnx.py\r\n```\r\n\r\nNow in torch 2.9.0:\r\n\r\n```bash\r\n# To trigger torch.export.draft_export once\r\n# torch.export.export strict=False/True both\r\n# fail:\r\n\r\nTORCH_ONNX_ENABLE_DRAFT_EXPORT=True python export_to_onnx.py\r\n```\r\n\r\n## Remove `torch.onnx.dynamo_export` and the `onnxrt` torch compile backend (#158130, #158258)\r\n\r\n`torch.onnx.dynamo_export` is removed. Please use `torch.onnx.export` instead.\r\nThe experimental ONNX Runtime compile backend (`torch.compile(backend=\"onnxrt\")`) is no longer supported.\r\n\r\n## Remove `torch.onnx.enable_fake_mode` (#161222)\r\n\r\nThe `dynamo=True` mode uses `FakeTensor`s by default which is memory efficient.\r\n\r\n## Some public facing ONNX utility APIs for the TorchScript based exporter are now private (#161323)\r\n\r\nDeprecated members in `torch.onnx.verification` are removed. Previously private `torch.onnx.symbolic_opsets*` functions will no longer be accessible. Consider making a copy of the source code if you need to access any private functions for compatibility with the TorchScript based exporter.\r\n\r\n## Remove `torch.onnx.symbolic_caffe2` (#157102)\r\n\r\nSupport for `caffe2` in the ONNX exporter has ended and is removed.\r\n\r\n## Remove `/d2implyavx512upperregs` flag that slows build (#159431)\r\n\r\nRe-introduced AVX512 optimizations for Windows VS2022 builds, may cause issues with specific versions of VS2022, see #145702\r\n\r\n## Add `ScalarType` to shim conversion and `stable::Tensor.scalar_type` (#160557)\r\n\r\nBefore, user extensions could only in abstract pass around obfuscated dtypes appearing as `int32_ts`. Now, users can confidently use `torch::headeronly::ScalarType` in their extensions for major scalar types. This PR enables ABI stability by adding a translation layer through the shim, so that even if the `ScalarType` enum values change in the future, user extensions need not fear.\r\n\r\nThis change adds ScalarType support for user extensions and is only narrowly BC breaking for unpopular dtypes: `quint*`s, `qint*`s, `Bits*`, `dummy_uint*`s, `dummy_int*`s, `Float8_e8m0fnu`, and `Float4_e2m1fn_x2` in the use case where an extension retrieves a Tensor dtype of the above and passes it into `aoti_torch_call_dispatcher`.\r\n\r\n# Deprecations\r\n## Deprecate `pin_memory_device` param in `torch.utils.data.DataLoader` (#158323)\r\n\r\nWe move enabling `pin_memory` back inside `BaseDataLoaderIter`. This is required for `StatefulDataloader` which leveraged `BaseDataLoaderIter` direclty rather than the `Dataloader` class init\r\n\r\n## Deprecate `torch.export.export_for_training` API in favor of equivalent `torch.export.export` API (#158203)\r\n\r\n`torch.export.export_for_training` exists because we couldn't migrate internal usages of export to the final IR. Now that we have completed the migration, we deprecated and deleted this API.\r\n\r\n# New Features\r\n## Python Frontend\r\n- Add utility to get the kernel currently registered on the dispatcher (#158393)\r\n- Extend `__torch_function__` handler to be triggered by elements within a list (#160256)\r\n- Add `torch.hash_tensor` reduction function (#154149)\r\n\r\n## FX\r\n- Extend torch function support to ALL arguments instead of just scalar type (but not inside of list, #145089)\r\n- Add `is_fx_symbolic_tracing` flag (#161385)\r\n\r\n## Dynamo\r\n- Experimental API for ahead-of-time compiling models in fullgraph mode (#161383)\r\n- Add a hook for recompilations (#157961)\r\n- DynamicInts prototype (#162194)\r\n\r\nIntroduces an API for annotating dynamic integer inputs & attributes for `torch.compile`, by wrapping plain ints with `DynamicInt()`.\r\nDynamicInt objects also work in eager mode, acting as their underlying values when passed as scalar inputs.\r\n\r\n```python\r\na = DynamicInt(4)\r\ny = a + 2  # DynamicInt(6)\r\nz = torch.ones(a)  # torch.ones(4)\r\n\r\nfn = torch.compile(torch.ones)\r\nfn(a)  # compiled fn takes a dynamic integer input\r\nfn(2)  # returns torch.ones(2) without recompiling\r\n```\r\n\r\n\r\n## Optimizer\r\n- Introduce Muon optimizer to PyTorch (#160213)\r\n\r\n## Profiler\r\n- Add GC Events to Python Stack Tracer (#161209)\r\n- Add a custom profiler configuration option (#151656)\r\n\r\n## Inductor\r\n- Allow user to pass in custom partitioner function (#157580)\r\n\r\n## Export\r\n- Add support for param mutation under inference mode (#159661)\r\n\r\n## AOTDispatcher\r\n- Add AOTDispatcher config to set backward autocast behavior (#156356)\r\n\r\n## Quantization\r\n- Enable cpu fp8 qlinear and cpu fp8 qconv (#155678, #157076)\r\n\r\n## ONNX\r\n- RMS Norm support in opset 23 (#159377)\r\n\r\n## C++ Extensions\r\n- Build out a stable set of ATen ops in `torch/csrc/stable/ops.h`:  `amax`, `narrow`, `new_empty` + `new_zeros` dtype variant, `pad`, (#159328, #158974, #159508, #161597, #160214)\r\n- Add `torch::stable::Tensor()` default constructor,  `is_cpu`, and `get_device_index`(#159507, #160212, #160143)\r\n- Add beginnings of `torch::stable::accelerator` with support for DeviceGuard and Stream (#159679, #160453)\r\n- Start building out `torch/headeronly`: c10 Macros, STD_TORCH_CHECK, ScalarTypes (like BFloat16 and Half, #158035, #158365, #157912, #158377, #159302, #159414, #159412, #159415, #159411, #159911)\r\n- Remove cmake cache and reconfigure again if it is invalid (#156958)\r\n- Cut a version of `TORCH_ERROR_CODE_CHECK` in `headeronly` from AOTI (#159604)\r\n- Remove `wheel` from build requirements (#158027)\r\n- Error when `TORCH_STABLE_ONLY` is defined in `TensorBase.h` (#161658)\r\n\r\n## Build Frontend\r\n- Add transpose to `torch/csrc/stable` (#158160)\r\n- Add `zero_()` and `empty_like(t)` to `torch/csrc/stable/ops.h` (#158866)\r\n\r\n## Release Engineering\r\n- Add support for CUDA 13.0 in CI/CD builds. Enable CUDA compression mode for binary size reduction for CUDA 13.0 builds (#160956, #161073, #161257, #161663, #161316, #160201, #160770, #161013, #161916, #162268, #162322, #162383, #161833)\r\n\r\n- Enable CUDA 12.6, 12.8 and 13.0 support for Linux ARM64 CD builds (#162364, #160720, #159481)\r\n\r\n- Add support for Python 3.14 in CI/CD builds (#156889, #157559, #159261, #159869, #160593, #160788, #161255, #159725)\r\n\r\n- Enable NVSHMEM integration (#151261, #153010, #154538, #155506, #156685, #158938, #161321, #160778, #159907, #160465)\r\n\r\n## CUDA\r\n- Add getter for CUDA graph exec to allow mutation of captured kernel params (#161294)\r\n- Implement support for `cudnn_batch_norm_out` kernel to replace the autogen approach (#123020)\r\n\r\n## CPU\r\n- Support GQA for flash attention (#157893)\r\n\r\n## MPS\r\n- Partial sparse support for MPS backend (#159729, #160254, #160223, #161846, #162007, #157238)\r\n- Add `avg_pool3d`, `max_unpool1d/2d/3d`, `max_pool3d`, `max_pool3d` bwd pass, and `avg_pool3d` bwd pass for MPS (#158877,#159789, #156467, #157498, #159089)\r\n\r\n## ROCm\r\n- OCP Micro-scaling Format (mx-fp8/mx-fp4) Support (#151360)\r\n\r\n## XPU\r\n- Enable `FlexAttention` on Intel GPU (#143553)\r\n\r\n# Improvements\r\n## Python Frontend\r\n- Speed up `torch.load` under `FakeTensorMode` by reducing random reads (#157931)\r\n- Make `torch.utils.benchmark.utils.timer` accelerator agnostic (#157131)\r\n- Improve error message for weight-only load errors (#159935)\r\n\r\n## torch.nn\r\n- Allow `register_buffer` with `Tensor`-like objects (#159455)\r\n- Improve error message for unsupported padding configurations (#160866)\r\n- Validate target is 0D when input is 1D in `NLLLoss` (#161412)\r\n\r\n## Optimizer\r\n- Resolve warning in LBFGS when converting a tensor with `requires_grad=True` to a scalar (#160389)\r\n- Resolve `SequentialLR` deprecation warning about invoking `step(epoch)` (#149392)\r\n\r\n## Autograd\r\n- Support deterministic `torch.nn.Upsample` `mode=\"trilinear\"` backward (#154239)\r\n\r\n## Distributed\r\n### c10d\r\n  - Add improvements to eager init of `ProcessGroupNCCL` (#156748)\r\n  - Simplify unique hash management of `ProcessGroupNCCL` (#156790)\r\n  - Support per operation timeouts in `ProcessGroupGloo` (#158128)\r\n  - Allow ping to be retried in `TCPStore` (#159165)\r\n  - Support scalar tensor for functional `all_gather` (#149913)\r\n  - Expos `unsafe_get_ptr` for dist.ProcessGroupNCCL.NCCLConfig (#161136)\r\n  - Add batch option for `send/recv_object_list` (#160342)\r\n  - Make FakeStore optional to be passed into fake backend (#162164)\r\n  - Enable complex datatype support in `ProcessGroupGloo` (#156633)\r\n  - Move thread-local capture mode guard to include `work.isStarted` (#160398)\r\n### DistributedDataParallel (DDP)\r\n  - Support ddp zero hook XCCL path (#159240)\r\n### DTensor\r\n  - Relax `device_mesh` argument constraint in `local_map` (#157049)\r\n  - Support complex numbers in DTensor redistribute (#157329)\r\n  - Rework partial propagation in point-wise op and support mul (#157340)\r\n  - Allow dynamic shapes for `DTensor` slice (#157953)\r\n  - Implement `histc` op (#158298)\r\n  - Made dispatch to sharding prop over decomps (#159324)\r\n  - Support user-supplied Generator for random ops (#159933)\r\n  - Add `propagate_tensor_meta` function that skips cache if `_are_we_tracing` (#161334)\r\n  - Support `local_map` as a decorator (#161353)\r\n### Device Mesh\r\n  - Enable the use of user set backend and pg option even for the global mesh (#157501)\r\n  - Enable slicing a submesh with warnings (#158899)\r\n  - Allow controlling PG backend and options via `init_device_mesh` (#159371)\r\n### FullyShardedDataParallel2 (FSDP2)\r\n  - Support custom `all_gather` and `reduce_scatter` comms (#155189)\r\n  - Made it fail `set_allocate_memory_from_process_group` if used together with custom comm hooks (#157487)\r\n  - Use `reduceOpSum` when world size is 1 (#157529)\r\n  - Skipp `allgather` when world size is 1 (#160135)\r\n  - Use `post_reduce_stream.record_event()` on hsdp+cpuoffload (#160481)\r\n### Tensor Parallel (TP)\r\n  - Improve `parallelize_module` API to support more cases (#157182)\r\n### TensorPipe\r\n  - Update TensorPipe pinned dependency version (#159834)\r\n### TorchElastic\r\n  - Enable NUMA binding integration with elastic agent and `torchrun` (#149334)\r\n  - Support NUMA Binding for Callable Entrypoints (#160163, #161183)\r\n### Pipeline Parallelism (PP)\r\n  - Add `eval()` API to schedule (#157795)\r\n  - Allow intermediate nodes in zero bubble to have multiple grads (#159084)\r\n  - Support `OVERLAP_F_B` computation type (#158978)\r\n  - Initializ P2P communicators on first step (#160210)\r\n  - Add `DualPipeV` schedule (#159591)\r\n\r\n## Linear Algebra Frontend\r\n- Use rocSOLVER for Cholesky inversion on AMD. (#157154)\r\n- Add option for using TF32 as fp32 internal precision for matmul/linear/conv on MKLDNN (#157520)\r\n- Make einsum produce contiguous outputs in more cases (#161755)\r\n\r\n## Profiler\r\n- Add more CUDA API for kernel launcher (#156016)\r\n- Allow Custom Time Unit When Printing Profiler Table (#157913)\r\n- Update CUDA runtime kernel identification logic (#157890)\r\n\r\n## FX\r\n- Fix DCE eliminating random operations by improving `is_impure()` (#151524, #157981)\r\n- Support converting a float32 tensor to a scalar in FX trace. (#158216)\r\n- Correctly copy `self.module_stack` in ModuleStackTracer (#159956)\r\n- Add tool to track events in graph split (#159795)\r\n- Add `node_name_match` to subgraph rewriter (#157574)\r\n\r\n## Dynamo\r\n- Improve tracing support for various Python builtin data structures/modules:\r\n  - `list`s (e.g. #153969)\r\n  - `set`s (e.g. #153150)\r\n  - `dict`s (e.g. #154794)\r\n  - `iter` (e.g. #156371)\r\n  - `itertools` (e.g. #159693)\r\n  - `collections` (e.g. #159365)\r\n  - `collections.NamedTuple` (#159367)\r\n  - frozen `dataclasses.dataclass` (#159529)\r\n- Graph break error messages link to a website with more information (#159011)\r\n- Add option for `TorchDispatchMode` to ignore `torch.compile` internals (#161648)\r\n\r\n## Inductor\r\n- Add Inductor support for MTIA backend (#159211)\r\n- Share default device context when all graph partitions and cudagraph-unsafe ops are on the same device(#162873)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Enable AOTI for CPU on Windows (#158915)\r\n- Re-enable TMA templates w/ AOTI (#157819)\r\n- Don't allow int32 indices if `{non-inf, > int32_max}` upper bound is provided (#159433)\r\n- Add RecordFunction to C shim so that profiling works with AOTI (#159842)\r\n- Add AOTI C shim functions for collective ops (#154492)\r\n- Add missing ops to set of C-shim ops which can have nullptr returns (#158073)\r\n\r\n## Export\r\n- Handle `None` & ellipsis slicing/select in non-strict (#157821)\r\n- Extend FP8 types in serialization (#158430)\r\n- Improve error messages for deserialization (#159881)\r\n- Support serialization for `triton_kernel_wrapper_functional` HOP (#161314)\r\n- Support serialization for complex constants (#161517)\r\n- Add runtime asserts to `while_loop` HOP subgraphs (#158467)\r\n- Warn on side-effectful code in strict mode (#160060)\r\n- Support for vmap in pre-dispatch export (#154650)\r\n- Support vmap and custom autograd function/improve DTensor constructor inefficiency (#162240)\r\n\r\n## AOTDispatcher\r\n- Skip logging in fp8 activation quantization if there are no nodes to be quantized (#158129)\r\n- Add `aot_export_joint_with_descriptors` and `aot_compile_joint_with_descriptors` (#158715)\r\n- Extract out `prepare_aot_module_simplified` for use in next PR (#158319)\r\n- Rename modules in AOTAutograd (#158449)\r\n- Track descriptors for all inputs/outputs of AOTAutograd traced graph (#158624)\r\n- Improve graph output alias with subclass error message (#159619)\r\n- Pass fw/bw compilers to `aot_export_joint_with_descriptors` (#159814)\r\n\r\n## Composability\r\n- Meta implementation for `aten.add.Scalar` (#161332)\r\n- `aten.expand_copy` decomp (#161688)\r\n- Fix result dtype cast in decomp for `aten.linalg_vector_norm` (#155111)\r\n- Add dtype checks in meta implementation for several ordering ops (#159556)\r\n- Fix meta function for `aten.complex` (#160894)\r\n- Improve unbacked symint (dynamic shape) support for several decompositions (#148815, #156902, #157008, #158894, #159184, #160683, #160253, #162084, #162099, #162109, #160462)\r\n\r\n## Quantization\r\n- Avoid getting model device once per node for pt2e quantization flow (#159901)\r\n- Fixes bug in implementation of `HistogramObserver` (#156457)\r\n- Support `bias=None` for `fbgemm_linear_fp16_weight` CPU op (#158535)\r\n- Add Static Dispatch Kernel for `wrapped_fbgemm_linear_fp16_weight` for Sigmoid (#160451)\r\n\r\n## Nested Tensor (NJT)\r\n- Added initial `log_softmax()` support (#159662)\r\n\r\n## Foreach\r\n- Invoke `vector.reserve()` consistently for non-inplace foreach operations (#161128)\r\n- Faster and safer lambda expression capture in `has_integral_tensor()` (#161042)\r\n\r\n## ONNX\r\n- Support symbolic arguments in ONNX exporter (#157734)\r\n- Fix `torch.tensor` warning in ONNX `symbolic_opset10` export  (#158835)\r\n\r\n## C++ Frontend\r\n- Generalized `AllocatorConfig` to be device-agnostic via new `AcceleratorAllocatorConfig` (#149601, #150312)\r\n- Added `Scalar::isUnsigned()` method (#159877)\r\n- Exposed `ModelRunner` from nativert as public (#159989)\r\n- Improve error message for `torch.binomial` enforcing float inputs (#157658)\r\n\r\n## Build Frontend\r\n- Fix dev warning in `Dependencies.cmake` (#159702)\r\n- Fix building system gloo with CUDA/HIP (#146637)\r\n- Build `libtorch` without NVSHMEM (#160910)\r\n- Improve BLAS feature detection (#143846)\r\n\r\n## Release Engineering\r\n- Enable vLLM testing workflow (#160583, #161565, #162292, #162000, #161797)\r\n- Enable Windows ARM64 CI testing (#148753, #161504)\r\n- Enable PyTorch ROCm CI for MI355X testing. (#158889)\r\n\r\n## CUDA\r\n- Make cublaslt/hipblaslt workspaces persistent (#156495)\r\n- Remove unnecessary warnings during the ATen compilation process (#157703)\r\n- Slightly improve error message from `repeat_interleave` kernel (#157996)\r\n- Add framework for explanations for common CUDA errors (#158395)\r\n- Upgrade KernelLauncher `kernelLaunchCheck` to print help string (#158896)\r\n- Prep for cutlass upgrade by ignoring `Wunused-but-set-variable` (#159276)\r\n- Workaround ATen SFINAE under `libc++` (#161101)\r\n- Implement changes to CCCL (CUB/Thrust/LibCUDACXX) usage in ATen (#153373)\r\n- Add maybe unused flag to remove warning (#157655)\r\n- Use new CCCL API in v2.8 (#160554)\r\n- Improve cupy device placement when device is provided with explicit index (#158529)\r\n\r\n## CPU (AArch64)\r\n- Made PyTorch compilable with gcc-14 on ARM (#157867)\r\n\r\n## MPS\r\n- Add `shifted_chebyshev_polynomial_[tuvw]`, `igamma/igammac,grid_sampler_3d, native_dropout`/`native_dropout_backward`  (#157488, #161927, #160541, #162108)\r\n- Extend atomic operations to all int types (#158179)\r\n- Extend `index_put` to complex types (#160159)\r\n- Extend `addmm` to integral types (#160270)\r\n- Add support for unsigned types (#159094)\r\n- Add API to query GPU core count (#160414)\r\n- Add `kthvalue` (#161817)\r\n- Type-promote tensor-iterator common dtype (#160334)\r\n- Implement `logcumsumexp` metal kernel (#156858)\r\n- Enable `dlpack` integration (#158888)\r\n- Dynamic reductions (#159355)\r\n- Update `avg_pool2d` to use Metal kernel when `ceil_mode=True` (#161011)\r\n\r\n## ROCm\r\n- Additional hipify mappings (#158056, #158352, #161992)\r\n- Refactor `composable_kernel` (CK) backend user interface to improve user experience (#152951)\r\n- Allow use of `rocSOLVER` for Cholesky inversion. (#157154)\r\n- AOT Inductor enable gfx950 for max autotune using CK (#159195)\r\n- Add flag `torch.backends.miopen.immediate` to toggle MIOpen Immediate Mode instead of relying on `deterministic=True` and `benchmark=False` (#158951)\r\n- MIOpen convolutions no longer call `reshape_` or unexpectedly change memory formats (#161687)\r\n\r\n## XPU\r\n- Support Intel GPU quantization ops in AOTInductor (#156572)\r\n- Add `device_id` to Intel GPU properties to distinguish iGPUs with identical names (#156481)\r\n\r\n# Bug Fixes\r\n## Python Frontend\r\n- Add option in `torch.utils.cpp_extension.load_inline` to override gencode (#156850)\r\n- Fix `max_width` computation in Tensor printing (#126859)\r\n- Improve `pin_memory` error message on CPU-only systems (#159994)\r\n- Making batching rule for `F.embedding` DTensor-aware (#162117)\r\n\r\n## Autograd\r\n- Fix `torch.autograd.Function` memory leak due to `torch.utils.checkpiont` early stopping (#161171)\r\n- Fix `torch.autograd.graph.GradientEdge` for `torch.autograd.Function` (#160098)\r\n- Match 0-dim gradients device type regardless of subclass-ness (#160165)\r\n\r\n## Distributed\r\n### c10d\r\n  - Fix slow init due to repeated dns resolution failure in socket (#159596)\r\n  - Fix `setGroupName` and `setGroupDesc` in `group_split` and `merge_remote_group` (#159429)\r\n  - Fix a bug of distributed 'gather' with noncontiguous tensors on the Gloo backend (#158903)\r\n  - Fix a bug of distributed 'gather' with noncontiguous tensors on the NCCL backend (#159549)\r\n  - Fix data inconsistencies when using `batch_isend_irecv` with 2D tensor views by making P2P tensors dense (#163719)\r\n  - Handle discontiguous `allgather`/`reducescatter` inputs (#163712)\r\n### Device Mesh\r\n  - Fix the not incorrectly chained each of the strings as iterables (#160709)\r\n### DistributedDataParallel (DDP)\r\n  - Fix incorrect interaction between `DDPOptimizer` and donated buffers (#160745)\r\n### DTensor\r\n  - Fix DTensor handling of conjugate bit (#158030)\r\n  - Fix `OpSchema` equality check (#161231)\r\n  - Fix `grouped_mm` strategy for invalid stride cases (#158245)\r\n  - Fix `F.one_hot` in DTensor (#162307)\r\n  - Always disabled `ShardingPropagation` cache if compiling (#156868)\r\n### FullyShardedDataParallel (FSDP)\r\n  - Fix the bug in FSDP offload `pin_memory` (#157147)\r\n  - Fix to ensure writeback handles `NO_SHARD` correctly by flattening tensors before copying (#154369)\r\n### FullyShardedDataParallel2 (FSDP2)\r\n  - Fix error message for `fsdp_pre_all_gather` (#160817)\r\n  - Fix the issue with `set_reduce_scatter_divide_factor` errors and `MixedPrecisionPolicy`  (#155964)\r\n### Pipeline Parallelism (PP)\r\n  - Fix eval step under `no_grad()` (#159293)\r\n  - Fix zero bubble schedules for `eval()` (#159475)\r\n### TensorPipe\r\n  - Fix `import torch` if compiled without `TensorPipe` (#159461)\r\n### TorchElastic\r\n  - Fix wrong log file name in the docs of `torch.distributed.elastic.multiprocessing.start_processes()` (#160396)\r\n\r\n## Linear Algebra Frontend\r\n- Avoid downcasts for fp16 matmul on the BLAS backend (#161999)\r\n\r\n## Profiler\r\n- Fix Linter for Global Annotations flag in Snapshot (#157858)\r\n\r\n## FX\r\n- Fix `split_module` with symint (#160093)\r\n- Fix `getattr_recursive` with ModuleList (#161204)\r\n- Skip const folding with symbolic expression (#161437)\r\n- Fix qualified name for methods of `torch.Tensor` (#162224)\r\n\r\n## Dynamo\r\n- Fix segfault due to interaction between Dynamo backends and `torch.compiler.reset()` (#156527)\r\n- Fix crash due to bad interaction with recompilations and with blocks in Python 3.11+ (#162318)\r\n\r\n## torch.nn\r\n- Fix silent correctness w/ backpropping grads for `FlexAttention` (#163677)\r\n- Fix `return_lse` warning message in `FlexAttention` (#163578)\r\n- Fix `FlexAttention` head broadcast (#163426)\r\n\r\n## Inductor\r\n- Fix wrong meta function for `constant_pad_nd` (#159878)\r\n- Fix learnable bias assertion error in Inductor (#161170)\r\n- Fix int64 from `MutationOutput` Buffer (#162020)\r\n- Fix Inductor CUDA sort `NaN` behavior (#159308)\r\n- Fix layout for local buf in outer loop fusion (#160857)\r\n- Fix slice scatter `dtype` consistency (#160851)\r\n- Fix 3d tiled online softmax (#162341)\r\n- Fix unsafe collective reorder past wait in Inductor (#157489)\r\n- Fix `FallbackKernel` alias function to avoid incorrect aliasing for custom ops (#163227)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Fix a bug from `load_constants` (#161887)\r\n- Fix wrong propagation of fallback_ops_dict in `gen_aoti_c_shim` (#159904)\r\n- Fix unbacked symint and memory leak in Inductor memory planning (#159839)\r\n- Fix memory leak in AOTI when calling `aoti_torch_as_strided` (#162118)\r\n- Explicitly delete `wait_tensor` returned tensor (#159502)\r\n- Fix memory leak from `all_reduce` (#159818)\r\n\r\n## Composability\r\n- Make functionalization ViewMeta serializable with pickle (#163769)\r\n\r\n## Export\r\n- Fix bug in constants lifting pass (#157719)\r\n- Fix `from_node` provenance in unlift pass (#157943)\r\n- Fix `NaN` serialization (#155359)\r\n- Fix deserialization for unbacked symbol ranges (#158681)\r\n- Fix runtime assert handling in deserialization (#159060)\r\n- Fix for FQN handling in unflattener (#159418)\r\n- Fix `nn_module_stack` for `assert_tensor_metadata` nodes (#159625)\r\n- Fix usage for `move_to_device_pass` (#159992, #160528, #162301)\r\n- Avoid name overwrites for aliased exported module parameters (#160600)\r\n- Avoid inling `dynamo.disables` in unflattening (#161306)\r\n- Fix deserialization issue for storage offset (#162172)\r\n- Remove `.contiguous()` when saving weights to raw bytes to preserve original storage size of tensor (#163587)\r\n\r\n## Quantization\r\n- Avoid `NaN` in fp8 output of CPU `qlinear` and `qconv` ops (#160957)\r\n- Fix segmentation fault when `choose_qparams_optimized` (#161966)\r\n\r\n## Foreach\r\n- `chunk_size` should always be `int64_t` for Foreach functors (#156872)\r\n\r\n## ONNX\r\n- Make onnx export SDPA match ATen behavior (#159973)\r\n- Fix `rotary_embedding_23` implementation (#162865)\r\n- Fix export behavior when model has `None` as output (#160200)\r\n- Fix lower opset version support in `dynamo=True` (#161056)\r\n- Fix `index_put_` usage (#161263)\r\n\r\n## C++ Extensions\r\n- Fix CPP extension distributed warning for `TORCH_CUDA_ARCH_LIST` to only log when running on non-distributed or on rank 0 (#162764)\r\n\r\n## C++ Frontend\r\n- Fix `torch.utils.cpp_extension` parser for clang version 20.1.7+libcxx (#157666)\r\n- Fix `MakeTensor::computeStorageSize()` calculation (#158690)\r\n- Fix static initialization order issue with `AllocatorConfig` (#159629)\r\n\r\n## Build Frontend\r\n- Turn on `BUILD_BUNDLEPTXAS=1` to allow compile on newer GPUs(#163988)\r\n\r\n## CUDA\r\n- Handle uninitialized `torch.backends.cuda.matmul.fp32_precision` (#161102)\r\n- Fix nansum in non-JIT build (#158633)\r\n- Decrease launch bounds of CTCLoss backward for blackwell to avoid crash (#159522)\r\n- Implement workaround for `cudaErrorNotSupported` (#162412)\r\n- Fix missing `__syncthreads` in MultiMarginLoss backward (#158994)\r\n- Roll-back cuDNN frontend upgrade and update Meta registration due to compile issues (#163104)\r\n- Disable cuDNN for 3D convolutions with `kernel size != 1` for cuDNN 9.8+ (#163581)\r\n\r\n## CPU\r\n- Add check so non-aarch64 platforms can hit `MKLDNN` path (#162168)\r\n\r\n## MPS\r\n- Fix batch norm incorrect gradient (#156867)\r\n- Do not crash if `tensor dim > INT_MAX` (#158824)\r\n- Avoid outputing zeros from `exponential_` for MPS (#159386)\r\n- Fix MPS autocast for `ConvTranspose3d` (#160345)\r\n- Fix MPS `conv3d` autocast bias dtype mismatch (#160423)\r\n- Fix error check for `torch.var` on scalar (#160889)\r\n- Fix `index_add` for complex + int64, int64 input + zerodim index (#160926, #161511)\r\n- Fix `constant_pad_nd_mps` bug when pad is empty (#161149)\r\n- Fix `index_select` for `scalar_types` (#161206)\r\n- Fix `index_copy` for scalars and `index_copy` for strided indices (#161267, #161333)\r\n- Ensure that tensors are contiguous before using MPS linear kernel (#161641)\r\n- Address `NaN`s if SDPA is called with all values masked from query (#157727)\r\n- Fix invalid formatting (#158436)\r\n- Fix empty input in posneg functions (#161824)\r\n- Migrate round unary op to Metal (#161712)\r\n- Type-promote tensor-iterator common dtype (#160334)\r\n- Fix regression in 2.8.0 for `scaled_dot_product_attention` using MPS (#163598)\r\n- Chunk `fillBuffer` into 4Gb slices to avoid regression on MacOS 26 (#164108)\r\n- Fix latent bug that can result in segfault in CPP extensions (#164093)\r\n\r\n## ROCm\r\n- Fix Inductor with cudagraph trees `hip:0` device error (#161221)\r\n- Fix some build failures and support some BLAS calls on Windows (#161981)\r\n- Fix undefined symbol linker error after exposing MIOpen symbols on Windows (#156479)\r\n- Fix finding ROCm/HIP version on Windows (#156486)\r\n- Fix LoadHIP handling of environment variable paths on Windows (#159080)\r\n- Add hipcc compatibility flags to `cpp_extension.py` on Windows (#159790)\r\n- In SDPA via AOTriton, `logsumexp` needs scaling back to natural base (#156903)\r\n- Check stream graph capture status in `memcpy_and_sync` inline function (#158165)\r\n\r\n## XPU\r\n- Fix `cpp_extension` compatibility with `intel-deep-learning-essentials-2025.2` (#161012)\r\n\r\n## JIT\r\n- Make `ErrorReport::CallStack` thread-safe (#160386)\r\n- Fix `RemoveProfileNodesAndSpecializeTypes` handling for `Tensor?` that is resolved to `None` (#161538)\r\n\r\n# Performance\r\n## Optimizer\r\n- Use `addmm` to improve Newton–Schulz orthogonalization in Muon (#161379)\r\n- Avoid stream sync in SWA `AveragedModel.update_parameters()` (#157705)\r\n\r\n## Autograd\r\n- Fix SVD forward-mode AD multiplication priority (#161027)\r\n\r\n## Dynamo\r\n- Recursive `dict` tag optimization for faster guard evaluation (#159183)\r\n\r\n## Inductor\r\n- Improve performance of A16W4 and A16W8 `GEMM` template (#159127, #161148)\r\n- More aggressive persistent reduction (#161055)\r\n- Add a few outer dimension reduction cases for LOAF (#162028)\r\n- Fuse two RoPE kernels into a single kernel and improving runtime efficiency (#161420)\r\n\r\n## Export\r\n- Caching optimizations for placeholder naming pass (#158594)\r\n- Add Static Dispatch Kernel for `fmod.Scalar` and `scale_gradient` (#160654, #160454)\r\n\r\n## CUDA\r\n- Use a nonblocking copy to avoid stream synchronization for GPU tensor indexing with CPU mask (#156384)\r\n- Disable cudagraph GCs by default to improve capture performance (#158649)\r\n\r\n## Release Engineering\r\n- Upgrade to ROCm 6.4.1 and 6.4.2 patch releases (#156636, #158887, #158886, #158651, #159001)\r\n- Migrate RPyTorch ROCm CI to MI325 capacity (#159059, #159649, #161184)\r\n- Enable B200 PyTorch benchmark testing (#158011, #157341)\r\n\r\n## MPS\r\n- Optimize cummin/cummax metal kernels (#156794)\r\n- Speedup `torch.full` for 1-byte types (#158874)\r\n- Speedup `argmax`/`argmin` (#159524)\r\n- Improve performance of `max_pool3d` (#157875)\r\n- Avoid calling tensor ops in `max_pool3d` impl (#157874)\r\n- Move `max_pool2d` to Metal for `stride != 1` (#157876)\r\n\r\n## ROCm\r\n- SDPA now uses AOTriton to 0.11b (#161754)\r\n- `hipblaslt` is used by default on gfx908 for ROCm >= 6.3 (#159092)\r\n- Enable miopen channels last 3d for conv and batchnorm (#160529)\r\n- Remove extra transposes in NHWC convolutions on MIOpen (#160435)\r\n- Remove extra sync in `tensor.item()` (#158486)\r\n- Elementwise and reduction kernel perf improvements (#159430, #159652, #160444, #160466, #161054, #161180, #161181)\r\n- Enable build of `fbgemm_gpu genai` sources for grouped GEMM support (#160676)\r\n\r\n## XPU\r\n- Enable tensor memory descriptor Triton template for Intel GPU (#161600)\r\n\r\n# Documentation\r\n## Python Frontend\r\n- Improve documentation for `torch.lobpcg`, `torch.clone`, `torch.matmul`, `torch.max`, `torch.gather`, `torch.Tensor.scatter_`, `torch.empty_like`, `torch.randint`, `torch.mul`, `torch.min`, `torch.max`. `torch.sort`, `torch.full_like`, `torch.histogramdd`, `torch.hamming_window` (#156139, #157007, #161424, #156153, #157929, #157920, #158050, #158731, #160312, #161539, #162051, #158275, #152682)\r\n- Remove torchscript related sections in serialization docs (#156648)\r\n- Fix typo in `torch.set_float32_matmul_precision` docs (#158191)\r\n- Fix docstring for `torch.nn.utils.clip_grads_with_norm_` to reflect clamping behavior (#158200)\r\n- Fix the Doc issue on the description of edge_order in `torch.gradient` (#159130)\r\n- Add `torch.segment_reduce` docs (#154352)\r\n- Add examples to `torch.is_floating_point` and `torch.is_complex` docs (#161951)\r\n## torch.nn\r\n- Improve description of `padding` for `avg_poolnd` (#159142)\r\n- Improve `CrossEntropyLoss` docs with example of incorrect target specification (#155649)\r\n- Remove redundant dtype conversion in `scaled_dot_product_attention` example (#161613)\r\n\r\n## Optimizer\r\n- Document specific optimizer modules APIs e.g., `torch.optim.adam.Adam`, properly (#158483, #158669, #160194)\r\n- Add note for clarity in Adafactor doc #154862 (#155248)\r\n- Minorly improve `zero_grad` description (#161239)\r\n\r\n## Autograd\r\n- Improve `torch.inference_mode` docs and error message (#161164)\r\n\r\n## Distributed\r\n### c10d\r\n  - Documented barrier collective's interaction with `device_id` (#159389)\r\n  - Fix comment to match logic in `distributed_c10d.py` (#162158)\r\n### DTensor\r\n  - Rewrote doc of `TupleStrategy` (#158132)\r\n  - Documented `redistribute_costs` (#158495)\r\n### FullyShardedDataParallel (FSDP)\r\n  - Removed FSDP1 developer note (#158991)\r\n\r\n## Profiler\r\n- Update PT2 Profiler Torch-Compiled Region Image (#158066)\r\n- Fix Experimental Config Documentatation(#156586)\r\n- Update README (#159816)\r\n\r\n## FX\r\n- Fix typos in `torch/` (`torch/fx/`, #156604)\r\n- Add typing (#158450)\r\n- Fix typo in FX interpreter class docs (#162055)\r\n- Remove allow-untyped-defs from `torch/fx/experimental/migrate_gradual_types/util.py` (#157236)\r\n\r\n## Inductor\r\n- Add documentation for CUDAGraph partition (#159450)\r\n\r\n## Export\r\n- Update docs around draft export, dynamism, and PT2 Archive (#157750)\r\n\r\n## ONNX\r\n- Update export docstring (#162622)\r\n- Delete deprecated tutorial page link (#157310)\r\n- Filter out torchscript sentences (#158850)\r\n- Fix doc typo for `symbolic_multi_out` (#160702)\r\n- `onnx.md` to simplify deprecated entities (#159312)\r\n- Update export docstring and set `fallback=False` by default (#162622, #162726)\r\n- Fix typo in error message: summit -> submit (#162587)\r\n\r\n## Release Engineering\r\n- Add decorator to create deprecation warnings (#155127)\r\n- Add runnable code examples to export documentation (#158506)\r\n- Add developer notes for integrating new backends into PyTorch (#158644)\r\n\r\n## XPU\r\n- Update supported OS to Windows 11 & Ubuntu 24.04/25.04 for Intel client GPU (#161699)\r\n\r\n# Security\r\n## Python Frontend\r\n- Don't store flamegraph to tmp folder (#157374)\r\n\r\n# Developers\r\n## Python Frontend\r\n- Better sample inputs for addmm OpInfo (#160234)\r\n\r\n## Distributed\r\n### c10d\r\n  - Add `waitcounter` for watchdog and heartbeat monitoring thread (#157480)\r\n  - Made `torch.distributed.breakpoint` set a long timeout (#158481)\r\n  - Add `check_rng_sync` util (#160283)\r\n  - Add `FlightRecorder` support for `ProcessGroupXCCL` (#158568)\r\n  - Add `early_stop` kwarg to `torch.utils.checkpoint` (#160781)\r\n### DTensor\r\n  - Wrap sharding prop error with contextual exception (#161574)\r\n  - Add check if tracing for sharding propagation to handle un-hashable keys in DTensor (#160798)\r\n### Device Mesh\r\n  - Add error when users try to slice non contiguous flattened dim submesh (#157523)\r\n  - Make the repr shorter when debug ENV not set (#158822)\r\n### ShardedTensor\r\n  - Make error message descriptive in ShardedTensor creation (#150627, #159423)\r\n### Pipeline Parallelism (PP)\r\n  - Add profiling to schedule execution (#160753)\r\n\r\n## FX\r\n- Consolidate stack trace in Tracer (#156257, #157302, #158266)\r\n- Separate provenance tracking to different levels (#160383, #158399, #158796, #159484)\r\n- Fix `register_foward_pre_hook not supported on ScriptModule` error (#156904)\r\n- Add `__eq__` function to NodeSource (#158170)\r\n- Add `__hash__` function to NodeSource (#158322)\r\n- Cache dict and string rep for better perf in NodeSource (#158372)\r\n- Recover node source from dict (#158373, #158473)\r\n- Include error stacktrace and graph module in `tlparse` error (#158469)\r\n- Add `expanded_def` option for FX printing, render descriptor, update tests (#158708)\r\n- Remove `co_lnotab` in favor of `co_linetable` (#159227)\r\n- Remove duplicate imports (#161685)\r\n- Include Output tensor metadata for `CompiledFxGraph` (#159311)\r\n\r\n## Inductor\r\n- Deprecate `allow_tf32` in `tl.dot(..., allow_tf32=...)`, use `tl.dot(..., input_precision=...)` (#160711)\r\n- Log autotune choices and benchmark result to scuba/chrome trace (#159496)\r\n- Add TLParse artifact for logging runtime of collective and compute ops (#159730)\r\n- Call `jit_post_compile_hook` within Inductor Triton Kernel compile path (#161443)\r\n- Prune configs that require more shared memory than the hardware limit (#161996)\r\n- Runtime estimations using nccl estimator on mm only benchmark mode (#161405)\r\n- Don't use `torch.backends.cuda.matmul.allow_tf32` in Inductor cache key (#159480)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Better error message when no .so/cpp files are found (#156863)\r\n- Clean up old APIs in AOTI c shim (#158400)\r\n- Add Inductor provenance mapping for cpp extern kernel (#161656, #162069)\r\n- Print out error msg when nvcc compiler fails (#157203)\r\n- Add kernel information JSON generation for AOTI packages (#160540)\r\n\r\n## Composability\r\n- Stop suggesting to use `guard_size_oblivious` on data dependent errors (#160510)\r\n- Avoid unnecessary slices resulting in data-dependent errors (#157528)\r\n\r\n## Quantization\r\n- Revamp dtype documentation (#156087)\r\n- Use new type statement to fix public API of types (#158487)\r\n\r\n## Dataloader Frontend\r\n- Add `torch.utils.data` samplers benchmark script (#156974)\r\n- Add `torch.utils.data.Dataloader` benchmark script (#159432)\r\n\r\n## Release Engineering\r\n- Replace `setup.py develop` with `pip install -e` for development builds (#155998, #156027, #156710)  (#156709)\r\n\r\n## XPU\r\n- Upgrade Intel GPU software stack package to intel-deep-learning-essentials-2025.2 (#158733)\r\n",
    "analyzed_at": "2025-11-06 00:00:30"
  },
  "pytorch/pytorch#v2.8.0": {
    "tag_name": "v2.8.0",
    "repo_name": "pytorch/pytorch",
    "new_features": [],
    "improvements": [],
    "bug_fixes": [],
    "other_changes": [],
    "processed_body": "# PyTorch 2.8.0 Release Notes\r\n- [Highlights](#highlights)\r\n- [Backwards Incompatible Changes](#backwards-incompatible-changes)\r\n- [Deprecations](#deprecations)\r\n- [New Features](#new-features)\r\n- [Improvements](#improvements)\r\n- [Bug fixes](#bug-fixes)\r\n- [Performance](#performance)\r\n- [Documentation](#documentation)\r\n- [Developers](#developers)\r\n\r\n\r\n# Highlights\r\n<table>\r\n  <tr>\r\n   <td><strong>Unstable</strong></td>\r\n  </tr>\r\n  <tr>\r\n   <td>torch::stable::Tensor</td>\r\n  </tr>\r\n  <tr>\r\n   <td>High-performance quantized LLM inference on Intel CPUs with native PyTorch</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Experimental Wheel Variant Support</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Inductor CUTLASS backend support</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Inductor Graph Partition for CUDAGraph</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Control Flow Operator Library</td>\r\n  </tr>\r\n  <tr>\r\n   <td>HuggingFace SafeTensors support in PyTorch Distributed Checkpointing</td>\r\n  </tr>\r\n  <tr>\r\n   <td>SYCL support in PyTorch CPP Extension API</td>\r\n  </tr>\r\n  <tr>\r\n   <td>A16W4 on XPU Device</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Hierarchical compilation with torch.compile</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Intel GPU distributed backend (XCCL) support</td>\r\n  </tr>\r\n</table>\r\n\r\nFor more details about these highlighted features, you can look at the [release blogpost](https://pytorch.org/blog/pytorch-2-8/).\r\nBelow are the full release notes for this release.\r\n\r\n# Tracked Regressions\r\n### Windows wheel builds with CUDA 12.9.1 stack overflow during build (#156181)\r\nDue to a bug introduced in CUDA 12.9.1, we are unable to complete full Windows wheel builds with this\r\nversion, as compilation of `torch.segment_reduce()` crashes the build. Thus, we provide a wheel\r\nwithout `torch.segment_reduce()` included in order to sidestep the issue. If you need support\r\nfor `torch.segment_reduce()`, please utilize a different version.\r\n\r\n# Backwards Incompatible Changes\r\n\r\n## CUDA Support\r\n### Removed support for Maxwell and Pascal architectures with CUDA 12.8 and 12.9 builds (#157517, #158478, #158744)\r\nDue to binary size limitations, support for sm50 - sm60 architectures with CUDA 12.8 and 12.9 has\r\nbeen dropped for the 2.8.0 release. If you need support for these architectures, please utilize\r\nCUDA 12.6 instead.\r\n\r\n## Python Frontend\r\n### Calling an op with an input dtype that is unsupported now raises `NotImplementedError` instead of `RuntimeError` (#155470)\r\nPlease update exception handling logic to reflect this.\r\n\r\nIn 2.7.0\r\n```\r\ntry:\r\n    torch.nn.Hardshrink()(torch.randint(0, 5, (10,)))\r\nexcept RuntimeError:\r\n    ...\r\n```\r\n\r\nIn 2.8.0\r\n```\r\ntry:\r\n    torch.nn.Hardshrink()(torch.randint(0, 5, (10,)))\r\nexcept NotImplementedError:\r\n    ...\r\n```\r\n\r\n### Added missing in-place on view check to custom `autograd.Function` (#153094)\r\n\r\nIn 2.8.0, if a custom `autograd.Function` mutates a view of a leaf requiring grad,\r\nit now properly raises an error. Previously, it would silently leak memory.\r\n```\r\n   class Func(torch.autograd.Function):\r\n        @staticmethod\r\n        def forward(ctx, inp):\r\n            inp.add_(1)\r\n            ctx.mark_dirty(inp)\r\n            return inp\r\n\r\n        @staticmethod\r\n        def backward(ctx, gO):\r\n            pass\r\n\r\n    a = torch.tensor([1.0, 2.0], requires_grad=True)\r\n    b = a.view_as(a)\r\n    Func.apply(b)\r\n```\r\nOutput:\r\n\r\nVersion 2.7.0\r\n```\r\nRuns without error, but leaks memory\r\n```\r\nVersion 2.8.0\r\n```\r\nRuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation\r\n```\r\n\r\n### An error is now properly thrown for the out variant of `tensordot` when called with a `requires_grad=True` tensor (#150270)\r\n\r\nPlease avoid passing an out tensor with `requires_grad=True` as gradients cannot be\r\ncomputed for this tensor.\r\n\r\nIn 2.7.0\r\n```\r\na = torch.empty((4, 2), requires_grad=True)\r\nb = torch.empty((2, 4), requires_grad=True)\r\nc = torch.empty((2, 2), requires_grad=True)\r\n# does not error, but gradients for c cannot be computed\r\ntorch.tensordot(a, b, dims=([1], [0]), out=c)\r\n```\r\n\r\nIn 2.8.0\r\n```\r\na = torch.empty((4, 2), requires_grad=True)\r\nb = torch.empty((2, 4), requires_grad=True)\r\nc = torch.empty((2, 2), requires_grad=True)\r\ntorch.tensordot(a, b, dims=([1], [0]), out=c)\r\n# RuntimeError: tensordot(): the 'out' tensor was specified and requires gradients, and\r\n# its shape does not match the expected result. Either remove the 'out' argument, ensure\r\n# it does not require gradients, or make sure its shape matches the expected output.\r\n```\r\n\r\n## torch.compile\r\n### Specialization of a tensor shape with `mark_dynamic` applied now correctly errors (#152661)\r\n\r\nPrior to 2.8, it was possible for a guard on a symbolic shape to be incorrectly\r\nomitted if the symbolic shape evaluation was previously tested with guards\r\nsuppressed (this often happens within the compiler itself). This has been fixed\r\nin 2.8 and usually will just silently \"do the right thing\" and add the correct\r\nguard. However, if the new guard causes a tensor marked with `mark_dynamic` to become\r\nspecialized, this can result in an error. One workaround is to use\r\n`maybe_mark_dynamic` instead of `mark_dynamic`.\r\n\r\nSee the discussion in issue #157921 for more\r\ncontext.\r\n\r\nVersion 2.7.0\r\n```python\r\nimport torch\r\n\r\nembed = torch.randn(2, 8192)\r\nx = torch.zeros(8192)\r\n\r\ntorch._dynamo.mark_dynamic(x, 0)\r\n\r\n@torch.compile\r\ndef f(embedding_indices, x):\r\n    added_tokens_mask = torch.where(x > 10000, 1, 0)\r\n    ei = torch.narrow(embedding_indices, 1, 0, x.size(0))\r\n    return ei.clone()\r\n\r\nf(embed, x)\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nimport torch\r\n\r\nembed = torch.randn(2, 8192)\r\nx = torch.zeros(8192)\r\n\r\ntorch._dynamo.maybe_mark_dynamic(x, 0)\r\n\r\n@torch.compile\r\ndef f(embedding_indices, x):\r\n    added_tokens_mask = torch.where(x > 10000, 1, 0)\r\n    ei = torch.narrow(embedding_indices, 1, 0, x.size(0))\r\n    return ei.clone()\r\n\r\nf(embed, x)\r\n```\r\n\r\n### Several config variables related to `torch.compile` have been renamed or removed\r\n- Dynamo config variable `enable_cpp_framelocals_guard_eval` has changed to no longer have any effect (#151008).\r\n- Inductor config variable `rocm.n_max_profiling_configs` is deprecated (#152341).\r\nInstead, use ck-tile based configs `rocm.ck_max_profiling_configs` and\r\n`rocm.ck_tile_max_profiling_configs`.\r\n- Inductor config variable `autotune_fallback_to_aten` is deprecated (#154331).\r\nInductor will no longer silently fall back to `ATen`. Please add `\"ATEN\"` to\r\n`max_autotune_gemm_backends` for the old behavior.\r\n- Inductor config variables `use_mixed_mm` and `mixed_mm_choice` are deprecated (#152071). Inductor now supports prologue fusion, so there is no need for\r\nspecial cases now.\r\n- Inductor config setting `descriptive_names = False` is deprecated (#151481). Please use one of the other available\r\noptions: `\"torch\"`, `\"original_aten\"`, or `\"inductor_node\"`.\r\n- `custom_op_default_layout_constraint` has moved from inductor config to functorch config (#148104). Please reference it via\r\n`torch._functorch.config.custom_op_default_layout_constraint` instead of\r\n`torch._inductor.config.custom_op_default_layout_constraint`.\r\n- AOTI config variable `emit_current_arch_binary` is deprecated (#155768).\r\n- AOTI config variable `aot_inductor.embed_cubin` has been renamed to `aot_inductor.embed_kernel_binary` (#154412).\r\n- AOTI config variable `aot_inductor.compile_wrapper_with_O0` has been renamed to `compile_wrapper_opt_level` (#148714).\r\n\r\n### Added a stricter aliasing/mutation check for `HigherOrderOperator`s (e.g. `cond`), which will explicitly error out if alias/mutation among inputs and outputs is unsupported (#148953, #146658).\r\n\r\nFor affected `HigherOrderOperator`s, add `.clone()` to aliased outputs to address this.\r\n\r\nVersion 2.7.0\r\n```python\r\nimport torch\r\n\r\n@torch.compile(backend=\"eager\")\r\ndef fn(x):\r\n    return torch.cond(x.sum() > 0, lambda x: x, lambda x: x + 1, [x])\r\n\r\nfn(torch.ones(3))\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nimport torch\r\n\r\n@torch.compile(backend=\"eager\")\r\ndef fn(x):\r\n    return torch.cond(x.sum() > 0, lambda x: x.clone(), lambda x: x + 1, [x])\r\n\r\nfn(torch.ones(3))\r\n```\r\n\r\n### `guard_or_x` and `definitely_x` have been consolidated (#152463)\r\nWe removed `definitely_true` / `definitely_false` and associated APIs, replacing them with\r\n`guard_or_true` / `guard_or_false`, which offer similar functionality and can be used to\r\nachieve the same effect. Please migrate to the latter.\r\n\r\nVersion 2.7.0\r\n```python\r\nfrom torch.fx.experimental.symbolic_shapes import definitely_false, definitely_true\r\n\r\n...\r\nif definitely_true(x):\r\n  ...\r\n\r\nif definitely_false(y):\r\n  ...\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nfrom torch.fx.experimental.symbolic_shapes import guard_or_false, guard_or_true\r\n\r\n...\r\nif guard_or_false(x):\r\n  ...\r\n\r\n# alternatively: if guard_or_false(torch.sym_not(y))\r\nif not guard_or_true(y):\r\n  ...\r\n```\r\n\r\n## torch.export\r\n### `torch.export.export_for_inference` has been removed in favor of `torch.export.export_for_training().run_decompositions()` (#149078)\r\n\r\nVersion 2.7.0\r\n```python\r\nimport torch\r\n\r\n...\r\nexported_program = torch.export.export_for_inference(mod, args, kwargs)\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nimport torch\r\n\r\n...\r\nexported_program = torch.export.export_for_training(\r\n    mod, args, kwargs\r\n).run_decompositions(decomp_table=decomp_table)\r\n```\r\n\r\n### Switched default to `strict=False` in `torch.export.export` and `export_for_training` (#148790, #150941)\r\n\r\nThis differs from the previous release default of `strict=True`. To revert to the old default\r\nbehavior, please explicitly pass `strict=True`.\r\n\r\nVersion 2.7.0\r\n```python\r\nimport torch\r\n\r\n# default behavior is strict=True\r\ntorch.export.export(...)\r\ntorch.export.export_for_training(...)\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nimport torch\r\n\r\n# strict=True must be explicitly passed to get the old behavior\r\ntorch.export.export(..., strict=True)\r\ntorch.export.export_for_training(..., strict=True)\r\n```\r\n\r\n## ONNX\r\n### Default opset in `torch.onnx.export` is now 18 (#156023)\r\n\r\nWhen `dynamo=False`, the default ONNX opset version has been updated from 17 to 18. Users can set `opset_version` to explicitly select an opset version.\r\n\r\nVersion 2.7\r\n\r\n```py\r\n# opset_version=17\r\ntorch.onnx.export(...)\r\n```\r\n\r\nVersion 2.8\r\n\r\n```py\r\n# To preserve the original behavior\r\ntorch.onnx.export(..., opset_version=17)\r\n\r\n# New: opset_version=18\r\ntorch.onnx.export(...)\r\n```\r\n\r\n### The `JitTraceConvertStrategy` has been removed (#152556)\r\n\r\nSupport for JIT traced and scripted modules in the ONNX exporter when `dynamo=True` has been removed. You are encouraged to export an nn.Module directly, or create an `ExportedProgram` using `torch.export` before exporting to ONNX.\r\n\r\n### `onnxscript>=0.3.1` is required for the `dynamo=True` option (#157017)\r\n\r\nYou must upgrade `onnxscript` to version 0.3.1 or higher for it to be compatible with PyTorch 2.8.\r\n\r\n## Build Frontend\r\n### Removed the `torch/types.h` include from `Dispatcher.h` (#149557)\r\nThis can cause build errors in C++ code that implicitly relies on this include (e.g. very old versions of `torchvision`).\r\n\r\nNote that `Dispatcher.h` does not belong as an include from `torch/types.h` and was only present as a\r\nshort-term hack to appease `torchvision`. If you run into `torchvision` build errors, please\r\nupdate to a more recent version of `torchvision` to resolve this.\r\n\r\n### Upgraded `DLPack` to 1.0 (#145000)\r\nAs part of the upgrade, some of the `DLDeviceType` enum values have been renamed. Please switch\r\nto the new names.\r\n\r\nVersion 2.7.0\r\n```\r\nfrom torch.utils.dlpack import DLDeviceType\r\n\r\nd1 = DLDeviceType.kDLGPU\r\nd2 = DLDeviceType.kDLCPUPinned\r\n...\r\n```\r\n\r\nVersion 2.8.0\r\n```\r\nfrom torch.utils.dlpack import DLDeviceType\r\n\r\nd1 = DLDeviceType.kDLCUDA  # formerly kDLGPU\r\nd2 = DLDeviceType.kDLCUDAHost  # formerly kDLCPUPinned\r\n...\r\n```\r\n\r\n### NVTX3 code has been moved from `cmake/public/cuda.cmake` to `cmake/Dependencies.cmake` (#151583)\r\n\r\nThis is a BC-breaking change for the build system interface. Downstream projects that previously got NVTX3 through `cmake/public/cuda.cmake`\r\n(i.e.. calling `find_package(TORCH REQUIRED)`) will now need to explicitly configure NVTX3 support in the library itself (i.e. use `USE_SYSTEM_NVTX=1`).\r\nThe change is to fix the broken behavior where downstream projects couldn't find NVTX3 anyway due to the `PROJECT_SOURCE_DIR` mismatch.\r\n\r\nVersion 2.7.0:\r\n- A downstream project using `-DUSE_SYSTEM_NVTX` would be able to find NVTX3 and `torch::nvtx3` via PyTorch's `cmake/public/cuda.cmake` logic.\r\n- A downstream project NOT using `-DUSE_SYSTEM_NVTX` would encounter build errors with CUDA 12.8 or above.\r\n\r\nVersion 2.8.0:\r\n- A downstream project using `-DUSE_SYSTEM_NVTX` will not be able to find NVTX3 or `torch::nvtx3` via PyTorch's `cmake/public/cuda.cmake`. The downstream project now needs to explicitly find NVTX3 and torch::nvtx3 by implementing the same logic in PyTorch's `cmake/Dependences.cmake`.\r\n- A downstream project NOT using `-DUSE_SYSTEM_NVTX` will proceed building without NVTX unless another part of the build process re-enables NVTX.\r\n\r\n# Deprecations\r\n### MPS support for MacOS Ventura will be removed in 2.9\r\nPyTorch 2.8 is the last release that will support GPU acceleration on MacOS Ventura. In the next\r\nrelease (2.9), MacOS Sonoma (released in Sept. 2023) or above will be required to use the MPS\r\nbackend.\r\n\r\n### `torch.ao.quantization` is deprecated and will be removed in 2.10 (#153892)\r\nTo migrate:\r\n- Eager mode quantization (`torch.ao.quantization.quantize`, `torch.ao.quantization.quantize_dynamic`)\r\n  - Weight-only and dynamic quantization: use `torchao` eager mode `quantize_`.\r\n  - Static quantization: use `torchao` PT2E quantization.\r\n- FX graph mode quantization (`torch.ao.quantization.quantize_fx.prepare_fx`, `torch.ao.quantization.quantize_fx.convert_fx`): use `torchao` PT2E quantization (`torchao.quantization.quantize_pt2e.prepare_pt2e`, `torchao.quantization.quantize_pt2e.convert_pt2e`).\r\n\r\nNote that PT2E quantization has been migrated to `torchao` (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e). See https://github.com/pytorch/ao/issues/2259 and https://docs.pytorch.org/ao/main/quick_start.html#pytorch-2-export-quantization for more details.\r\n\r\n### The `dynamo=False` (current default) option for `torch.onnx.export` is deprecated (#152478, #155580)\r\n\r\nThe default will be `dynamo=True` starting from PyTorch 2.9. You are encouraged to migrate to use the `dynamo=True` option in `torch.onnx.export`. This flag makes `torch.export.export` the default export path, replacing `TorchScript`.\r\n\r\nTo maintain the old behavior, set `dynamo=False` explicitly. You are encouraged to also experiment with the `fallback=True` option that will make the exporter fall back to the `dynamo=False` path if there are errors.\r\n\r\n# New Features\r\n## CUDA\r\n- Support capture of event record and wait in CUDAGraphs for timing (#155372)\r\n\r\n## torch.compile\r\n#### Dynamo\r\n- Added support for hierarchical compilation via `nested_compile_region` (#156449)\r\n- Allow guards to be dropped with custom filter functions via `guard_filter_fn` (#150936)\r\n- Added `dont_skip_tracing` decorator to skip over most Dynamo `skipfiles` rules (#150586)\r\n\r\n#### Inductor\r\n- Added support for mapping a Dynamo graph to multiple different Inductor graphs, which can be optimized separately (#147648, #147038)\r\n\r\n## torch.export\r\n- Introduced [`draft-export`](https://docs.pytorch.org/docs/main/export/draft_export.html), an export variant designed to consistently produce a graph and generate a debugging report of issues encountered during tracing (#152637, #153219, #149465, #153627, #154190, #155744, #150876, #150948, #151051, #151065, #150809, #151797)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Added support for `TorchBind` objects (#150196, #154265)\r\n- Added config variable `aot_inductor.model_name_for_generated_files` for specifying model name (#154129)\r\n\r\n## MPS\r\n- `MPSInductor`: `torch.compile` for Apple GPUs (#150121, #149342, #151449, #151754, #149687, #149180, #149221, #153598, #152788, #153787, #152214, #151152, #155891, #154578, #151272, #151288, #153997, #151871, #153362, #156566, #150661, #153582)\r\n\r\n## ONNX\r\n- Added new strategy `draft_export` (#147529, [docs](https://docs.pytorch.org/docs/main/draft_export.html)) to provide debugging information upon data-dependent / constraint errors when obtaining an `ExportedProgram` with `torch.onnx.export`\r\n\r\n- Added support for symbolic operators in the `dynamo=True` export path (#148905, #149678, #150038, [docs](https://docs.pytorch.org/docs/main/onnx_ops.html#symbolic-operators)). Two operators `torch.onnx.ops.symbolic` and `torch.onnx.ops.symbolic_multi_out` are defined to allow you to create symbolic ONNX operators directly in your PyTorch models. You can use them in a `forward` method:\r\n\r\n```python\r\ndef forward(self, x: torch.Tensor) -> torch.Tensor:\r\n    # Optionally use is_in_onnx_export to control the behavior during onnx export\r\n\r\n    if torch.onnx.is_in_onnx_export():\r\n        # Create a symbolic ONNX operator with the name \"CustomOp\" in the \"custom_domain\" domain.\r\n        # The output tensor will have the specified dtype and shape\r\n        return torch.onnx.ops.symbolic(\r\n            \"custom_domain::CustomOp\",\r\n            (x,),\r\n            dict(attr_key=\"attr_value\"),\r\n            dtype=x.dtype,\r\n            shape=x.shape,\r\n            version=1,\r\n        )\r\n    else:\r\n        return x\r\n```\r\n\r\n## Python Frontend\r\n- Added Generalized Pareto Distribution (GPD) (#135968)\r\n\r\n## Quantization\r\n- Introduced `torch.float4_e2m1fn_x2` dtype (#148791)\r\n\r\n## XPU\r\n- Support Intel distributed backend (XCCL) (#141856)\r\n- Support SYCL kernels through C++ extension (#132945)\r\n\r\n# Improvements\r\n## Build Frontend\r\n- Removed outdated warning about `TORCH_CUDA_ARCH_LIST` (#152715, #155314)\r\n- Made Eigen an optional build dependency (#155955)\r\n- Updated CUTLASS to 3.9.2 (#152779)\r\n\r\n## Composability\r\n- Enhanced custom op support with serializable op profiles and fake registration overrides (#151817, #150807, #150806)\r\n\r\n## C++ Frontend\r\n- Exposed `bicubic` mode for `torch::nn::functional::grid_sample` (#150817)\r\n\r\n## CUDA\r\n- Introduced `no_implicit_headers` mode for `load_inline()` on custom CUDA extensions (#149480)\r\n- Support large batch sizes in SDPA memory-efficient attention backend (#154029, #154663)\r\n- Fixed invalid indexing in SDPA memory-efficient attention backward (#155397)\r\n- Support SDPA attention backends on sm121 (DGX Spark) (#152314)\r\n- Added FP8 row-wise scaled-mm for sm12x (GeForce Blackwell) (#155991)\r\n\r\n## cuDNN\r\n- Updated cuDNN frontend version to 1.12 (#153888)\r\n\r\n## Distributed\r\n#### c10d\r\n- Enhanced `TCPStore` with clone and queuing features (#150966, #151045, #150969, #151485)\r\n- Added a collective time estimator for NCCL comms (#149343)\r\n- Made `getDefaultBackend` more fault tolerant without relying on exceptions (#149152)\r\n- Specified the default PyTorch Distributed backend for MPS (#149538)\r\n- Supported `masterListenFd` in `TCPStoreLibUvBackend` (#150215)\r\n- Used shared stores in gloo (#150230)\r\n- Improved FR dump robustness with all watchdog broadcast wait, reduce dump timeout and shrinked mutex range (#150652, #151329, #155949)\r\n- Added the record of each individual collective being coalesced in FR (#151238)\r\n- Implemented safer book-keeping of NCCL communicators (#150681)\r\n- Clarified behavior of `TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK` (#150682)\r\n- Registered also future allocations in mempool with NCCL (#150684)\r\n- Avoided computing `global_rank` when `group_rank` is used (#151373)\r\n- Exposed NCCL communicator from `ProcessGroupNCCL` via an unsafe API (#152496)\r\n- Added split sizes info dump for uneven all2all bw calculation (#151438)\r\n- Made FR vendor neutral so that other backends can use it and integrated into gloo. (#152585, #152563, #154929, #152614)\r\n- Added `needs_contiguous_strides` tag in functional collective (#153399, #153523)\r\n- Allowed `split_group` to work with non-nccl backends (#152175)\r\n- Simplified `new_subgroups()` by using `new_subgroups_by_enumeration()` (#153843)\r\n- Made only current thread allocate to pool in `ProcessGroupNCCL` (#153990)\r\n- Enabled using `c10::Half` for gloo (#153862)\r\n- Released GIL in PG destructor (#154976)\r\n- Enhanced `get_process_group_ranks()` to accept `group=None` (#154902)\r\n- Skipped updating the default device distributed backend if already registered (#155320)\r\n- Enabled querying the build and runtime NCCL versions (#156305)\r\n- Disabled NCCL NVLS when using deterministic mode (#156381)\r\n- Made `init_process_group` support index-only device id (#156214)\r\n- Support enabling / disabling NaN detector per-`ProcessGroup` (#151723)\r\n- Added support for `reduce_scatter` and `ReduceOp::AVG` in `ProcessGroupGloo` (#149781, #149869)\r\n- Added FP8 support in `ProcessGroupNCCL` (#152706)\r\n- Added `ibverbs` backend in gloo and enabled gloo CUDA when used with a backend that supports `GPUDirect` (#153015, #153425, #153406)\r\n\r\n#### DeviceMesh\r\n- Improved device selection logic (#150897)\r\n\r\n#### DistributedDataParallel (DDP)\r\n- Added one option to allow skipping all reduce unused parameters (#151503)\r\n- Added check on received data to avoid segfault in the DDP reducer (#152143)\r\n- Propagated `use_python_reducer` to C++ reducer (#152735)\r\n`DistributedStateDict` (DSD)\r\n- Supported non-tensor-data `write_size` in planner write items (#149699)\r\n- Skip popping meta device tensors (#153185)\r\n\r\n#### DTensor\r\n- Made `StridedShard` support uneven sharding (#150490)\r\n- Added op support for `torch.cumsum` (#151071)\r\n- Added `DTensor` `redistribute` fwd/bwd datatype conversion to enable `SimpleFSDP` mixed precision training (#150740)\r\n- Added rich support to `torch.distributed.tensor.debug.visualize_sharding` (#152027)\r\n\r\n#### FullyShardedDataParallel2 (FSDP2)\r\n- Added `PrivateUse1` backend in FSDP collectives and device type to pre forward hook (#147260, #149487)\r\n- Added `set_reshard_after_forward` (#149103)\r\n- Allowed different dtypes for no grad model params (#154103)\r\n- Respected `reshard_after_forward=True` for root model and kept root unsharded when not specifying `reshard_after_forward` (#154704, #155319)\r\n- Allowed forcing FSDP2 to always use SUM reductions (#155915)\r\n- Made assert on `all_reduce_event` only if it's not CPU device (#150316)\r\n- Enabled NCCL zero-copy (user buffer registration) for FSDP2 (#150564)\r\n\r\n#### Pipeline Parallelism\r\n- Added schedule visualizer (#150347)\r\n- Allowed unused kwargs in ZB path (#153498)\r\n- Added `get_pipeline_order()` for Gpipe and 1F1B (#155935)\r\n\r\n#### ShardedTensor\r\n- Added support for 0-size `ShardedTensor` and recalculated metadata from `all_gather` (#152583)\r\n\r\n#### TensorParallel\r\n- Added a `ParallelStyle PrepareModuleInputOutput` (#150372)\r\n\r\n#### torchelastic\r\n- No shutdown of rendezvous on leaving workers (#152525)\r\n\r\n## torch.compile\r\n#### Dynamo\r\n- Improved tracing support for python sets, tensor subclasses with `__torch_function__`, and `namedtuple` subclasses (#153150, #149792, #153982)\r\n- Eliminated all Compiled Autograd dynamic shapes recompiles for compile time reduction (#151962, #152119,\r\n#151962, #149707, #149709,\r\n#148799, #148801)\r\n- Added `reason` field to `torch.compiler.disable` (#150341)\r\n- Removed `lru_cache` warnings for functions in the top-level `torch` namespace (#157718)\r\n\r\n#### Inductor\r\n- Added block sparse support for FlexAttention on CPU (#147196)\r\n- Introduced new config settings:\r\n  - `aot_inductor.custom_ops_to_c_shims` and `aot_inductor.custom_op_libs`: allow for specifying custom op C shim (#153968)\r\n  - `max_fusion_buffer_group_pairwise_attempts`: limits fusions to specified node distance (#154688)\r\n  - `cuda.cutlass_enabled_ops`: controls CUTLASS operation selection (#155770)\r\n  - `triton.cudagraph_capture_sizes`: allows specifying certain shapes for which to capture CUDAGraphs; skips CUDAGraphs for other shapes (#156551)\r\n  - `use_static_cuda_launcher`: enables launching compiled triton statically to improve cold start times (#148890)\r\n  - `assume_unaligned_fallback_output`: allows inductor to track unaligned outputs (#150777)\r\n  - `cuda.cutlass_tma_only`: controls whether or not to only use TMA-compatible kernels in CUTLASS (#152815)\r\n  - `static_launch_user_defined_triton_kernels`: enables statically launching user defined triton kernels (#153725)\r\n  - `precompilation_timeout_seconds`: controls the timeout on precompilation (#153788)\r\n  - `disable_decompose_k`: disables new `DecomposeK` GEMM Kernels (#154421)\r\n  - `min_num_split`: sets the minimum number of splits in a split reduction (#155941)\r\n  - `max_autotune_flex_search_space`: allows specifying the size of the search space for flex attention autotuning (#156307)\r\n- Introduced environment variable `LOG_AUTOTUNE_RESULTS` for autotune log (#156254)\r\n- Improved numerical stability of CPU Welford reduction for normalizations (#145061)\r\n\r\n## torch.export\r\n- Improved handling of builtin ops (`min`, `max`, `math.pow`) (#151348)\r\n- Added min/max ranges for dim hints (#149590)\r\n- Allow registering normal classes to `pytree.register_dataclass` (#147752)\r\n- Allow specifying integer inputs as dynamic (#151842)\r\n- Inline `jit.script`ed functions in export (#155180)\r\n- Pretty printing for graph signature (#149710)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Support for device-side TMA (#157241)\r\n- Added `num_runners` to `AOTIModelPackageLoader` (#149364)\r\n\r\n## FX\r\n- Updated codegen compare op to `==` (#150611)\r\n- Map names to operand indices when const folding submodules (#150692)\r\n- Improved stacktrace when tracing (#151029, #155486)\r\n- Support edge dialect ops in `normalize_function` (#143689)\r\n- Fixed path naming in minifier (#153130)\r\n- Added `graph_code_verbose_log` artifact for FX passes (#153775)\r\n- Improved cache key graph printing performance (#151928)\r\n- Added flag to `fx.passes.split_module` to normalize input names (#157793)\r\n\r\n## Linear Algebra Frontend\r\n- Add tensor overlap check for `cross` (#154999)\r\n\r\n## MPS\r\n- Added support for a number of `torch.special` operations as well as `index_copy`, `hardshrink`, `rsub`, `col2im`, and `isin` (#149174, #149203 #149123, #149368, #149378, #149563, #149687, #149705, #149783, #149407/#149680, #150279, #151754, #153786, #154326, #155304, #156263, #155382, #154010, #149816, #152282, #156090, #150060, #151600, #155002, #154671)\r\n- Extended dtype support for:\r\n  * `index_put` with half precision floats (#151869)\r\n  * `ConvTranspose3D` with FP32 and complex (#154696)\r\n  * `log1p` and `sigmoid` with int64 (#151791)\r\n- Compute activation kernels at float precision (#155735)\r\n\r\n## Nested Tensor (NJT)\r\n- Fixed contiguity in NJT string representation (#153529)\r\n\r\n## torch.nn\r\n- Added warning for module full backward hook when no input requires gradient (#155339)\r\n- Added Half support for `weight_norm` on CPU (#148878)\r\n\r\n## ONNX\r\n- Updated ONNX to 1.18 (#152200)\r\n- Added support for opsets (18-23) when `dynamo=True` (#149901, #154596)\r\n- Added float4 support (#151069, #156353)\r\n- Added support for ONNX operators `Attention-23` and `RotaryEmbedding-23` as native PyTorch ops (#156431, #156367, #154745)\r\n- Added support for `torch.scan` (#154513)\r\n- Added support for 0/1-sized example inputs on dynamic dimensions (#155717)\r\n- Add `group_norm` support from opset 21 (#152138)\r\n- Added `asdict` method to `VerificationInfo` class (#151024)\r\n- Support running bfloat16 models with ONNX Runtime (#149646)\r\n- Updated ONNX program doc formatting and improve robustness (#151623)\r\n- Updated `dynamic_shapes` behavior to use `torch.export.dim.DYNAMIC` (#153065)\r\n- Set the name of the producing node using the value name (#155413)\r\n- Improved support for symbolic operators `sym_float`, `sym_not`, `sym_min`, `sym_max` (#153200, #152111, #152196)\r\n\r\n## Optimizer\r\n- Added `TensorLR` variant for fused Adagrad on CPU (#153078)\r\n- Convert tensor lr to 0-dim as needed for the optimizer to normally work (#145674)\r\n- Added `lr_lambda` type check in `MultiplicativeLR` (#151973)\r\n\r\n## Profiler\r\n- Added support for on-demand memory snapshot (#150559)\r\n- Added PT2 compile context to visualizer (#152862)\r\n- Added PT2 to memory snapshot (#152707)\r\n- Added flag to toggle global and local callbacks for annotations (#154932)\r\n- Pass overload names to Kineto (#149333)\r\n- Set duration to -1 for unfinished CPU events (#150131)\r\n- Start at index with most events (#154571)\r\n\r\n## Python Frontend\r\n- Introduced `torch.AcceleratorError` (#152023)\r\n- Implemented `Size.__radd__()` (#152554)\r\n- Updated `get_default_device()` to also respect `torch.device` context manager (#148621)\r\n\r\n## Quantization\r\n- Improved x86 PT2E quantization support with new uint8 ops (pointwise `mul` / `add` / `add_relu` and `batch_norm2d`), qconv1d-relu fusion, and lowering pass (#151112, #152411, #152811, #150751, #149708)\r\n- Support boolean tensor for `torch.fused_moving_avg_obs_fake_quant` on CUDA (#153699)\r\n\r\n## Release Engineering\r\n- Updated gcc11 to gcc13 in manylinux images (#152825, #152825, #150635, #158445)\r\n- Updated to cmake 3.27.2 (#154783, #150549, #153380)\r\n\r\n## ROCm\r\n- Allow user to override default flags for `cpp_extension` (#152432)\r\n- Enabled support for sparse compressed `mm`/`bmm`/`addmm` (#153262)\r\n\r\n## Sparse Frontend\r\n- Enabled sparse compressed tensor invariant checks for `PrivateUse1` extension (#149374)\r\n\r\n## torch.func\r\n- Add batching rules for ops: `torch.Tensor.scatter_add_` (#150543), `torch.matrix_exp` (#155202)\r\n\r\n## XPU\r\n- Support safe softmax, GQA, fp32 causal mask for SDP and increase maximum head dim from 256 to 576 on Intel GPU (#151999, #150992, #152091)\r\n- Add memory reporting to Memory Profiler for Intel GPU (#152842)\r\n- Support Intel GPU profiler toggle functionality (#155135)\r\n- Support distributed memory tracker integration for Intel GPU (#150703)\r\n- Improved error handling and reporting in Intel GPU CMake files (#149353)\r\n- Support `embed_cubin` and `multi_arch_kernel_binary` options in AOTI for Intel GPU (#154514, #153924)\r\n- Added generic and Intel GPU specific Stream and Event in `UserDefineClass` (#155787)\r\n- Support int4 WOQ GEMM on Intel GPU (#137566)\r\n\r\n# Bug Fixes\r\n## Build Frontend\r\n- Support builds with `CMake-4.x` (#150203)\r\n- Fixed fbgemm build with `gcc-12+` (#150847)\r\n- Force build to conform to C++ standard on Windows by adding `/permissive-` flag (#149035)\r\n\r\n## Composability\r\n- Fixed support for 1-element tuple returns from custom ops (#155447)\r\n- Avoid overflow in `torch.norm` for scalar input (#144073)\r\n\r\n## CPU (x86)\r\n- Fixed apparent copy-paste bug in `log_softmax` reduced-precision fp kernel (#156379)\r\n\r\n## CUDA\r\n- Fixed deterministic indexing with broadcast (#154296)\r\n- Fixed `torch.backends.cuda.matmul.allow_fp16_accumulation` crash when using cuBLASLt (#153083)\r\n- Enable `AsyncMM` on Blackwell (#153519)\r\n- Fixed `torch.cuda.MemPool` for multithreaded use-cases (#153356)\r\n- Fix to avoid calling `sum()` on a default-constructed gamma / beta in `layer_norm` (#156600)\r\n- Avoid hangs by erroring out for negative offsets or K=0 in grouped GEMMs (#153226)\r\n- Don't error out in `empty_cache` under mempool context (#158180)\r\n\r\n## Distributed\r\n#### c10d\r\n- Fixed extra CUDA context created by barrier (#149144)\r\n- Fixed the logic to use group rank instead of global rank when possible (#149488)\r\n- Fixed ET trace collection of `all_to_all` (#149485)\r\n- Disabled start event recording for coalesced col and improved profile title (#150863)\r\n- Fixed connection reset in tcp store (#150987, #151052)\r\n- Fixed unused `group` input argument in `new_subgroups()` (#152765, #153798)\r\n- Fixed tcp init when using port 0 (#154156)\r\n- Adopted a vector to temporarily keep the reference to future object to avoid blocking inside Flight Recorder (#156653)\r\n\r\n#### Distributed Checkpointing (DCP)\r\n- Fixed to use global coordinator rank in `broadcast_object` util function (#155912)\r\n\r\n#### DistributedDataParallel (DDP)\r\n- Fixed `DDPOptimizer` issue on static tensor index (#155746)\r\n\r\n#### DTensor\r\n- Fixed `local_map` with multi-threading (#149070)\r\n- Fixed `new_local_tensor` in `redistribute` be None case (#152303)\r\n- Fixed bug visualizing 1D Tensor using rich (#152871)\r\n\r\n#### Pipeline Parallelism\r\n- Optimized memory usage by releasing output memory earlier (#153383)\r\n\r\n#### RPC\r\n- Made torch importable if compiled without `TensorPipe` (#154382)\r\n\r\n#### ShardedTensor\r\n- Fixed sharded tensor `gather` when a local tensor on certain ranks has zero elements (#150914)\r\n\r\n#### TensorParallel\r\n- Turn async-TP applicability asserts back into silent skips (#158736)\r\n\r\n## torch.compile\r\n#### Dynamo\r\n- Eliminated silent incorrectness issues in the Compiled Autograd initial trace (#149014, #155521, #155289, #149336)\r\n- Fixed various tracing errors involving einops, `dict(mapping_proxy)`, and the FlexAttention HOP (#157754, #157515, #157519)\r\n- Fixed unpack hook semantics for memory savings in checkpointing and offloading for Compiled Autograd (#147242, #153300)\r\n- Fixed sources for dataclass defaults and the `lru_cache` method (#158689, #157308)\r\n- Fixed spammy errors when an invalid `TORCH_LOGS` argument is passed (#151678)\r\n\r\n#### Inductor\r\n- Support special kwargs in AMD triton configs (#154605)\r\n- Fixed minifier when one has multiple Python runtimes (#155918)\r\n- Bug fix for int8 GEMM compensation epilogue (#152408)\r\n\r\n## torch.export\r\n- Fixed tracing of the following: `aten.is_nonzero` (#149637), `torch.bincount()` (#152497), `aten.div` (#150874) slicing (#150104), and `attn_mask` (#158618), `aten.to` (#153972), scalar tensor construction (#154661)\r\n- Fixed `dynamic_shapes` spec for kwargs (#148772, #149528, #150103)\r\n- Fixed input bugs in unflattener (#149206, #153474, #153000)\r\n- Fix nonstrict tracing of `functools.partial` (#153408), and higher order ops (#149295)\r\n- Fixed serialization/deserialization of `None` inputs (#150515), `math` module (#154643), `call_torchbind` (#155647), and enums (#154821)\r\n- Fixed state dict modification in run_decompositions (#151436)\r\n- Fixed subclass access custom op bug (#149698)\r\n\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Fixed AOTI `update_constant_buffer` issue (#149243)\r\n- Fixed a memory leak in `model_package_loader` (#152334)\r\n- Don't alloc weights in `AOTIModel` if they don't exist (#152692)\r\n- Fixed state of `ConstantFolding` (#153152)\r\n- Fixed index offset for optional tensor return (#155073)\r\n- Fixed float8 type printing for min/max value printing (#154466)\r\n\r\n## Linear Algebra Frontend\r\n- Fix to workaround LAPACK workspace size being returned as a floating point value (#149682)\r\n- Fixed the accumulation type for `dot` and `gemv` (#152676)\r\n- Fixed `torch.lobpcg` to compute same largest eigenvalue as scipy and `np.linalg.eig` (#152789)\r\n- Fixed 32-bit indexing overflows in `ReducedPrecisionGemV` (#150949)\r\n\r\n## MPS\r\n- Fixed various op support issues: unary/binary ops with `2**32`+ element inputs, binary ops with inputs with different dtypes, ops with complex scalar inputs, `cholesky` decomp, `floor_divide` type promotion, `index_kernel` with large inputs, `lerp` with complex inputs, `logit` with half/bfloat16 inputs, SDPA memory leak, `torch.special.entr`, `tri[ul]`, matrix inversion with `N>1024`, and `where` with non-contiguous `cond` (#152479, #155183, #149233, #151176, #151282, #158239, #152371, #149974, #158237, #146754, #158867, #155184, #152204)\r\n\r\n## torch.nn\r\n- Fixed `load_state_dict` behavior for `nn.LazyLinear` (#147599)\r\n\r\n## ONNX\r\n- Fixed bfloat16 support in `onnx_program` callable (#151121)\r\n- Produce correct dtypes for bf16/f8 in IR TorchTensor (#151259)\r\n- Preserve all legacy exporter params in fallback (#156659)\r\n- Fixed 4D tensor conversion for SDPA (#157509)\r\n\r\n## Optimizer\r\n- Fixed bug where `lr_scheduler` unexpectedly calls `step()` when init argument `last_epoch > -1` (#149312)\r\n- Fixed `CosineAnnealingWarmRestarts` resetting `T_cur` (#151289)\r\n\r\n## Profiler\r\n- Fixed empty C call queue in python tracer (#150370)\r\n- Removed decref from python context in python tracer (#151625)\r\n- Enable all configured activities in CUPTI Range Profiler mode (#154749)\r\n\r\n## Python Frontend\r\n- Fixed segfault during numpy string tensor conversion (#155364)\r\n- Added checks for empty tensor list (#155383)\r\n- Fixed sample validation for `MixtureSameFamily` distribution (#151317)\r\n- Fixed bug where creating a second `Wishart` or `Uniform` distribution modifies constraints on the first (#154361)\r\n- Fix to properly export `torch::utils::tensor_to_numpy` symbol (#154178)\r\n- Fixed `torch.[con]cat[enate]` to avoid crashing on empty inputs (#155460)\r\n- Unify `torch.tensor` and `torch.ops.aten.scalar_tensor` behavior (#158655)\r\n\r\n## Release Engineering\r\n- Checkout optional submodules when publishing a release tarball (#156615)\r\n- Fixed MacOS MP hang in Python-3.12+ (#155698)\r\n- Fixed static functions when using module in MSVC (#148675)\r\n- Fixed VS2022-caused AVX512 illegal instruction issue (#153480)\r\n\r\n## ROCm\r\n- Fixed build error for opportunistic fastatomics with newer compilers (#152841)\r\n\r\n#### TunableOp\r\n- More TF32 support (#149088)\r\n- Fixed offline tuning for `ScaledGEMM` (#149677)\r\n- Fixed row-wise `ScaledGEMM` (#152403)\r\n- Support submatrices in offline tuning for ROCm (#151138)\r\n\r\n## Vulkan\r\n- Fixed `torch.is_vulkan_available()` on Mac (#155595)\r\n\r\n## XPU\r\n- Fixed matmul accuracy when `offset > 0` (#154495)\r\n- Fixed `torch.xpu.is_bf16_supported` to correctly report presence of Intel GPU (#152317)\r\n- Fixed AOT compilation in SYCL C++ extension (#156364)\r\n\r\n# Performance\r\n## Autograd\r\n- Improved autograd streams synchronization (#151079, #157914)\r\n\r\n## CPU (AArch64)\r\n- Compute `ELU(0)` with the cheaper definition (#155765)\r\n\r\n## CUDA\r\n- Improved performance of `cat` and `index_select` (#150233, #152380, #151715)\r\n\r\n## Dataloader Frontend\r\n- Reduced memory usage of `SubsetRandomSampler` by iterating over list instead of tensor (#149126)\r\n\r\n## torch.compile\r\n#### Inductor\r\n- Improved performance of GEMMs (#147315, #151530, #149373, #156174, #155444)\r\n- Added a config option `cpp.use_small_dequant_buffer` to use a small dequant buffer for WOQ int4 GEMM (#156395)\r\n- Support graph partitioning on custom ops (#149782)\r\n- Optimized the heuristics of parallel reduction on CPU (#149614)\r\n\r\n## torch.export\r\n- Cache unflattened graph module (#150030)\r\n\r\n## JIT\r\n- Improved Dead Code Elimination (DCE) compile times for large graphs (#153645)\r\n\r\n## Linear Algebra Frontend\r\n- Introduced fast path for `torch.dot` with float16/bfloat16 (#152799)\r\n\r\n## MPS\r\n- Improved performance of `LayerNorm`, `mm` / `bmm`, `sum` / `prod` reductions, arithmetic ops,\r\nbinary kernels, SDPA, `linear`, and `cumsum` / `cumprod` (#152010, #150541, #150566, #147644, #149730, #152781, #152210, #157494)\r\n\r\n## Python Frontend\r\n- Optimized SVE embedding performance (#150176)\r\n- Improved performance for `torch.tensordot` when contracting to a scalar (#145936)\r\n\r\n## ROCm\r\n- Improved performance of `softmax`, `NLLLoss`, in-place sum, max pooling backward / reductions on NHWC\r\ninputs, max pooling, multi-dimensional reductions, and non-vectorized elementwise kernels (#149076, #149779, #149548, #151230, #152267, #154522, #154619, #155806, #153184)\r\n- Improved scatter add performance on MI250X (#151724)\r\n- Extended vectorized elementwise kernel to more heterogenous tensor types (#149738)\r\n- Use `HipSparseLT` to further accelerate semi-structured (e.g. 2:4) sparsity (#150578)\r\n\r\n## Sparse Frontend\r\n- Skip sparse tensor invariant validation when loading sparse Tensors from external storage (#154610, #154759, #154638)\r\n\r\n## XPU\r\n- Enabled post-op fusion for oneDNN convolution on Intel GPU (#150287)\r\n- Reduced host overhead for Intel GPU by eliminating meaningless API calls (#151111)\r\n- Improved INT4 WOQ GEMM for Intel GPU by introducing a cache mechanism to reduce the oneDNN integration overhead further (#147693)\r\n- Improved scalar tensor case handling in `addmm`, `baddmm` to reduce oneDNN integration overhead on Intel GPU (#153051)\r\n\r\n# Documentation\r\n## Autograd\r\n- Added more details on why `ctx.save_for_backward` is important in note about extending autograd (#153005)\r\n- Updated docs of `torch.autograd.graph.saved_tensors_hooks` to avoid refcycle (#153049)\r\n- Updated gradient behavior note in `torch.amin` and `torch.amax` (#155071)\r\n\r\n## CUDA\r\n- Fixed deprecated amp APIs in docs (#154553)\r\n- Documented device memory apis in correct module (#155126)\r\n- Documented non-pytorch CUDA memory allocation and how to query it (#150880)\r\n\r\n## Distributed\r\n#### c10d\r\n- Documented object collectives limitations (#150815)\r\n- Updated `NCCLConfig` with QOS variable (#151821)\r\n- Documented `get_default_backend_for_device` (#158236)\r\n\r\n#### FullyShardedDataParallel2 (FSDP2)\r\n- Updated `ignored_params` docstring and added unit tests (#149074)\r\n- Added pointer to torchtitan (#153079)\r\n- Added warning for incorrected grad results at world size 1 (#154928)\r\n\r\n## torch.export\r\n- Added mini tutorial for provenance tracking (#152211)\r\n- Updated docs for `Dims` and `ExportGraphSignature` (#156262, #156244)\r\n\r\n## Linear Algebra Frontend\r\n- Addressed ambiguity in docs for `torch.linalg.norm()`'s ord argument of +2 & -2 (#155148)\r\n\r\n## torch.nn\r\n- Improved documentation for transformer-related layers, `nn.RNN`, `nn.functional` loss functions, `interpolate` saturate cast behavior, `ConvTranspose2d` `stride` / `output_size` arguments, and `register_full_backward_hook` (#155123, #153620, #148436, #151304, #150819, #150609, #151785)\r\n- Fixed examples for `nn.Sequential` and `nn.LazyModuleMixin` (#147304, #150596)\r\n- Documented padding size limitations in `nn.modules.padding` and `AvgPoolND` (#155618, #152680)\r\n\r\n## ONNX\r\n- Convert .rst doc files to markdown (#155228, #155556)\r\n- Improved docstring of ONNX symbolic ops (#149668)\r\n- Added note for attention op symbolic function (#156441)\r\n- Added ONNX Dynamo metadata documentation (#155816)\r\n\r\n## Optimizer\r\n- Added scripts to generate plots of `LRScheduler`s (#149189)\r\n- Included other accelerators in capturable docstr for optimizers (#149770)\r\n- Updated SGD documentation to match implementation and document that dampening is skipped in SGD first step (#149884, #152833)\r\n- Fixed doc for `CosineAnnealingLR` to accurately reflect its recursive learning rate schedule (#152936)\r\n- Fixed incorrect citation of authors in `Adafactor` documentation (#145209)\r\n- Added `load_state_dict` hint doc about invoke order work with `lr_scheduler` (#149942)\r\n\r\n## Python Frontend\r\n- Make `torch.Library`'s `kind` have no default value to be consistent with the code (#149390)\r\n- Added 32-bit complex to the list of dtypes (#144590)\r\n- Clarified behavior when integer dtype is used with `requires_grad=True` in `tensor.to()` (#150913)\r\n- Optimized `cdist` param description (#151178)\r\n- Updated serialization docs (#153631)\r\n- Render `Example:` and not `Example::` in docs (#153978)\r\n- Added docstring indicating undefined behavior for converting inf to int (#154781)\r\n- Updated `as_strided()` docs (#149146)\r\n- Fixed `keepdim` param optional description (#151197)\r\n- Clarify that x and dx are mutually exclusive in `torch.trapezoid` docs (#151190)\r\n- Documented `out_dtype` arg for torch GEMM operations (#151704)\r\n- Fixed the basic description of `torch.min()`, `torch.max()`, `torch.all()`, and `torch.any()` (#152658)\r\n- Added `torch.triu_indices`, `torch.tril_indices` dtype description (#150749)\r\n- Optimized `torch.equal` description (#149618)\r\n\r\n## Quantization\r\n- Fixed incorrect `get_default_qat_qconfig` in `prepare_qat_fx` docs (#155100)\r\n\r\n## Release Engineering\r\n- Migrated to new theme (#149331)\r\n\r\n## XPU\r\n- Improved \"Getting Started on Intel GPU\" hardware requirements and notes (#151886)\r\n\r\n# Developers\r\n## Distributed\r\n#### c10d\r\n- Added param recording for uniqueID broadcasting and allgather (#149166)\r\n- Added logger config and more loggings, e.g. `nccl_version` and thread name/id, for flight record in PGNCCL (#150356, #150513, #151048, #152648, #155142, #155754)\r\n- Surfaced error type when we unlink and create named pipe for DumpPipe (#150648)\r\n- Improved the logs on remote shutdown of tcpstore (#153586)\r\n- Enhanced Error Logging in `new_subgroups()` for Non-Divisible World Sizes (#154124)\r\n- Added a logger for all nccl collectives with its time duration when completed (#156008)\r\n- Updated error message in `get_backend()` with more details (#141796)\r\n\r\n#### FullyShardedDataParallel (FSDP1)\r\n- Print FQNs when debugging `FlatParamHandle` (#151336)\r\n\r\n#### FullyShardedDataParallel2 (FSDP2)\r\n- Added FSDP2 logging (#155826)\r\n\r\n#### RPC\r\n- Correctly pass exceptions raised from `rpc_init` to CPython (#154325)\r\n\r\n#### torchelastic\r\n- Added the logging of start of torch elastic workers (#150849)\r\n- Passed event log handler to record function calls (#155457)\r\n- Added `torch.distributed.run` option to provide destination for event logging (#155268)\r\n\r\n## torch.export\r\n- Add `TracingContext` (#149294)\r\n- Monkeypatch fake mode so it errors on invalid custom ops (#149410)\r\n- Fixed torch export docs for preserve_module_call_signature (#151140)\r\n- Improved error message for deserializing custom triton op (#152029)\r\n- Better type annotation for lift_constants_pass (#152072)\r\n- Fixed bug in `detect_attr_assignment` (#151824)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Refactor `AOTInductor` runtime API for Intel GPU (#153929)\r\n- Improve stable library APIs (#152040)\r\n- Add a basic shim and `stable::Tensor is_contiguous` API (#156228)\r\n\r\n## FX\r\n- Gracefully exit minimizer when there is no discrepancy in block mode (#154076)\r\n\r\n## Optimizer\r\n- Improve decorator typing for Optimizer subclasses (#153374)\r\n- Optimize typing in `lr_scheduler.py` (#151219)\r\n- Fixed the type hint of `step()` with default value (#153367)\r\n\r\n## Release Engineering\r\n- Added support for CUDA 12.9 in CI/CD (#154980, #156630, #155895, #155799, #155496, #155340, #155819, #156108)\r\n- Added support for ROCm 6.4 in CI/CD (#151236, #151345, #151355, #153253, #156112)\r\n- Moved CI from ubuntu 20.04 images to ubuntu 22.04 and 24.04 (#154437, #154153, #149142)\r\n- Moved CI to CUDA 12.8 (#154004, #152810, #155087, #148963)\r\n- Enabled CI on MI300 (#150667, #152133, #148394, #153134)\r\n- Enabled CI on H100 (#153900, #154562, #153170, #155861, #155719, #156429)\r\n- Enabled CD for Windows Arm64 (#150310, #152109, #149850, #152099)\r\n- Enabled testing of binary Docker builds in CI/CD (#151483, #151488, #151489, #151706)\r\n- Added smoke test to validate NCCL and cuDNN versions in PyPI packages (#149885, #150194)\r\n- Enabled monitoring for performance tests (#153452, #153453, #153454, #153456)\r\n- Improved benchmarking and performance testing on MacOS (#151721, #151747, #151748, #153897, #155493, #153897, #155493)\r\n- Use `setup-python` from for Mac tests (#155698)\r\n- Removed CUDA 11.8 and 12.4 support in CI/CD (#155509, #154169, #152362, #155555, #154893)\r\n- Removed Anaconda support in CI/CD (#147789, #152338, #152431, #152377, #152433, #147476, #151035, #152860, #152702, #154303, #154309)",
    "analyzed_at": "2025-11-06 00:02:44"
  },
  "pytorch/pytorch#v2.7.1": {
    "tag_name": "v2.7.1",
    "repo_name": "pytorch/pytorch",
    "new_features": [],
    "improvements": [
      {
        "feature_type": "improvement",
        "description": "Improve Error logging in torch.compile",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/149831"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Mark mutable custom operators as cacheable in torch.compile",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/151194"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Improve PyTorch Wheel size due to introduction of addition of 128 bit vectorization",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/148320",
          "https://github.com/pytorch/pytorch/pull/152396"
        ]
      }
    ],
    "bug_fixes": [
      {
        "feature_type": "bug_fix",
        "description": "Fix Excessive cudagraph re-recording for HF LLM models",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/152287"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix torch.compile on some HuggingFace models",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/151154"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix crash due to Exception raised inside torch.autocast",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/152503"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Implement workaround for a graph break with older version einops",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/153925"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix an issue with tensor.view(dtype).copy_(...)",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/151598"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix assertion error due to inductor permuting inputs to flex attention",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/151959"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix performance regression on nanogpt speedrun",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/152641"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix extra CUDA context created by barrier",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/149144"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix an issue related to Distributed Fused Adam in Rocm/APEX when using nccl_ub feature",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/150010"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Add a workaround random hang in non-blocking API mode in NCCL 2.26",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/154055"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix MacOS compilation error with Clang 17",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/151344"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix binary kernels produce incorrect results when one of the tensor arguments is from a wrapped scalar on MPS devices",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/152997"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix fmsub function definition",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/152075"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Floating point exception in torch.mkldnn_max_pool2d",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/151848"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix abnormal inference output with XPU:1 device",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/153067"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Illegal Instruction Caused by grid_sample on Windows",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/152613"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix ONNX decomposition does not preserve custom CompositeImplicitAutograd ops",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/151826"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix error with dynamic linking of libgomp library",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/150084"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix segfault in profiler with Python 3.13",
        "pr_links": [
          "https://github.com/pytorch/pytorch/pull/153848"
        ]
      }
    ],
    "other_changes": [],
    "processed_body": "This release is meant to fix the following issues (regressions / silent correctness):\r\n\r\n### Torch.compile\r\nFix Excessive cudagraph re-recording for HF LLM models ([#152287](https://github.com/pytorch/pytorch/pull/152287)) \r\nFix torch.compile on some HuggingFace models  ([#151154](https://github.com/pytorch/pytorch/pull/151154))\r\nFix crash due to Exception raised inside torch.autocast ([#152503](https://github.com/pytorch/pytorch/pull/152503))\r\nImprove Error logging in torch.compile ([#149831](https://github.com/pytorch/pytorch/pull/149831))\r\nMark mutable custom operators as cacheable in torch.compile ([#151194](https://github.com/pytorch/pytorch/pull/151194))\r\nImplement workaround for a graph break with older version einops ([#153925](https://github.com/pytorch/pytorch/pull/153925))\r\nFix an issue with tensor.view(dtype).copy_(...) ([#151598](https://github.com/pytorch/pytorch/pull/151598))\r\n\r\n### Flex Attention\r\nFix assertion error due to inductor permuting inputs to flex attention ([#151959](https://github.com/pytorch/pytorch/pull/151959))\r\nFix performance regression on nanogpt speedrun ([#152641](https://github.com/pytorch/pytorch/pull/152641))\r\n\r\n### Distributed\r\nFix extra CUDA context created by barrier ([#149144](https://github.com/pytorch/pytorch/pull/149144))\r\nFix an issue related to Distributed Fused Adam in Rocm/APEX when using nccl_ub feature ([#150010](https://github.com/pytorch/pytorch/pull/150010))\r\nAdd a workaround random hang in non-blocking API mode in NCCL 2.26 ([#154055](https://github.com/pytorch/pytorch/pull/154055))\r\n\r\n### MacOS\r\nFix MacOS compilation error with Clang 17 ([#151316](https://github.com/pytorch/pytorch/pull/151344))\r\nFix binary kernels produce incorrect results when one of the tensor arguments is from a wrapped scalar on MPS devices ([#152997](https://github.com/pytorch/pytorch/pull/152997))\r\n\r\n### Other\r\nImprove PyTorch Wheel size due to introduction of addition of 128 bit vectorization ([#148320](https://github.com/pytorch/pytorch/pull/148320)) ([#152396](https://github.com/pytorch/pytorch/pull/152396))\r\nFix fmsub function definition ([#152075](https://github.com/pytorch/pytorch/pull/152075))\r\nFix Floating point exception in torch.mkldnn_max_pool2d ([#151848](https://github.com/pytorch/pytorch/pull/151848))\r\nFix abnormal inference output with XPU:1 device ([#153067](https://github.com/pytorch/pytorch/pull/153067))\r\nFix Illegal Instruction Caused by grid_sample on Windows ([#152613](https://github.com/pytorch/pytorch/pull/152613))\r\nFix ONNX decomposition does not preserve custom CompositeImplicitAutograd ops ([#151826](https://github.com/pytorch/pytorch/pull/151826))\r\nFix error with dynamic linking of libgomp library ([#150084](https://github.com/pytorch/pytorch/pull/150084))\r\nFix segfault in profiler with Python 3.13 ([#153848](https://github.com/pytorch/pytorch/pull/153848))",
    "analyzed_at": "2025-11-06 00:03:10"
  },
  "fastapi/fastapi#0.121.0": {
    "tag_name": "0.121.0",
    "repo_name": "fastapi/fastapi",
    "new_features": [
      {
        "feature_type": "new_feature",
        "description": "Add support for dependencies with scopes, including scope='request' for dependencies with yield that exit before the response is sent",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14262"
        ]
      }
    ],
    "improvements": [],
    "bug_fixes": [],
    "other_changes": [
      {
        "feature_type": "other",
        "description": "Update FastAPI People - Contributors and Translators",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14273"
        ]
      },
      {
        "feature_type": "other",
        "description": "Update FastAPI People - Sponsors",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14274"
        ]
      },
      {
        "feature_type": "other",
        "description": "Update FastAPI GitHub topic repositories",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14280"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump mkdocs-macros-plugin from 1.4.0 to 1.4.1",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14277"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump mkdocstrings[python] from 0.26.1 to 0.30.1",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14279"
        ]
      }
    ],
    "processed_body": "### Features\r\n\r\n* ✨ Add support for dependencies with scopes, support `scope=\"request\"` for dependencies with `yield` that exit before the response is sent. PR [#14262](https://github.com/fastapi/fastapi/pull/14262) by [@tiangolo](https://github.com/tiangolo).\r\n    * New docs: [Dependencies with `yield` - Early exit and `scope`](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-with-yield/#early-exit-and-scope).\r\n\r\n### Internal\r\n\r\n* 👥 Update FastAPI People - Contributors and Translators. PR [#14273](https://github.com/fastapi/fastapi/pull/14273) by [@tiangolo](https://github.com/tiangolo).\r\n* 👥 Update FastAPI People - Sponsors. PR [#14274](https://github.com/fastapi/fastapi/pull/14274) by [@tiangolo](https://github.com/tiangolo).\r\n* 👥 Update FastAPI GitHub topic repositories. PR [#14280](https://github.com/fastapi/fastapi/pull/14280) by [@tiangolo](https://github.com/tiangolo).\r\n* ⬆ Bump mkdocs-macros-plugin from 1.4.0 to 1.4.1. PR [#14277](https://github.com/fastapi/fastapi/pull/14277) by [@dependabot[bot]](https://github.com/apps/dependabot).\r\n* ⬆ Bump mkdocstrings[python] from 0.26.1 to 0.30.1. PR [#14279](https://github.com/fastapi/fastapi/pull/14279) by [@dependabot[bot]](https://github.com/apps/dependabot).\r\n",
    "analyzed_at": "2025-11-06 00:33:35"
  },
  "fastapi/fastapi#0.120.4": {
    "tag_name": "0.120.4",
    "repo_name": "fastapi/fastapi",
    "new_features": [],
    "improvements": [],
    "bug_fixes": [
      {
        "feature_type": "bug_fix",
        "description": "Fix security schemes in OpenAPI when added at the top level app",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14266"
        ]
      }
    ],
    "other_changes": [],
    "processed_body": "### Fixes\r\n\r\n* 🐛 Fix security schemes in OpenAPI when added at the top level app. PR [#14266](https://github.com/fastapi/fastapi/pull/14266) by [@YuriiMotov](https://github.com/YuriiMotov).",
    "analyzed_at": "2025-11-06 00:33:45"
  },
  "fastapi/fastapi#0.120.3": {
    "tag_name": "0.120.3",
    "repo_name": "fastapi/fastapi",
    "new_features": [],
    "improvements": [],
    "bug_fixes": [],
    "other_changes": [
      {
        "feature_type": "other",
        "description": "Reduce internal cyclic recursion in dependencies, from 2 functions calling each other to 1 calling itself",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14256"
        ]
      },
      {
        "feature_type": "other",
        "description": "Refactor internals of dependencies, simplify code and remove get_param_sub_dependant",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14255"
        ]
      },
      {
        "feature_type": "other",
        "description": "Refactor internals of dependencies, simplify using dataclasses",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14254"
        ]
      },
      {
        "feature_type": "other",
        "description": "Update note for untranslated pages",
        "pr_links": [
          "https://github.com/fastapi/fastapi/pull/14257"
        ]
      }
    ],
    "processed_body": "### Refactors\r\n\r\n* ♻️ Reduce internal cyclic recursion in dependencies, from 2 functions calling each other to 1 calling itself. PR [#14256](https://github.com/fastapi/fastapi/pull/14256) by [@tiangolo](https://github.com/tiangolo).\r\n* ♻️ Refactor internals of dependencies, simplify code and remove `get_param_sub_dependant`. PR [#14255](https://github.com/fastapi/fastapi/pull/14255) by [@tiangolo](https://github.com/tiangolo).\r\n* ♻️ Refactor internals of dependencies, simplify using dataclasses. PR [#14254](https://github.com/fastapi/fastapi/pull/14254) by [@tiangolo](https://github.com/tiangolo).\r\n\r\n### Docs\r\n\r\n* 📝 Update note for untranslated pages. PR [#14257](https://github.com/fastapi/fastapi/pull/14257) by [@YuriiMotov](https://github.com/YuriiMotov).",
    "analyzed_at": "2025-11-06 00:33:53"
  },
  "home-assistant/core#2025.10.3": {
    "tag_name": "2025.10.3",
    "repo_name": "home-assistant/core",
    "new_features": [
      {
        "feature_type": "new_feature",
        "description": "Add description placeholders in Uptime Kuma config flow",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154252"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add description placeholders to pyLoad config flow",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154254"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add missing `long_press` entry for trigger_type in strings.json for Hue",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154437"
        ]
      }
    ],
    "improvements": [
      {
        "feature_type": "improvement",
        "description": "Remove redundant state write in Smart Meter Texas",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154126"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "AsusWRT: Pass only online clients to the device list from the API",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154322"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Use `async_schedule_reload` instead of `async_reload` for ZHA",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154397"
        ]
      }
    ],
    "bug_fixes": [
      {
        "feature_type": "bug_fix",
        "description": "PushSafer: Handle empty data section properly",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154109"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix state class for Overkiz water consumption",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154164"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix August integration to handle unavailable OAuth implementation at startup",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154244"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Yale integration to handle unavailable OAuth implementation at startup",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154245"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix home wiziard total increasing sensors returning 0",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154264"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Bluetooth discovery for devices with alternating advertisement names",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154347"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "OpenUV: Fix update by skipping when protection window is null",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154487"
        ]
      }
    ],
    "other_changes": [
      {
        "feature_type": "other",
        "description": "Bump aioasuswrt to 1.5.1 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153209"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump frontend 20251001.4",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154218"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump aioamazondevices to 6.4.1 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154228"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move URL out of Mealie strings.json",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154230"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move URL out of Mastodon strings.json",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154231"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move URL out of Switcher strings.json",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154240"
        ]
      },
      {
        "feature_type": "other",
        "description": "Remove URL from ViCare strings.json",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154243"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move url like strings to placeholders for nibe",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154249"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump pyprobeplus to 1.1.0 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154265"
        ]
      },
      {
        "feature_type": "other",
        "description": "Update Snoo strings.json to include weaning_baseline",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154268"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move Electricity Maps url out of strings.json",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154284"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump aioamazondevices to 6.4.3 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154293"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move URL out of Overkiz Config Flow descriptions",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154315"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move Ecobee authorization URL out of strings.json",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154332"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move URLs out of SABnzbd strings.json",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154333"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move developer url out of strings.json for coinbase setup flow",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154339"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump opower to 0.15.7 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154351"
        ]
      },
      {
        "feature_type": "other",
        "description": "update pysqueezebox lib to 0.13.0 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154358"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move URL out of sfr_box strings.json",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154364"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move translatable URLs out of strings.json for huawei lte",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154368"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump aioairq to 0.4.7 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154386"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump aiocomelit to 1.1.2 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154393"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move igloohome API access URL into constant placeholders",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154430"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move translatable URLs out of strings.json for isy994",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154464"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump aioamazondevices to 6.4.4 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154538"
        ]
      },
      {
        "feature_type": "other",
        "description": "Move URL out of Nuheat strings.json",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154580"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump pyvesync version to 3.1.2 (dependency)",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154650"
        ]
      }
    ],
    "processed_body": "- Bump aioasuswrt to 1.5.1 ([@kennedyshead] - [#153209]) ([asuswrt docs]) (dependency)\r\n- PushSafer: Handle empty data section properly ([@LennartC] - [#154109]) ([pushsafer docs])\r\n- Remove redudant state write in Smart Meter Texas ([@srirams] - [#154126]) ([smart_meter_texas docs])\r\n- Fix state class for Overkiz water consumption ([@Yvan13120] - [#154164]) ([overkiz docs])\r\n- Bump frontend 20251001.4 ([@piitaya] - [#154218]) ([frontend docs])\r\n- Bump aioamazondevices to 6.4.1 ([@chemelli74] - [#154228]) ([alexa_devices docs]) (dependency)\r\n- Move URL out of Mealie strings.json ([@andrew-codechimp] - [#154230]) ([mealie docs])\r\n- Move URL out of Mastodon strings.json ([@andrew-codechimp] - [#154231]) ([mastodon docs])\r\n- Move URL out of Switcher strings.json ([@thecode] - [#154240]) ([switcher_kis docs])\r\n- Remove URL from ViCare strings.json ([@CFenner] - [#154243]) ([vicare docs])\r\n- Fix August integration to handle unavailable OAuth implementation at startup ([@bdraco] - [#154244]) ([august docs])\r\n- Fix Yale integration to handle unavailable OAuth implementation at startup ([@bdraco] - [#154245]) ([yale docs])\r\n- Move url like strings to placeholders for nibe ([@elupus] - [#154249]) ([nibe_heatpump docs])\r\n- Add description placeholders in Uptime Kuma config flow ([@tr4nt0r] - [#154252]) ([uptime_kuma docs])\r\n- Add description placeholders to pyLoad config flow ([@tr4nt0r] - [#154254]) ([pyload docs])\r\n- Fix home wiziard total increasing sensors returning 0 ([@jbouwh] - [#154264]) ([homewizard docs])\r\n- Bump pyprobeplus to 1.1.0 ([@pantherale0] - [#154265]) ([probe_plus docs]) (dependency)\r\n- Update Snoo strings.json to include weaning_baseline ([@dschafer] - [#154268]) ([snoo docs])\r\n- Move Electricity Maps url out of strings.json ([@jpbede] - [#154284]) ([co2signal docs])\r\n- Bump aioamazondevices to 6.4.3 ([@chemelli74] - [#154293]) ([alexa_devices docs]) (dependency)\r\n- Move URL out of Overkiz Config Flow descriptions ([@iMicknl] - [#154315]) ([overkiz docs])\r\n- AsusWRT: Pass only online clients to the device list from the API ([@Vaskivskyi] - [#154322]) ([asuswrt docs])\r\n- Move Ecobee authorization URL out of strings.json ([@ogruendel] - [#154332]) ([ecobee docs])\r\n- Move URLs out of SABnzbd strings.json ([@shaiu] - [#154333]) ([sabnzbd docs])\r\n- Move developer url out of strings.json for coinbase setup flow ([@ogruendel] - [#154339]) ([coinbase docs])\r\n- Fix Bluetooth discovery for devices with alternating advertisement names ([@bdraco] - [#154347]) ([bluetooth docs])\r\n- Bump opower to 0.15.7 ([@tronikos] - [#154351]) ([opower docs]) (dependency)\r\n- update pysqueezebox lib to 0.13.0 ([@wollew] - [#154358]) ([squeezebox docs]) (dependency)\r\n- Move URL out of sfr_box strings.json ([@epenet] - [#154364]) ([sfr_box docs])\r\n- Move translatable URLs out of strings.json for huawei lte ([@sonianuj287] - [#154368]) ([huawei_lte docs])\r\n- Bump aioairq to 0.4.7 ([@Sibgatulin] - [#154386]) ([airq docs]) (dependency)\r\n- Bump aiocomelit to 1.1.2 ([@chemelli74] - [#154393]) ([comelit docs]) (dependency)\r\n- Use `async_schedule_reload` instead of `async_reload` for ZHA ([@puddly] - [#154397]) ([zha docs])\r\n- Move igloohome API access URL into constant placeholders ([@DannyS95] - [#154430]) ([igloohome docs])\r\n- Add missing`long_press` entry for trigger_type in strings.json for Hue ([@mvdwetering] - [#154437]) ([hue docs])\r\n- Move translatable URLs out of strings.json for isy994 ([@sonianuj287] - [#154464]) ([isy994 docs])\r\n- OpenUV: Fix update by skipping when protection window is null ([@wbyoung] - [#154487]) ([openuv docs])\r\n- Bump aioamazondevices to 6.4.4 ([@chemelli74] - [#154538]) ([alexa_devices docs]) (dependency)\r\n- Move URL out of Nuheat strings.json ([@tstabrawa] - [#154580]) ([nuheat docs])\r\n- Bump pyvesync version to 3.1.2 ([@cdnninja] - [#154650]) ([vesync docs]) (dependency)\r\n\r\n[#152881]: https://github.com/home-assistant/core/pull/152881\r\n[#153209]: https://github.com/home-assistant/core/pull/153209\r\n[#153582]: https://github.com/home-assistant/core/pull/153582\r\n[#154109]: https://github.com/home-assistant/core/pull/154109\r\n[#154126]: https://github.com/home-assistant/core/pull/154126\r\n[#154164]: https://github.com/home-assistant/core/pull/154164\r\n[#154181]: https://github.com/home-assistant/core/pull/154181\r\n[#154218]: https://github.com/home-assistant/core/pull/154218\r\n[#154228]: https://github.com/home-assistant/core/pull/154228\r\n[#154230]: https://github.com/home-assistant/core/pull/154230\r\n[#154231]: https://github.com/home-assistant/core/pull/154231\r\n[#154240]: https://github.com/home-assistant/core/pull/154240\r\n[#154243]: https://github.com/home-assistant/core/pull/154243\r\n[#154244]: https://github.com/home-assistant/core/pull/154244\r\n[#154245]: https://github.com/home-assistant/core/pull/154245\r\n[#154249]: https://github.com/home-assistant/core/pull/154249\r\n[#154252]: https://github.com/home-assistant/core/pull/154252\r\n[#154254]: https://github.com/home-assistant/core/pull/154254\r\n[#154264]: https://github.com/home-assistant/core/pull/154264\r\n[#154265]: https://github.com/home-assistant/core/pull/154265\r\n[#154268]: https://github.com/home-assistant/core/pull/154268\r\n[#154284]: https://github.com/home-assistant/core/pull/154284\r\n[#154293]: https://github.com/home-assistant/core/pull/154293\r\n[#154315]: https://github.com/home-assistant/core/pull/154315\r\n[#154322]: https://github.com/home-assistant/core/pull/154322\r\n[#154332]: https://github.com/home-assistant/core/pull/154332\r\n[#154333]: https://github.com/home-assistant/core/pull/154333\r\n[#154339]: https://github.com/home-assistant/core/pull/154339\r\n[#154347]: https://github.com/home-assistant/core/pull/154347\r\n[#154351]: https://github.com/home-assistant/core/pull/154351\r\n[#154358]: https://github.com/home-assistant/core/pull/154358\r\n[#154364]: https://github.com/home-assistant/core/pull/154364\r\n[#154368]: https://github.com/home-assistant/core/pull/154368\r\n[#154386]: https://github.com/home-assistant/core/pull/154386\r\n[#154393]: https://github.com/home-assistant/core/pull/154393\r\n[#154397]: https://github.com/home-assistant/core/pull/154397\r\n[#154430]: https://github.com/home-assistant/core/pull/154430\r\n[#154437]: https://github.com/home-assistant/core/pull/154437\r\n[#154464]: https://github.com/home-assistant/core/pull/154464\r\n[#154487]: https://github.com/home-assistant/core/pull/154487\r\n[#154538]: https://github.com/home-assistant/core/pull/154538\r\n[#154580]: https://github.com/home-assistant/core/pull/154580\r\n[#154650]: https://github.com/home-assistant/core/pull/154650\r\n[@CFenner]: https://github.com/CFenner\r\n[@DannyS95]: https://github.com/DannyS95\r\n[@LennartC]: https://github.com/LennartC\r\n[@Sibgatulin]: https://github.com/Sibgatulin\r\n[@Vaskivskyi]: https://github.com/Vaskivskyi\r\n[@Yvan13120]: https://github.com/Yvan13120\r\n[@andrew-codechimp]: https://github.com/andrew-codechimp\r\n[@bdraco]: https://github.com/bdraco\r\n[@cdnninja]: https://github.com/cdnninja\r\n[@chemelli74]: https://github.com/chemelli74\r\n[@dschafer]: https://github.com/dschafer\r\n[@elupus]: https://github.com/elupus\r\n[@epenet]: https://github.com/epenet\r\n[@frenck]: https://github.com/frenck\r\n[@iMicknl]: https://github.com/iMicknl\r\n[@jbouwh]: https://github.com/jbouwh\r\n[@jpbede]: https://github.com/jpbede\r\n[@kennedyshead]: https://github.com/kennedyshead\r\n[@mvdwetering]: https://github.com/mvdwetering\r\n[@ogruendel]: https://github.com/ogruendel\r\n[@pantherale0]: https://github.com/pantherale0\r\n[@piitaya]: https://github.com/piitaya\r\n[@puddly]: https://github.com/puddly\r\n[@shaiu]: https://github.com/shaiu\r\n[@sonianuj287]: https://github.com/sonianuj287\r\n[@srirams]: https://github.com/srirams\r\n[@thecode]: https://github.com/thecode\r\n[@tr4nt0r]: https://github.com/tr4nt0r\r\n[@tronikos]: https://github.com/tronikos\r\n[@tstabrawa]: https://github.com/tstabrawa\r\n[@wbyoung]: https://github.com/wbyoung\r\n[@wollew]: https://github.com/wollew\r\n[airq docs]: https://www.home-assistant.io/integrations/airq/\r\n[alexa_devices docs]: https://www.home-assistant.io/integrations/alexa_devices/\r\n[asuswrt docs]: https://www.home-assistant.io/integrations/asuswrt/\r\n[august docs]: https://www.home-assistant.io/integrations/august/\r\n[bluetooth docs]: https://www.home-assistant.io/integrations/bluetooth/\r\n[co2signal docs]: https://www.home-assistant.io/integrations/co2signal/\r\n[coinbase docs]: https://www.home-assistant.io/integrations/coinbase/\r\n[comelit docs]: https://www.home-assistant.io/integrations/comelit/\r\n[ecobee docs]: https://www.home-assistant.io/integrations/ecobee/\r\n[frontend docs]: https://www.home-assistant.io/integrations/frontend/\r\n[homewizard docs]: https://www.home-assistant.io/integrations/homewizard/\r\n[huawei_lte docs]: https://www.home-assistant.io/integrations/huawei_lte/\r\n[hue docs]: https://www.home-assistant.io/integrations/hue/\r\n[igloohome docs]: https://www.home-assistant.io/integrations/igloohome/\r\n[isy994 docs]: https://www.home-assistant.io/integrations/isy994/\r\n[mastodon docs]: https://www.home-assistant.io/integrations/mastodon/\r\n[mealie docs]: https://www.home-assistant.io/integrations/mealie/\r\n[nibe_heatpump docs]: https://www.home-assistant.io/integrations/nibe_heatpump/\r\n[nuheat docs]: https://www.home-assistant.io/integrations/nuheat/\r\n[openuv docs]: https://www.home-assistant.io/integrations/openuv/\r\n[opower docs]: https://www.home-assistant.io/integrations/opower/\r\n[overkiz docs]: https://www.home-assistant.io/integrations/overkiz/\r\n[probe_plus docs]: https://www.home-assistant.io/integrations/probe_plus/\r\n[pushsafer docs]: https://www.home-assistant.io/integrations/pushsafer/\r\n[pyload docs]: https://www.home-assistant.io/integrations/pyload/\r\n[sabnzbd docs]: https://www.home-assistant.io/integrations/sabnzbd/\r\n[sfr_box docs]: https://www.home-assistant.io/integrations/sfr_box/\r\n[smart_meter_texas docs]: https://www.home-assistant.io/integrations/smart_meter_texas/\r\n[snoo docs]: https://www.home-assistant.io/integrations/snoo/\r\n[squeezebox docs]: https://www.home-assistant.io/integrations/squeezebox/\r\n[switcher_kis docs]: https://www.home-assistant.io/integrations/switcher_kis/\r\n[uptime_kuma docs]: https://www.home-assistant.io/integrations/uptime_kuma/\r\n[vesync docs]: https://www.home-assistant.io/integrations/vesync/\r\n[vicare docs]: https://www.home-assistant.io/integrations/vicare/\r\n[yale docs]: https://www.home-assistant.io/integrations/yale/\r\n[zha docs]: https://www.home-assistant.io/integrations/zha/",
    "analyzed_at": "2025-11-06 00:34:38"
  },
  "home-assistant/core#2025.10.2": {
    "tag_name": "2025.10.2",
    "repo_name": "home-assistant/core",
    "new_features": [
      {
        "feature_type": "new_feature",
        "description": "Add motion presets to SmartThings AC",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153830"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add plate_count for Miele KM7575",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153868"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add missing entity category and icons for smlight integration",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154131"
        ]
      }
    ],
    "improvements": [
      {
        "feature_type": "improvement",
        "description": "Switch Roborock to v4 of the code login api",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153593"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Gemini: Use default model instead of recommended where applicable",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153676"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Align Shelly `presencezone` entity to the new API/firmware",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153737"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Synology DSM: Don't reinitialize API during configuration",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153739"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Remove stale entities from Alexa Devices",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153759"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "vesync correct fan set modes",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153761"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Handle ESPHome discoveries with uninitialized Z-Wave antennas",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153790"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Catch update exception in AirGradient",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153828"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Limit SimpliSafe websocket connection attempts during startup",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153853"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Handle timeout errors gracefully in Nord Pool services",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153856"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Add missing translation string for Satel Integra subentry type",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153905"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Do not auto-set up ZHA zeroconf discoveries during onboarding",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153914"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Adjust OTBR config entry name for ZBT-2",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153940"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Don't mark ZHA coordinator as via_device with itself",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154004"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Filter out invalid Renault vehicles",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154070"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Z-Wave: ESPHome discovery to update all options",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154113"
        ]
      }
    ],
    "bug_fixes": [
      {
        "feature_type": "bug_fix",
        "description": "Prevent reloading the ZHA integration while adapter firmware is being updated",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/152626"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Wallbox fix Rate Limit issue for multiple chargers",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153074"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix power device classes for system bridge",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153201"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix MQTT Lock state reset to unknown when a reset payload is received",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153647"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix ViCare pressure sensors missing unit of measurement",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153691"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Modbus Fix message_wait_milliseconds is no longer applied",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153709"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix missing google_assistant_sdk.send_text_command",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153735"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix sensors availability check for Alexa Devices",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153743"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Tuya cover position when only control is available",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153803"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix delay_on and auto_off with multiple triggers",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153839"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix PIN validation for Comelit SimpleHome",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153840"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix restore cover state for Comelit SimpleHome",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153887"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "fix typo in icon assignment of AccuWeather integration",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153890"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix HA hardware configuration message for Thread without HAOS",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153933"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix update interval for AccuWeather hourly forecast",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153957"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix empty llm api list in chat log",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153996"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix for multiple Lyrion Music Server on a single Home Assistant server for Squeezebox",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154081"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Shelly RPC cover update when the device is not initialized",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154159"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix shelly remove orphaned entities",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154182"
        ]
      }
    ],
    "other_changes": [
      {
        "feature_type": "other",
        "description": "Updated VRM client and accounted for missing forecasts",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153464"
        ]
      },
      {
        "feature_type": "other",
        "description": "Update frontend to 20251001.2",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/154143"
        ]
      }
    ],
    "processed_body": "- Prevent reloading the ZHA integration while adapter firmware is being updated ([@puddly] - [#152626]) ([zha docs]) ([homeassistant_hardware docs])\r\n- Wallbox fix Rate Limit issue for multiple chargers ([@hesselonline] - [#153074]) ([wallbox docs])\r\n- Fix power device classes for system bridge ([@timmo001] - [#153201]) ([system_bridge docs])\r\n- Bump PyCync to 0.4.1 ([@Kinachi249] - [#153401]) ([cync docs]) (dependency)\r\n- Updated VRM client and accounted for missing forecasts ([@AndyTempel] - [#153464]) ([victron_remote_monitoring docs]) (dependency)\r\n- Bump python-roborock to 2.50.2 ([@Lash-L] - [#153561]) ([roborock docs]) (dependency)\r\n- Bump aioamazondevices to 6.2.8 ([@chemelli74] - [#153592]) ([alexa_devices docs]) (dependency)\r\n- Switch Roborock to v4 of the code login api ([@Lash-L] - [#153593]) ([roborock docs])\r\n- Fix MQTT Lock state reset to unknown when a reset payload is received ([@jbouwh] - [#153647]) ([mqtt docs])\r\n- Gemini: Use default model instead of recommended where applicable ([@Shulyaka] - [#153676]) ([google_generative_ai_conversation docs])\r\n- Fix ViCare pressure sensors missing unit of measurement ([@CFenner] - [#153691]) ([vicare docs])\r\n- Bump pyvesync to 3.1.0 ([@cdnninja] - [#153693]) ([vesync docs]) (dependency)\r\n- Modbus Fix message_wait_milliseconds is no longer applied ([@peetersch] - [#153709]) ([modbus docs])\r\n- Bump opower to 0.15.6 ([@tronikos] - [#153714]) ([opower docs]) (dependency)\r\n- Version bump pydaikin to 2.17.0 ([@fredrike] - [#153718]) ([daikin docs]) (dependency)\r\n- Version bump pydaikin to 2.17.1 ([@fredrike] - [#153726]) ([daikin docs]) (dependency)\r\n- Fix missing google_assistant_sdk.send_text_command ([@tronikos] - [#153735]) ([google_assistant_sdk docs])\r\n- Bump airOS to 0.5.5 using formdata for v6 firmware ([@CoMPaTech] - [#153736]) ([airos docs]) (dependency)\r\n- Align Shelly `presencezone` entity to the new API/firmware ([@bieniu] - [#153737]) ([shelly docs])\r\n- Synology DSM: Don't reinitialize API during configuration ([@oyvindwe] - [#153739]) ([synology_dsm docs])\r\n- Upgrade python-melcloud to 0.1.2 ([@Sander0542] - [#153742]) ([melcloud docs]) (dependency)\r\n- Fix sensors availability check for Alexa Devices ([@chemelli74] - [#153743]) ([alexa_devices docs])\r\n- Bump aioamazondevices to 6.2.9 ([@chemelli74] - [#153756]) ([alexa_devices docs])\r\n- Remove stale entities from Alexa Devices ([@chemelli74] - [#153759]) ([alexa_devices docs])\r\n- vesync correct fan set modes ([@cdnninja] - [#153761]) ([vesync docs])\r\n- Handle ESPHome discoveries with uninitialized Z-Wave antennas ([@balloob] - [#153790]) ([zwave_js docs])\r\n- Fix Tuya cover position when only control is available ([@epenet] - [#153803]) ([tuya docs])\r\n- Bump pySmartThings to 3.3.1 ([@joostlek] - [#153826]) ([smartthings docs]) (dependency)\r\n- Catch update exception in AirGradient ([@joostlek] - [#153828]) ([airgradient docs])\r\n- Add motion presets to SmartThings AC ([@joostlek] - [#153830]) ([smartthings docs])\r\n- Fix delay_on and auto_off with multiple triggers ([@Petro31] - [#153839]) ([template docs])\r\n- Fix PIN validation for Comelit SimpleHome ([@chemelli74] - [#153840]) ([comelit docs])\r\n- Bump aiocomelit to 1.1.1 ([@chemelli74] - [#153843]) ([comelit docs]) (dependency)\r\n- Limit SimpliSafe websocket connection attempts during startup ([@bachya] - [#153853]) ([simplisafe docs])\r\n- Handle timeout errors gracefully in Nord Pool services ([@gjohansson-ST] - [#153856]) ([nordpool docs])\r\n- Add plate_count for Miele KM7575 ([@derytive] - [#153868]) ([miele docs])\r\n- Fix restore cover state for Comelit SimpleHome ([@chemelli74] - [#153887]) ([comelit docs])\r\n- fix typo in icon assignment of AccuWeather integration ([@CFenner] - [#153890]) ([accuweather docs])\r\n- Add missing translation string for Satel Integra subentry type ([@Tommatheussen] - [#153905]) ([satel_integra docs])\r\n- Do not auto-set up ZHA zeroconf discoveries during onboarding ([@TheJulianJES] - [#153914]) ([zha docs])\r\n- `sharkiq` dependency bump to 1.4.2 ([@Freebien] - [#153931]) ([sharkiq docs]) (dependency)\r\n- Fix HA hardware configuration message for Thread without HAOS ([@TheJulianJES] - [#153933]) ([homeassistant_hardware docs])\r\n- Adjust OTBR config entry name for ZBT-2 ([@TheJulianJES] - [#153940]) ([otbr docs])\r\n- Bump pylamarzocco to 2.1.2 ([@zweckj] - [#153950]) ([lamarzocco docs]) (dependency)\r\n- Bump holidays to 0.82 ([@gjohansson-ST] - [#153952]) ([workday docs]) ([holiday docs]) (dependency)\r\n- Fix update interval for AccuWeather hourly forecast ([@bieniu] - [#153957]) ([accuweather docs])\r\n- Bump env-canada to 0.11.3 ([@michaeldavie] - [#153967]) ([environment_canada docs])\r\n- Fix empty llm api list in chat log ([@arturpragacz] - [#153996]) ([conversation docs])\r\n- Don't mark ZHA coordinator as via_device with itself ([@joostlek] - [#154004]) ([zha docs])\r\n- Filter out invalid Renault vehicles ([@epenet] - [#154070]) ([renault docs])\r\n- Bump aioamazondevices to 6.4.0 ([@chemelli74] - [#154071]) ([alexa_devices docs]) (dependency)\r\n- Bump brother to version 5.1.1 ([@bieniu] - [#154080]) ([brother docs]) (dependency)\r\n- Fix for multiple Lyrion Music Server on a single Home Assistant server for Squeezebox ([@peteS-UK] - [#154081]) ([squeezebox docs])\r\n- Z-Wave: ESPHome discovery to update all options ([@balloob] - [#154113]) ([zwave_js docs])\r\n- Add missing entity category and icons for smlight integration ([@piitaya] - [#154131]) ([smlight docs])\r\n- Update frontend to 20251001.2 ([@bramkragten] - [#154143]) ([frontend docs]) (dependency)\r\n- IOmeter bump version v0.2.0 ([@jukrebs] - [#154150]) ([iometer docs]) (dependency)\r\n- Bump deebot-client to 15.1.0 ([@edenhaus] - [#154154]) ([ecovacs docs]) (dependency)\r\n- Fix Shelly RPC cover update when the device is not initialized ([@thecode] - [#154159]) ([shelly docs])\r\n- Fix shelly remove orphaned entities ([@thecode] - [#154182]) ([shelly docs])\r\n\r\n[#152626]: https://github.com/home-assistant/core/pull/152626\r\n[#152881]: https://github.com/home-assistant/core/pull/152881\r\n[#153074]: https://github.com/home-assistant/core/pull/153074\r\n[#153201]: https://github.com/home-assistant/core/pull/153201\r\n[#153401]: https://github.com/home-assistant/core/pull/153401\r\n[#153464]: https://github.com/home-assistant/core/pull/153464\r\n[#153561]: https://github.com/home-assistant/core/pull/153561\r\n[#153582]: https://github.com/home-assistant/core/pull/153582\r\n[#153592]: https://github.com/home-assistant/core/pull/153592\r\n[#153593]: https://github.com/home-assistant/core/pull/153593\r\n[#153647]: https://github.com/home-assistant/core/pull/153647\r\n[#153676]: https://github.com/home-assistant/core/pull/153676\r\n[#153691]: https://github.com/home-assistant/core/pull/153691\r\n[#153693]: https://github.com/home-assistant/core/pull/153693\r\n[#153709]: https://github.com/home-assistant/core/pull/153709\r\n[#153714]: https://github.com/home-assistant/core/pull/153714\r\n[#153718]: https://github.com/home-assistant/core/pull/153718\r\n[#153726]: https://github.com/home-assistant/core/pull/153726\r\n[#153735]: https://github.com/home-assistant/core/pull/153735\r\n[#153736]: https://github.com/home-assistant/core/pull/153736\r\n[#153737]: https://github.com/home-assistant/core/pull/153737\r\n[#153739]: https://github.com/home-assistant/core/pull/153739\r\n[#153742]: https://github.com/home-assistant/core/pull/153742\r\n[#153743]: https://github.com/home-assistant/core/pull/153743\r\n[#153756]: https://github.com/home-assistant/core/pull/153756\r\n[#153759]: https://github.com/home-assistant/core/pull/153759\r\n[#153761]: https://github.com/home-assistant/core/pull/153761\r\n[#153790]: https://github.com/home-assistant/core/pull/153790\r\n[#153803]: https://github.com/home-assistant/core/pull/153803\r\n[#153826]: https://github.com/home-assistant/core/pull/153826\r\n[#153828]: https://github.com/home-assistant/core/pull/153828\r\n[#153830]: https://github.com/home-assistant/core/pull/153830\r\n[#153839]: https://github.com/home-assistant/core/pull/153839\r\n[#153840]: https://github.com/home-assistant/core/pull/153840\r\n[#153843]: https://github.com/home-assistant/core/pull/153843\r\n[#153853]: https://github.com/home-assistant/core/pull/153853\r\n[#153856]: https://github.com/home-assistant/core/pull/153856\r\n[#153868]: https://github.com/home-assistant/core/pull/153868\r\n[#153887]: https://github.com/home-assistant/core/pull/153887\r\n[#153890]: https://github.com/home-assistant/core/pull/153890\r\n[#153905]: https://github.com/home-assistant/core/pull/153905\r\n[#153914]: https://github.com/home-assistant/core/pull/153914\r\n[#153931]: https://github.com/home-assistant/core/pull/153931\r\n[#153933]: https://github.com/home-assistant/core/pull/153933\r\n[#153940]: https://github.com/home-assistant/core/pull/153940\r\n[#153950]: https://github.com/home-assistant/core/pull/153950\r\n[#153952]: https://github.com/home-assistant/core/pull/153952\r\n[#153957]: https://github.com/home-assistant/core/pull/153957\r\n[#153967]: https://github.com/home-assistant/core/pull/153967\r\n[#153996]: https://github.com/home-assistant/core/pull/153996\r\n[#154004]: https://github.com/home-assistant/core/pull/154004\r\n[#154070]: https://github.com/home-assistant/core/pull/154070\r\n[#154071]: https://github.com/home-assistant/core/pull/154071\r\n[#154080]: https://github.com/home-assistant/core/pull/154080\r\n[#154081]: https://github.com/home-assistant/core/pull/154081\r\n[#154113]: https://github.com/home-assistant/core/pull/154113\r\n[#154131]: https://github.com/home-assistant/core/pull/154131\r\n[#154143]: https://github.com/home-assistant/core/pull/154143\r\n[#154150]: https://github.com/home-assistant/core/pull/154150\r\n[#154154]: https://github.com/home-assistant/core/pull/154154\r\n[#154159]: https://github.com/home-assistant/core/pull/154159\r\n[#154182]: https://github.com/home-assistant/core/pull/154182\r\n[@AndyTempel]: https://github.com/AndyTempel\r\n[@CFenner]: https://github.com/CFenner\r\n[@CoMPaTech]: https://github.com/CoMPaTech\r\n[@Freebien]: https://github.com/Freebien\r\n[@Kinachi249]: https://github.com/Kinachi249\r\n[@Lash-L]: https://github.com/Lash-L\r\n[@Petro31]: https://github.com/Petro31\r\n[@Sander0542]: https://github.com/Sander0542\r\n[@Shulyaka]: https://github.com/Shulyaka\r\n[@TheJulianJES]: https://github.com/TheJulianJES\r\n[@Tommatheussen]: https://github.com/Tommatheussen\r\n[@arturpragacz]: https://github.com/arturpragacz\r\n[@bachya]: https://github.com/bachya\r\n[@balloob]: https://github.com/balloob\r\n[@bieniu]: https://github.com/bieniu\r\n[@bramkragten]: https://github.com/bramkragten\r\n[@cdnninja]: https://github.com/cdnninja\r\n[@chemelli74]: https://github.com/chemelli74\r\n[@derytive]: https://github.com/derytive\r\n[@edenhaus]: https://github.com/edenhaus\r\n[@epenet]: https://github.com/epenet\r\n[@fredrike]: https://github.com/fredrike\r\n[@frenck]: https://github.com/frenck\r\n[@gjohansson-ST]: https://github.com/gjohansson-ST\r\n[@hesselonline]: https://github.com/hesselonline\r\n[@jbouwh]: https://github.com/jbouwh\r\n[@joostlek]: https://github.com/joostlek\r\n[@jukrebs]: https://github.com/jukrebs\r\n[@michaeldavie]: https://github.com/michaeldavie\r\n[@oyvindwe]: https://github.com/oyvindwe\r\n[@peetersch]: https://github.com/peetersch\r\n[@peteS-UK]: https://github.com/peteS-UK\r\n[@piitaya]: https://github.com/piitaya\r\n[@puddly]: https://github.com/puddly\r\n[@thecode]: https://github.com/thecode\r\n[@timmo001]: https://github.com/timmo001\r\n[@tronikos]: https://github.com/tronikos\r\n[@zweckj]: https://github.com/zweckj\r\n[accuweather docs]: https://www.home-assistant.io/integrations/accuweather/\r\n[airgradient docs]: https://www.home-assistant.io/integrations/airgradient/\r\n[airos docs]: https://www.home-assistant.io/integrations/airos/\r\n[alexa_devices docs]: https://www.home-assistant.io/integrations/alexa_devices/\r\n[brother docs]: https://www.home-assistant.io/integrations/brother/\r\n[comelit docs]: https://www.home-assistant.io/integrations/comelit/\r\n[conversation docs]: https://www.home-assistant.io/integrations/conversation/\r\n[cync docs]: https://www.home-assistant.io/integrations/cync/\r\n[daikin docs]: https://www.home-assistant.io/integrations/daikin/\r\n[ecovacs docs]: https://www.home-assistant.io/integrations/ecovacs/\r\n[environment_canada docs]: https://www.home-assistant.io/integrations/environment_canada/\r\n[frontend docs]: https://www.home-assistant.io/integrations/frontend/\r\n[google_assistant_sdk docs]: https://www.home-assistant.io/integrations/google_assistant_sdk/\r\n[google_generative_ai_conversation docs]: https://www.home-assistant.io/integrations/google_generative_ai_conversation/\r\n[holiday docs]: https://www.home-assistant.io/integrations/holiday/\r\n[homeassistant_hardware docs]: https://www.home-assistant.io/integrations/homeassistant_hardware/\r\n[iometer docs]: https://www.home-assistant.io/integrations/iometer/\r\n[lamarzocco docs]: https://www.home-assistant.io/integrations/lamarzocco/\r\n[melcloud docs]: https://www.home-assistant.io/integrations/melcloud/\r\n[miele docs]: https://www.home-assistant.io/integrations/miele/\r\n[modbus docs]: https://www.home-assistant.io/integrations/modbus/\r\n[mqtt docs]: https://www.home-assistant.io/integrations/mqtt/\r\n[nordpool docs]: https://www.home-assistant.io/integrations/nordpool/\r\n[opower docs]: https://www.home-assistant.io/integrations/opower/\r\n[otbr docs]: https://www.home-assistant.io/integrations/otbr/\r\n[renault docs]: https://www.home-assistant.io/integrations/renault/\r\n[roborock docs]: https://www.home-assistant.io/integrations/roborock/\r\n[satel_integra docs]: https://www.home-assistant.io/integrations/satel_integra/\r\n[sharkiq docs]: https://www.home-assistant.io/integrations/sharkiq/\r\n[shelly docs]: https://www.home-assistant.io/integrations/shelly/\r\n[simplisafe docs]: https://www.home-assistant.io/integrations/simplisafe/\r\n[smartthings docs]: https://www.home-assistant.io/integrations/smartthings/\r\n[smlight docs]: https://www.home-assistant.io/integrations/smlight/\r\n[squeezebox docs]: https://www.home-assistant.io/integrations/squeezebox/\r\n[synology_dsm docs]: https://www.home-assistant.io/integrations/synology_dsm/\r\n[system_bridge docs]: https://www.home-assistant.io/integrations/system_bridge/\r\n[template docs]: https://www.home-assistant.io/integrations/template/\r\n[tuya docs]: https://www.home-assistant.io/integrations/tuya/\r\n[vesync docs]: https://www.home-assistant.io/integrations/vesync/\r\n[vicare docs]: https://www.home-assistant.io/integrations/vicare/\r\n[victron_remote_monitoring docs]: https://www.home-assistant.io/integrations/victron_remote_monitoring/\r\n[wallbox docs]: https://www.home-assistant.io/integrations/wallbox/\r\n[workday docs]: https://www.home-assistant.io/integrations/workday/\r\n[zha docs]: https://www.home-assistant.io/integrations/zha/\r\n[zwave_js docs]: https://www.home-assistant.io/integrations/zwave_js/",
    "analyzed_at": "2025-11-06 00:35:25"
  },
  "home-assistant/core#2025.10.1": {
    "tag_name": "2025.10.1",
    "repo_name": "home-assistant/core",
    "new_features": [
      {
        "feature_type": "new_feature",
        "description": "Add Roborock mop intensity translations",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153380"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add missing translation for media browser default title",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153430"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Add translation for turbo fan mode in SmartThings",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153445"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "Z-Wave to support migrating from USB to socket with same home ID",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153522"
        ]
      },
      {
        "feature_type": "new_feature",
        "description": "When discovering a Z-Wave adapter, always configure add-on in config flow",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153575"
        ]
      }
    ],
    "improvements": [
      {
        "feature_type": "improvement",
        "description": "Increase onedrive upload chunk size",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153406"
        ]
      },
      {
        "feature_type": "improvement",
        "description": "Debounce updates in Idasen Desk",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153503"
        ]
      }
    ],
    "bug_fixes": [
      {
        "feature_type": "bug_fix",
        "description": "Do not reset the adapter twice during ZHA options flow migration",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153345"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Nord Pool 15 minute interval",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153350"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Explicitly check for None in raw value processing of modbus",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153352"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Set config entry to None in ProxmoxVE",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153357"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Explicit pass in the config entry to coordinator in airtouch4",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153361"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Correct blocking update in ToGrill with lack of notifications",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153387"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Pushover: Handle empty data section properly",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153397"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Disable thinking for unsupported gemini models",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153415"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Satel Integra creating new binary sensors on YAML import",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153419"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix Z-Wave RGB light turn on causing rare ZeroDivisionError",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153422"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix sentence-casing in user-facing strings of slack",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153427"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix missing powerconsumptionreport in Smartthings",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153438"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Disable baudrate bootloader reset for ZBT-2",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153443"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix next event in workday calendar",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153465"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix missing parameter pass in onedrive",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153478"
        ]
      },
      {
        "feature_type": "bug_fix",
        "description": "Fix VeSync zero fan speed handling",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153493"
        ]
      }
    ],
    "other_changes": [
      {
        "feature_type": "other",
        "description": "Bump airOS dependency",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153065"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump airOS module for alternative login url",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153317"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump aiohasupervisor to 0.3.3",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153344"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump python-roborock to 2.49.1",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153396"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump pyportainer 1.0.2",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153326"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump pyportainer 1.0.3",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153413"
        ]
      },
      {
        "feature_type": "other",
        "description": "Update markdown field description in ntfy integration",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153421"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump aiohomekit to 3.2.19",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153423"
        ]
      },
      {
        "feature_type": "other",
        "description": "Update Home Assistant base image to 2025.10.0",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153441"
        ]
      },
      {
        "feature_type": "other",
        "description": "Update OVOEnergy to 3.0.1",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153476"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump pyTibber to 0.32.2",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153484"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump reolink-aio to 0.16.1",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153489"
        ]
      },
      {
        "feature_type": "other",
        "description": "Bump universal-silabs-flasher to 0.0.35",
        "pr_links": [
          "https://github.com/home-assistant/core/pull/153500"
        ]
      }
    ],
    "processed_body": "- Bump airOS dependency ([@CoMPaTech] - [#153065]) ([airos docs]) (dependency)\r\n- Bump airOS module for alternative login url ([@CoMPaTech] - [#153317]) ([airos docs]) (dependency)\r\n- Bump aiohasupervisor to 0.3.3 ([@agners] - [#153344]) ([hassio docs]) (dependency)\r\n- Do not reset the adapter twice during ZHA options flow migration ([@puddly] - [#153345]) ([zha docs])\r\n- Fix Nord Pool 15 minute interval ([@gjohansson-ST] - [#153350]) ([nordpool docs])\r\n- Explicitly check for None in raw value processing of modbus ([@alengwenus] - [#153352]) ([modbus docs])\r\n- Set config entry to None in ProxmoxVE ([@mib1185] - [#153357]) ([proxmoxve docs])\r\n- Explicit pass in the config entry to coordinator in airtouch4 ([@mib1185] - [#153361]) ([airtouch4 docs])\r\n- Add Roborock mop intensity translations ([@starkillerOG] - [#153380]) ([roborock docs])\r\n- Correct blocking update in ToGrill with lack of notifications ([@elupus] - [#153387]) ([togrill docs])\r\n- Bump python-roborock to 2.49.1 ([@Lash-L] - [#153396]) ([roborock docs]) (dependency)\r\n- Pushover: Handle empty data section properly ([@linuxkidd] - [#153397]) ([pushover docs])\r\n- Increase onedrive upload chunk size ([@zweckj] - [#153406]) ([onedrive docs])\r\n- Bump pyportainer 1.0.2 ([@erwindouna] - [#153326]) ([portainer docs]) (dependency)\r\n- Bump pyportainer 1.0.3 ([@erwindouna] - [#153413]) ([portainer docs]) (dependency)\r\n- Disable thinking for unsupported gemini models ([@Shulyaka] - [#153415]) ([google_generative_ai_conversation docs])\r\n- Fix Satel Integra creating new binary sensors on YAML import ([@Tommatheussen] - [#153419]) ([satel_integra docs])\r\n- Update `markdown` field description in ntfy integration ([@tr4nt0r] - [#153421]) ([ntfy docs])\r\n- Fix Z-Wave RGB light turn on causing rare `ZeroDivisionError` ([@TheJulianJES] - [#153422]) ([zwave_js docs])\r\n- Bump aiohomekit to 3.2.19 ([@bdraco] - [#153423]) ([homekit_controller docs]) (dependency)\r\n- Fix sentence-casing in user-facing strings of `slack` ([@NoRi2909] - [#153427]) ([slack docs])\r\n- Add missing translation for media browser default title ([@timmo001] - [#153430]) ([media_source docs])\r\n- Fix missing powerconsumptionreport in Smartthings ([@joostlek] - [#153438]) ([smartthings docs])\r\n- Update Home Assistant base image to 2025.10.0 ([@agners] - [#153441]) (dependency)\r\n- Disable baudrate bootloader reset for ZBT-2 ([@puddly] - [#153443]) ([homeassistant_connect_zbt2 docs])\r\n- Add translation for turbo fan mode in SmartThings ([@joostlek] - [#153445]) ([smartthings docs])\r\n- Fix next event in workday calendar ([@gjohansson-ST] - [#153465]) ([workday docs])\r\n- Update OVOEnergy to 3.0.1 ([@timmo001] - [#153476]) ([ovo_energy docs]) (dependency)\r\n- Fix missing parameter pass in onedrive ([@zweckj] - [#153478]) ([onedrive docs])\r\n- Bump pyTibber to 0.32.2 ([@Danielhiversen] - [#153484]) ([tibber docs]) (dependency)\r\n- Bump reolink-aio to 0.16.1 ([@starkillerOG] - [#153489]) ([reolink docs]) (dependency)\r\n- Fix VeSync zero fan speed handling ([@cdnninja] - [#153493]) ([vesync docs])\r\n- Bump universal-silabs-flasher to 0.0.35 ([@puddly] - [#153500]) ([homeassistant_hardware docs]) (dependency)\r\n- Debounce updates in Idasen Desk ([@abmantis] - [#153503]) ([idasen_desk docs])\r\n- Z-Wave to support migrating from USB to socket with same home ID ([@balloob] - [#153522]) ([zwave_js docs])\r\n- When discovering a Z-Wave adapter, always configure add-on in config flow ([@balloob] - [#153575]) ([zwave_js docs])\r\n\r\n[#152881]: https://github.com/home-assistant/core/pull/152881\r\n[#153065]: https://github.com/home-assistant/core/pull/153065\r\n[#153317]: https://github.com/home-assistant/core/pull/153317\r\n[#153326]: https://github.com/home-assistant/core/pull/153326\r\n[#153344]: https://github.com/home-assistant/core/pull/153344\r\n[#153345]: https://github.com/home-assistant/core/pull/153345\r\n[#153350]: https://github.com/home-assistant/core/pull/153350\r\n[#153352]: https://github.com/home-assistant/core/pull/153352\r\n[#153357]: https://github.com/home-assistant/core/pull/153357\r\n[#153361]: https://github.com/home-assistant/core/pull/153361\r\n[#153380]: https://github.com/home-assistant/core/pull/153380\r\n[#153387]: https://github.com/home-assistant/core/pull/153387\r\n[#153396]: https://github.com/home-assistant/core/pull/153396\r\n[#153397]: https://github.com/home-assistant/core/pull/153397\r\n[#153406]: https://github.com/home-assistant/core/pull/153406\r\n[#153413]: https://github.com/home-assistant/core/pull/153413\r\n[#153415]: https://github.com/home-assistant/core/pull/153415\r\n[#153419]: https://github.com/home-assistant/core/pull/153419\r\n[#153421]: https://github.com/home-assistant/core/pull/153421\r\n[#153422]: https://github.com/home-assistant/core/pull/153422\r\n[#153423]: https://github.com/home-assistant/core/pull/153423\r\n[#153427]: https://github.com/home-assistant/core/pull/153427\r\n[#153430]: https://github.com/home-assistant/core/pull/153430\r\n[#153438]: https://github.com/home-assistant/core/pull/153438\r\n[#153441]: https://github.com/home-assistant/core/pull/153441\r\n[#153443]: https://github.com/home-assistant/core/pull/153443\r\n[#153445]: https://github.com/home-assistant/core/pull/153445\r\n[#153465]: https://github.com/home-assistant/core/pull/153465\r\n[#153476]: https://github.com/home-assistant/core/pull/153476\r\n[#153478]: https://github.com/home-assistant/core/pull/153478\r\n[#153484]: https://github.com/home-assistant/core/pull/153484\r\n[#153489]: https://github.com/home-assistant/core/pull/153489\r\n[#153493]: https://github.com/home-assistant/core/pull/153493\r\n[#153500]: https://github.com/home-assistant/core/pull/153500\r\n[#153503]: https://github.com/home-assistant/core/pull/153503\r\n[#153522]: https://github.com/home-assistant/core/pull/153522\r\n[#153575]: https://github.com/home-assistant/core/pull/153575\r\n[@CoMPaTech]: https://github.com/CoMPaTech\r\n[@Danielhiversen]: https://github.com/Danielhiversen\r\n[@Lash-L]: https://github.com/Lash-L\r\n[@NoRi2909]: https://github.com/NoRi2909\r\n[@Shulyaka]: https://github.com/Shulyaka\r\n[@TheJulianJES]: https://github.com/TheJulianJES\r\n[@Tommatheussen]: https://github.com/Tommatheussen\r\n[@abmantis]: https://github.com/abmantis\r\n[@agners]: https://github.com/agners\r\n[@alengwenus]: https://github.com/alengwenus\r\n[@balloob]: https://github.com/balloob\r\n[@bdraco]: https://github.com/bdraco\r\n[@cdnninja]: https://github.com/cdnninja\r\n[@elupus]: https://github.com/elupus\r\n[@erwindouna]: https://github.com/erwindouna\r\n[@frenck]: https://github.com/frenck\r\n[@gjohansson-ST]: https://github.com/gjohansson-ST\r\n[@joostlek]: https://github.com/joostlek\r\n[@linuxkidd]: https://github.com/linuxkidd\r\n[@mib1185]: https://github.com/mib1185\r\n[@puddly]: https://github.com/puddly\r\n[@starkillerOG]: https://github.com/starkillerOG\r\n[@timmo001]: https://github.com/timmo001\r\n[@tr4nt0r]: https://github.com/tr4nt0r\r\n[@zweckj]: https://github.com/zweckj\r\n[airos docs]: https://www.home-assistant.io/integrations/airos/\r\n[airtouch4 docs]: https://www.home-assistant.io/integrations/airtouch4/\r\n[google_generative_ai_conversation docs]: https://www.home-assistant.io/integrations/google_generative_ai_conversation/\r\n[hassio docs]: https://www.home-assistant.io/integrations/hassio/\r\n[homeassistant_connect_zbt2 docs]: https://www.home-assistant.io/integrations/homeassistant_connect_zbt2/\r\n[homeassistant_hardware docs]: https://www.home-assistant.io/integrations/homeassistant_hardware/\r\n[homekit_controller docs]: https://www.home-assistant.io/integrations/homekit_controller/\r\n[idasen_desk docs]: https://www.home-assistant.io/integrations/idasen_desk/\r\n[media_source docs]: https://www.home-assistant.io/integrations/media_source/\r\n[modbus docs]: https://www.home-assistant.io/integrations/modbus/\r\n[nordpool docs]: https://www.home-assistant.io/integrations/nordpool/\r\n[ntfy docs]: https://www.home-assistant.io/integrations/ntfy/\r\n[onedrive docs]: https://www.home-assistant.io/integrations/onedrive/\r\n[ovo_energy docs]: https://www.home-assistant.io/integrations/ovo_energy/\r\n[portainer docs]: https://www.home-assistant.io/integrations/portainer/\r\n[proxmoxve docs]: https://www.home-assistant.io/integrations/proxmoxve/\r\n[pushover docs]: https://www.home-assistant.io/integrations/pushover/\r\n[reolink docs]: https://www.home-assistant.io/integrations/reolink/\r\n[roborock docs]: https://www.home-assistant.io/integrations/roborock/\r\n[satel_integra docs]: https://www.home-assistant.io/integrations/satel_integra/\r\n[slack docs]: https://www.home-assistant.io/integrations/slack/\r\n[smartthings docs]: https://www.home-assistant.io/integrations/smartthings/\r\n[tibber docs]: https://www.home-assistant.io/integrations/tibber/\r\n[togrill docs]: https://www.home-assistant.io/integrations/togrill/\r\n[vesync docs]: https://www.home-assistant.io/integrations/vesync/\r\n[workday docs]: https://www.home-assistant.io/integrations/workday/\r\n[zha docs]: https://www.home-assistant.io/integrations/zha/\r\n[zwave_js docs]: https://www.home-assistant.io/integrations/zwave_js/",
    "analyzed_at": "2025-11-06 00:36:19"
  }
}