{
  "AUTOMATIC1111/stable-diffusion-webui": {
    "full_name": "AUTOMATIC1111/stable-diffusion-webui",
    "stargazers_count": 157866,
    "size": 36441,
    "topics": [
      "ai",
      "ai-art",
      "deep-learning",
      "diffusion",
      "gradio",
      "image-generation",
      "image2image",
      "img2img",
      "pytorch",
      "stable-diffusion",
      "text2image",
      "torch",
      "txt2img",
      "unstable",
      "upscaling",
      "web"
    ],
    "releases_count": 3,
    "major_releases": [
      {
        "tag_name": "v1.10.1",
        "name": "1.10.1",
        "body": "## 1.10.1\r\n\r\n### Bug Fixes:\r\n* fix image upscale on cpu ([#16275](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16275))",
        "published_at": "2025-02-09T08:00:10Z",
        "target_commitish": "master",
        "version_tuple": [
          1,
          10,
          1
        ],
        "version_key": "1.10.1"
      },
      {
        "tag_name": "v1.10.0",
        "name": "1.10.0",
        "body": "### Features:\r\n* A lot of performance improvements (see below in Performance section)\r\n* Stable Diffusion 3 support ([#16030](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16030), [#16164](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16164), [#16212](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16212))\r\n  * Recommended Euler sampler; DDIM and other timestamp samplers currently not supported\r\n  * T5 text model is disabled by default, enable it in settings\r\n* New schedulers:\r\n  * Align Your Steps ([#15751](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15751))\r\n  * KL Optimal ([#15608](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15608))\r\n  * Normal ([#16149](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16149))\r\n  * DDIM ([#16149](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16149))\r\n  * Simple ([#16142](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16142))\r\n  * Beta ([#16235](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16235))\r\n* New sampler: DDIM CFG++ ([#16035](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16035))\r\n\r\n### Minor:\r\n* Option to skip CFG on early steps ([#15607](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15607))\r\n* Add --models-dir option ([#15742](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15742))\r\n* Allow mobile users to open context menu by using two fingers press ([#15682](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15682))\r\n* Infotext: add Lora name as TI hashes for bundled Textual Inversion ([#15679](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15679))\r\n* Check model's hash after downloading it to prevent corruped downloads ([#15602](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15602))\r\n* More extension tag filtering options ([#15627](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15627))\r\n* When saving AVIF, use JPEG's quality setting ([#15610](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15610))\r\n* Add filename pattern: `[basename]` ([#15978](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15978))\r\n* Add option to enable clip skip for clip L on SDXL ([#15992](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15992))\r\n* Option to prevent screen sleep during generation ([#16001](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16001))\r\n* ToggleLivePriview button in image viewer ([#16065](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16065))\r\n* Remove ui flashing on reloading and fast scrollong ([#16153](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16153))\r\n* option to disable save button log.csv ([#16242](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16242))\r\n\r\n### Extensions and API:\r\n* Add process_before_every_sampling hook ([#15984](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15984))\r\n* Return HTTP 400 instead of 404 on invalid sampler error ([#16140](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16140))\r\n\r\n### Performance:\r\n* [Performance 1/6] use_checkpoint = False ([#15803](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15803))\r\n* [Performance 2/6] Replace einops.rearrange with torch native ops ([#15804](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15804))\r\n* [Performance 4/6] Precompute is_sdxl_inpaint flag ([#15806](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15806))\r\n* [Performance 5/6] Prevent unnecessary extra networks bias backup ([#15816](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15816))\r\n* [Performance 6/6] Add --precision half option to avoid casting during inference ([#15820](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15820))\r\n* [Performance] LDM optimization patches ([#15824](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15824))\r\n* [Performance] Keep sigmas on CPU ([#15823](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15823))\r\n* Check for nans in unet only once, after all steps have been completed\r\n* Added pption to run torch profiler for image generation\r\n\r\n### Bug Fixes:\r\n* Fix for grids without comprehensive infotexts ([#15958](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15958))\r\n* feat: lora partial update precede full update ([#15943](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15943))\r\n* Fix bug where file extension had an extra '.' under some circumstances ([#15893](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15893))\r\n* Fix corrupt model initial load loop ([#15600](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15600))\r\n* Allow old sampler names in API ([#15656](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15656))\r\n* more old sampler scheduler compatibility ([#15681](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15681))\r\n* Fix Hypertile xyz ([#15831](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15831))\r\n* XYZ CSV skipinitialspace ([#15832](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15832))\r\n* fix soft inpainting on mps and xpu, torch_utils.float64 ([#15815](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15815))\r\n* fix extention update when not on main branch ([#15797](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15797))\r\n* update pickle safe filenames\r\n* use relative path for webui-assets css ([#15757](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15757))\r\n* When creating a virtual environment, upgrade pip in webui.bat/webui.sh ([#15750](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15750))\r\n* Fix AttributeError ([#15738](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15738))\r\n* use script_path for webui root in launch_utils ([#15705](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15705))\r\n* fix extra batch mode P Transparency ([#15664](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15664))\r\n* use gradio theme colors in css ([#15680](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15680))\r\n* Fix dragging text within prompt input ([#15657](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15657))\r\n* Add correct mimetype for .mjs files ([#15654](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15654))\r\n* QOL Items - handle metadata issues more cleanly for SD models, Loras and embeddings ([#15632](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15632))\r\n* replace wsl-open with wslpath and explorer.exe ([#15968](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15968))\r\n* Fix SDXL Inpaint ([#15976](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15976))\r\n* multi size grid ([#15988](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15988))\r\n* fix Replace preview ([#16118](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16118))\r\n* Possible fix of wrong scale in weight decomposition ([#16151](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16151))\r\n* Ensure use of python from venv on Mac and Linux ([#16116](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16116))\r\n* Prioritize python3.10 over python3 if both are available on Linux and Mac (with fallback) ([#16092](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16092))\r\n* stoping generation extras ([#16085](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16085))\r\n* Fix SD2 loading ([#16078](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16078), [#16079](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16079))\r\n* fix infotext Lora hashes for hires fix different lora ([#16062](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16062))\r\n* Fix sampler scheduler autocorrection warning ([#16054](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16054))\r\n* fix ui flashing on reloading and fast scrollong ([#16153](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16153))\r\n* fix upscale logic ([#16239](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16239))\r\n* [bug] do not break progressbar on non-job actions (add wrap_gradio_call_no_job) ([#16202](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16202))\r\n* fix OSError: cannot write mode P as JPEG ([#16194](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16194))\r\n\r\n### Other:\r\n* fix changelog #15883 -> #15882 ([#15907](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15907))\r\n* ReloadUI backgroundColor --background-fill-primary ([#15864](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15864))\r\n* Use different torch versions for Intel and ARM Macs ([#15851](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15851))\r\n* XYZ override rework ([#15836](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15836))\r\n* scroll extensions table on overflow ([#15830](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15830))\r\n* img2img batch upload method ([#15817](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15817))\r\n* chore: sync v1.8.0 packages according to changelog ([#15783](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15783))\r\n* Add AVIF MIME type support to mimetype definitions ([#15739](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15739))\r\n* Update imageviewer.js ([#15730](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15730))\r\n* no-referrer ([#15641](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15641))\r\n* .gitignore trace.json ([#15980](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/15980))\r\n* Bump spandrel to 0.3.4 ([#16144](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16144))\r\n* Defunct --max-batch-count ([#16119](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16119))\r\n* docs: update bug_report.yml ([#16102](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16102))\r\n* Maintaining Project Compatibility for Python 3.9 Users Without Upgrade Requirements. ([#16088](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16088), [#16169](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16169), [#16192](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16192))\r\n* Update torch for ARM Macs to 2.3.1 ([#16059](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16059))\r\n* remove deprecated setting dont_fix_second_order_samplers_schedule ([#16061](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16061))\r\n* chore: fix typos ([#16060](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16060))\r\n* shlex.join launch args in console log ([#16170](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16170))\r\n* activate venv .bat ([#16231](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16231))\r\n* add ids to the resize tabs in img2img ([#16218](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16218))\r\n* update installation guide linux ([#16178](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16178))\r\n* Robust sysinfo ([#16173](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16173))\r\n* do not send image size on paste inpaint ([#16180](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16180))\r\n* Fix noisy DS_Store files for MacOS ([#16166](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/16166))\r\n",
        "published_at": "2024-07-27T03:55:24Z",
        "target_commitish": "master",
        "version_tuple": [
          1,
          10,
          0
        ],
        "version_key": "1.10.0"
      }
    ],
    "readme_content": "# Stable Diffusion web UI\r\nA web interface for Stable Diffusion, implemented using Gradio library.\r\n\r\n![](screenshot.png)\r\n\r\n## Features\r\n[Detailed feature showcase with images](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features):\r\n- Original txt2img and img2img modes\r\n- One click install and run script (but you still must install python and git)\r\n- Outpainting\r\n- Inpainting\r\n- Color Sketch\r\n- Prompt Matrix\r\n- Stable Diffusion Upscale\r\n- Attention, specify parts of text that the model should pay more attention to\r\n    - a man in a `((tuxedo))` - will pay more attention to tuxedo\r\n    - a man in a `(tuxedo:1.21)` - alternative syntax\r\n    - select text and press `Ctrl+Up` or `Ctrl+Down` (or `Command+Up` or `Command+Down` if you're on a MacOS) to automatically adjust attention to selected text (code contributed by anonymous user)\r\n- Loopback, run img2img processing multiple times\r\n- X/Y/Z plot, a way to draw a 3 dimensional plot of images with different parameters\r\n- Textual Inversion\r\n    - have as many embeddings as you want and use any names you like for them\r\n    - use multiple embeddings with different numbers of vectors per token\r\n    - works with half precision floating point numbers\r\n    - train embeddings on 8GB (also reports of 6GB working)\r\n- Extras tab with:\r\n    - GFPGAN, neural network that fixes faces\r\n    - CodeFormer, face restoration tool as an alternative to GFPGAN\r\n    - RealESRGAN, neural network upscaler\r\n    - ESRGAN, neural network upscaler with a lot of third party models\r\n    - SwinIR and Swin2SR ([see here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/pull/2092)), neural network upscalers\r\n    - LDSR, Latent diffusion super resolution upscaling\r\n- Resizing aspect ratio options\r\n- Sampling method selection\r\n    - Adjust sampler eta values (noise multiplier)\r\n    - More advanced noise setting options\r\n- Interrupt processing at any time\r\n- 4GB video card support (also reports of 2GB working)\r\n- Correct seeds for batches\r\n- Live prompt token length validation\r\n- Generation parameters\r\n     - parameters you used to generate images are saved with that image\r\n     - in PNG chunks for PNG, in EXIF for JPEG\r\n     - can drag the image to PNG info tab to restore generation parameters and automatically copy them into UI\r\n     - can be disabled in settings\r\n     - drag and drop an image/text-parameters to promptbox\r\n- Read Generation Parameters Button, loads parameters in promptbox to UI\r\n- Settings page\r\n- Running arbitrary python code from UI (must run with `--allow-code` to enable)\r\n- Mouseover hints for most UI elements\r\n- Possible to change defaults/mix/max/step values for UI elements via text config\r\n- Tiling support, a checkbox to create images that can be tiled like textures\r\n- Progress bar and live image generation preview\r\n    - Can use a separate neural network to produce previews with almost none VRAM or compute requirement\r\n- Negative prompt, an extra text field that allows you to list what you don't want to see in generated image\r\n- Styles, a way to save part of prompt and easily apply them via dropdown later\r\n- Variations, a way to generate same image but with tiny differences\r\n- Seed resizing, a way to generate same image but at slightly different resolution\r\n- CLIP interrogator, a button that tries to guess prompt from an image\r\n- Prompt Editing, a way to change prompt mid-generation, say to start making a watermelon and switch to anime girl midway\r\n- Batch Processing, process a group of files using img2img\r\n- Img2img Alternative, reverse Euler method of cross attention control\r\n- Highres Fix, a convenience option to produce high resolution pictures in one click without usual distortions\r\n- Reloading checkpoints on the fly\r\n- Checkpoint Merger, a tab that allows you to merge up to 3 checkpoints into one\r\n- [Custom scripts](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Custom-Scripts) with many extensions from community\r\n- [Composable-Diffusion](https://energy-based-model.github.io/Compositional-Visual-Generation-with-Composable-Diffusion-Models/), a way to use multiple prompts at once\r\n     - separate prompts using uppercase `AND`\r\n     - also supports weights for prompts: `a cat :1.2 AND a dog AND a penguin :2.2`\r\n- No token limit for prompts (original stable diffusion lets you use up to 75 tokens)\r\n- DeepDanbooru integration, creates danbooru style tags for anime prompts\r\n- [xformers](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Xformers), major speed increase for select cards: (add `--xformers` to commandline args)\r\n- via extension: [History tab](https://github.com/yfszzx/stable-diffusion-webui-images-browser): view, direct and delete images conveniently within the UI\r\n- Generate forever option\r\n- Training tab\r\n     - hypernetworks and embeddings options\r\n     - Preprocessing images: cropping, mirroring, autotagging using BLIP or deepdanbooru (for anime)\r\n- Clip skip\r\n- Hypernetworks\r\n- Loras (same as Hypernetworks but more pretty)\r\n- A separate UI where you can choose, with preview, which embeddings, hypernetworks or Loras to add to your prompt\r\n- Can select to load a different VAE from settings screen\r\n- Estimated completion time in progress bar\r\n- API\r\n- Support for dedicated [inpainting model](https://github.com/runwayml/stable-diffusion#inpainting-with-stable-diffusion) by RunwayML\r\n- via extension: [Aesthetic Gradients](https://github.com/AUTOMATIC1111/stable-diffusion-webui-aesthetic-gradients), a way to generate images with a specific aesthetic by using clip images embeds (implementation of [https://github.com/vicgalle/stable-diffusion-aesthetic-gradients](https://github.com/vicgalle/stable-diffusion-aesthetic-gradients))\r\n- [Stable Diffusion 2.0](https://github.com/Stability-AI/stablediffusion) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#stable-diffusion-20) for instructions\r\n- [Alt-Diffusion](https://arxiv.org/abs/2211.06679) support - see [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Features#alt-diffusion) for instructions\r\n- Now without any bad letters!\r\n- Load checkpoints in safetensors format\r\n- Eased resolution restriction: generated image's dimensions must be a multiple of 8 rather than 64\r\n- Now with a license!\r\n- Reorder elements in the UI from settings screen\r\n- [Segmind Stable Diffusion](https://huggingface.co/segmind/SSD-1B) support\r\n\r\n## Installation and Running\r\nMake sure the required [dependencies](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Dependencies) are met and follow the instructions available for:\r\n- [NVidia](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs) (recommended)\r\n- [AMD](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-AMD-GPUs) GPUs.\r\n- [Intel CPUs, Intel GPUs (both integrated and discrete)](https://github.com/openvinotoolkit/stable-diffusion-webui/wiki/Installation-on-Intel-Silicon) (external wiki page)\r\n- [Ascend NPUs](https://github.com/wangshuai09/stable-diffusion-webui/wiki/Install-and-run-on-Ascend-NPUs) (external wiki page)\r\n\r\nAlternatively, use online services (like Google Colab):\r\n\r\n- [List of Online Services](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Online-Services)\r\n\r\n### Installation on Windows 10/11 with NVidia-GPUs using release package\r\n1. Download `sd.webui.zip` from [v1.0.0-pre](https://github.com/AUTOMATIC1111/stable-diffusion-webui/releases/tag/v1.0.0-pre) and extract its contents.\r\n2. Run `update.bat`.\r\n3. Run `run.bat`.\r\n> For more details see [Install-and-Run-on-NVidia-GPUs](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Install-and-Run-on-NVidia-GPUs)\r\n\r\n### Automatic Installation on Windows\r\n1. Install [Python 3.10.6](https://www.python.org/downloads/release/python-3106/) (Newer version of Python does not support torch), checking \"Add Python to PATH\".\r\n2. Install [git](https://git-scm.com/download/win).\r\n3. Download the stable-diffusion-webui repository, for example by running `git clone https://github.com/AUTOMATIC1111/stable-diffusion-webui.git`.\r\n4. Run `webui-user.bat` from Windows Explorer as normal, non-administrator, user.\r\n\r\n### Automatic Installation on Linux\r\n1. Install the dependencies:\r\n```bash\r\n# Debian-based:\r\nsudo apt install wget git python3 python3-venv libgl1 libglib2.0-0\r\n# Red Hat-based:\r\nsudo dnf install wget git python3 gperftools-libs libglvnd-glx\r\n# openSUSE-based:\r\nsudo zypper install wget git python3 libtcmalloc4 libglvnd\r\n# Arch-based:\r\nsudo pacman -S wget git python3\r\n```\r\nIf your system is very new, you need to install python3.11 or python3.10:\r\n```bash\r\n# Ubuntu 24.04\r\nsudo add-apt-repository ppa:deadsnakes/ppa\r\nsudo apt update\r\nsudo apt install python3.11\r\n\r\n# Manjaro/Arch\r\nsudo pacman -S yay\r\nyay -S python311 # do not confuse with python3.11 package\r\n\r\n# Only for 3.11\r\n# Then set up env variable in launch script\r\nexport python_cmd=\"python3.11\"\r\n# or in webui-user.sh\r\npython_cmd=\"python3.11\"\r\n```\r\n2. Navigate to the directory you would like the webui to be installed and execute the following command:\r\n```bash\r\nwget -q https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/webui.sh\r\n```\r\nOr just clone the repo wherever you want:\r\n```bash\r\ngit clone https://github.com/AUTOMATIC1111/stable-diffusion-webui\r\n```\r\n\r\n3. Run `webui.sh`.\r\n4. Check `webui-user.sh` for options.\r\n### Installation on Apple Silicon\r\n\r\nFind the instructions [here](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Installation-on-Apple-Silicon).\r\n\r\n## Contributing\r\nHere's how to add code to this repo: [Contributing](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki/Contributing)\r\n\r\n## Documentation\r\n\r\nThe documentation was moved from this README over to the project's [wiki](https://github.com/AUTOMATIC1111/stable-diffusion-webui/wiki).\r\n\r\nFor the purposes of getting Google and other search engines to crawl the wiki, here's a link to the (not for humans) [crawlable wiki](https://github-wiki-see.page/m/AUTOMATIC1111/stable-diffusion-webui/wiki).\r\n\r\n## Credits\r\nLicenses for borrowed code can be found in `Settings -> Licenses` screen, and also in `html/licenses.html` file.\r\n\r\n- Stable Diffusion - https://github.com/Stability-AI/stablediffusion, https://github.com/CompVis/taming-transformers, https://github.com/mcmonkey4eva/sd3-ref\r\n- k-diffusion - https://github.com/crowsonkb/k-diffusion.git\r\n- Spandrel - https://github.com/chaiNNer-org/spandrel implementing\r\n  - GFPGAN - https://github.com/TencentARC/GFPGAN.git\r\n  - CodeFormer - https://github.com/sczhou/CodeFormer\r\n  - ESRGAN - https://github.com/xinntao/ESRGAN\r\n  - SwinIR - https://github.com/JingyunLiang/SwinIR\r\n  - Swin2SR - https://github.com/mv-lab/swin2sr\r\n- LDSR - https://github.com/Hafiidz/latent-diffusion\r\n- MiDaS - https://github.com/isl-org/MiDaS\r\n- Ideas for optimizations - https://github.com/basujindal/stable-diffusion\r\n- Cross Attention layer optimization - Doggettx - https://github.com/Doggettx/stable-diffusion, original idea for prompt editing.\r\n- Cross Attention layer optimization - InvokeAI, lstein - https://github.com/invoke-ai/InvokeAI (originally http://github.com/lstein/stable-diffusion)\r\n- Sub-quadratic Cross Attention layer optimization - Alex Birch (https://github.com/Birch-san/diffusers/pull/1), Amin Rezaei (https://github.com/AminRezaei0x443/memory-efficient-attention)\r\n- Textual Inversion - Rinon Gal - https://github.com/rinongal/textual_inversion (we're not using his code, but we are using his ideas).\r\n- Idea for SD upscale - https://github.com/jquesnelle/txt2imghd\r\n- Noise generation for outpainting mk2 - https://github.com/parlance-zz/g-diffuser-bot\r\n- CLIP interrogator idea and borrowing some code - https://github.com/pharmapsychotic/clip-interrogator\r\n- Idea for Composable Diffusion - https://github.com/energy-based-model/Compositional-Visual-Generation-with-Composable-Diffusion-Models-PyTorch\r\n- xformers - https://github.com/facebookresearch/xformers\r\n- DeepDanbooru - interrogator for anime diffusers https://github.com/KichangKim/DeepDanbooru\r\n- Sampling in float32 precision from a float16 UNet - marunine for the idea, Birch-san for the example Diffusers implementation (https://github.com/Birch-san/diffusers-play/tree/92feee6)\r\n- Instruct pix2pix - Tim Brooks (star), Aleksander Holynski (star), Alexei A. Efros (no star) - https://github.com/timothybrooks/instruct-pix2pix\r\n- Security advice - RyotaK\r\n- UniPC sampler - Wenliang Zhao - https://github.com/wl-zhao/UniPC\r\n- TAESD - Ollin Boer Bohan - https://github.com/madebyollin/taesd\r\n- LyCORIS - KohakuBlueleaf\r\n- Restart sampling - lambertae - https://github.com/Newbeeer/diffusion_restart_sampling\r\n- Hypertile - tfernd - https://github.com/tfernd/HyperTile\r\n- Initial Gradio script - posted on 4chan by an Anonymous user. Thank you Anonymous user.\r\n- (You)\r\n",
    "ci_configs": {
      ".github/workflows/on_pull_request.yaml": "https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/.github/workflows/on_pull_request.yaml",
      ".github/workflows/run_tests.yaml": "https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/.github/workflows/run_tests.yaml",
      ".github/workflows/warns_merge_master.yml": "https://raw.githubusercontent.com/AUTOMATIC1111/stable-diffusion-webui/master/.github/workflows/warns_merge_master.yml"
    },
    "processed_at": "2025-11-05 23:46:42"
  },
  "huggingface/transformers": {
    "full_name": "huggingface/transformers",
    "stargazers_count": 152117,
    "size": 395232,
    "topics": [
      "audio",
      "deep-learning",
      "deepseek",
      "gemma",
      "glm",
      "hacktoberfest",
      "llm",
      "machine-learning",
      "model-hub",
      "natural-language-processing",
      "nlp",
      "pretrained-models",
      "python",
      "pytorch",
      "pytorch-transformers",
      "qwen",
      "speech-recognition",
      "transformer",
      "vlm"
    ],
    "releases_count": 30,
    "major_releases": [
      {
        "tag_name": "v4.57.1",
        "name": "Patch release v4.57.1",
        "body": "This patch most notably fixes an issue with an optional dependency (`optax`), which resulted in parsing errors with `poetry`. It contains the following fixes:\r\n\r\n- [fix optax dep issue](https://github.com/huggingface/transformers/commit/0645c9ec3188e000aecf5060e2cdabcc156bb794)\r\n- [remove offload_state_dict from kwargs](https://github.com/huggingface/transformers/commit/a92b1e8a45e1863b95c5e2caa12f5597aee80279)\r\n- Fix bnb fsdp loading for pre-quantized checkpoint (#41415)\r\n- Fix tests fsdp (#41422)\r\n- Fix trainer for py3.9 (#41359)",
        "published_at": "2025-10-14T15:39:34Z",
        "target_commitish": "main",
        "version_tuple": [
          4,
          57,
          1
        ],
        "version_key": "4.57.1"
      },
      {
        "tag_name": "v4.57.0",
        "name": "v4.57.0: Qwen3-Next, Vault Gemma, Qwen3 VL, LongCat Flash, Flex OLMO, LFM2 VL, BLT, Qwen3 OMNI MoE, Parakeet, EdgeTAM, OLMO3",
        "body": "## New model additions\r\n\r\n### Qwen3 Next\r\n\r\n<img width=\"1200\" height=\"511\" alt=\"image\" src=\"https://github.com/user-attachments/assets/3abad6c4-5650-412d-a831-f8a30a5d962e\" />\r\n\r\nThe Qwen3-Next series represents the Qwen team's next-generation foundation models, optimized for extreme context length and large-scale parameter efficiency. \r\nThe series introduces a suite of architectural innovations designed to maximize performance while minimizing computational cost:\r\n- **Hybrid Attention**: Replaces standard attention with the combination of **Gated DeltaNet** and **Gated Attention**, enabling efficient context modeling.  \r\n- **High-Sparsity MoE**: Achieves an extreme low activation ratio as 1:50 in MoE layers ‚Äî drastically reducing FLOPs per token while preserving model capacity.\r\n- **Multi-Token Prediction(MTP)**: Boosts pretraining model performance, and accelerates inference.\r\n- **Other Optimizations**: Includes techniques such as **zero-centered and weight-decayed layernorm**, **Gated Attention**, and other stabilizing enhancements for robust training.  \r\n\r\nBuilt on this architecture, they trained and open-sourced Qwen3-Next-80B-A3B ‚Äî 80B total parameters, only 3B active ‚Äî achieving extreme sparsity and efficiency.\r\n\r\nDespite its ultra-efficiency, it outperforms Qwen3-32B on downstream tasks ‚Äî while requiring **less than 1/10 of the training cost**. \r\nMoreover, it delivers over **10x higher inference throughput** than Qwen3-32B when handling contexts longer than 32K tokens.\r\n\r\nFor more details, please visit their blog [Qwen3-Next](qwen3_next) ([blog post](https://qwenlm.github.io/blog/qwen3_next/)).\r\n\r\n* Adding Support for Qwen3-Next  by @bozheng-hit in #40771\r\n\r\n### Vault Gemma\r\n\r\n<img width=\"1282\" height=\"392\" alt=\"image\" src=\"https://github.com/user-attachments/assets/9412905b-4083-4994-9000-aa0dbf97eb6f\" />\r\n\r\n[VaultGemma](https://services.google.com/fh/files/blogs/vaultgemma_tech_report.pdf) is a text-only decoder model derived from [Gemma 2](https://huggingface.co/docs/transformers/en/model_doc/gemma2), notably it drops the norms after the Attention and MLP blocks, and uses full attention for all layers instead of alternating between full attention and local sliding attention. VaultGemma is available as a pretrained model with 1B parameters that uses a 1024 token sequence length.\r\n\r\nVaultGemma was trained from scratch with sequence-level differential privacy (DP). Its training data includes the same mixture as the [Gemma 2 models](https://huggingface.co/collections/google/gemma-2-release-667d6600fd5220e7b967f315), consisting of a number of documents of varying lengths. Additionally, it is trained using [DP stochastic gradient descent (DP-SGD)](https://arxiv.org/abs/1607.00133) and provides a (Œµ ‚â§ 2.0, Œ¥ ‚â§ 1.1e-10)-sequence-level DP guarantee, where a sequence consists of 1024 consecutive tokens extracted from heterogeneous data sources. Specifically, the privacy unit of the guarantee is for the sequences after sampling and packing of the mixture.\r\n\r\n* add: differential privacy research model  by @RyanMullins in #40851\r\n\r\n### Qwen3 VL\r\n\r\n<img width=\"3544\" height=\"1886\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5afa70cb-506e-4d56-baa3-30e7522ac653\" />\r\n\r\n[Qwen3-VL](https://huggingface.co/papers/2502.13923) is a multimodal vision-language model series, encompassing both dense and MoE variants, as well as Instruct and Thinking versions. \r\n\r\nBuilding upon its predecessors, Qwen3-VL delivers significant improvements in visual understanding while maintaining strong pure text capabilities. Key architectural advancements include: enhanced MRope with interleaved layout for better spatial-temporal modeling, DeepStack integration to effectively leverage multi-level features from the Vision Transformer (ViT), and improved video understanding through text-based time alignment‚Äîevolving from T-RoPE to text timestamp alignment for more precise temporal grounding. \r\n\r\nThese innovations collectively enable Qwen3-VL to achieve superior performance in complex multimodal tasks.\r\n\r\n* Adding Support for Qwen3-VL Series  by @JJJYmmm in #40795\r\n\r\n### Longcat Flash\r\n\r\n<img width=\"763\" height=\"468\" alt=\"image\" src=\"https://github.com/user-attachments/assets/289d33e0-6c71-458d-ae07-b7d454ac2adf\" />\r\n\r\nThe LongCatFlash model was proposed in [LongCat-Flash Technical Report](https://huggingface.co/papers/2509.01322) by the Meituan LongCat Team. LongCat-Flash is a 560B parameter Mixture-of-Experts (MoE) model that activates 18.6B-31.3B parameters dynamically (average ~27B). The model features a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and advanced reasoning capabilities.\r\n\r\nThe abstract from the paper is the following:\r\n\r\n*We present LongCat-Flash, a 560 billion parameter Mixture-of-Experts (MoE) language model featuring a dynamic computation mechanism that activates 18.6B-31.3B parameters based on context (average ~27B). The model incorporates a shortcut-connected architecture enabling high inference speed (>100 tokens/second) and demonstrates strong performance across multiple benchmarks including 89.71% accuracy on MMLU and exceptional agentic tool use capabilities.*\r\n\r\nTips:\r\n\r\n- LongCat-Flash uses a unique shortcut-connected MoE architecture that enables faster inference compared to traditional MoE models\r\n- The model supports up to 128k context length for long-form tasks\r\n- Dynamic parameter activation makes it computationally efficient while maintaining high performance\r\n- Best suited for applications requiring strong reasoning, coding, and tool-calling capabilities\r\n- The MoE architecture includes zero experts (nn.Identity modules) which act as skip connections, allowing tokens to bypass expert computation when appropriate\r\n\r\n* Add LongCat-Flash  by @molbap in #40730\r\n\r\n### Flex Olmo\r\n\r\n<img width=\"700\" height=\"414\" alt=\"image\" src=\"https://github.com/user-attachments/assets/7b92ee0f-5f5a-459c-ad4d-e01b5c10202e\" />\r\n\r\n[FlexOlmo](https://huggingface.co/papers/2507.07024) is a new class of language models (LMs) that supports (1) distributed training without data sharing, where different model parameters are independently trained on closed datasets, and (2) data-flexible inference, where these parameters along with their associated data can be flexibly included or excluded from model inferences with no further training. FlexOlmo employs a mixture-of-experts (MoE) architecture where each expert is trained independently on closed datasets and later integrated through a new domain-informed routing without any joint training. FlexOlmo is trained on FlexMix, a corpus we curate comprising publicly available datasets alongside seven domain-specific sets, representing realistic approximations of closed sets. \r\n\r\nYou can find all the original FlexOlmo checkpoints under the [FlexOlmo](https://huggingface.co/collections/allenai/flexolmo-68471177a386b6e20a54c55f) collection.\r\n\r\n* Add FlexOlmo model  by @2015aroras in #40921\r\n\r\n### LFM2 VL\r\n\r\n<img width=\"2300\" height=\"1400\" alt=\"image\" src=\"https://github.com/user-attachments/assets/ef0605cd-9512-458c-915a-62316e14d90c\" />\r\n\r\n[LFM2-VL](https://www.liquid.ai/blog/lfm2-vl-efficient-vision-language-models) first series of vision-language foundation models developed by [Liquid AI](https://liquid.ai/). These multimodal models are designed for low-latency and device-aware deployment. LFM2-VL extends the LFM2 family of open-weight Liquid Foundation Models (LFMs) into the vision-language space, supporting both text and image inputs with variable resolutions.\r\n\r\n#### Architecture\r\n\r\nLFM2-VL consists of three main components: a language model backbone, a vision encoder, and a multimodal projector. LFM2-VL builds upon the LFM2 backbone, inheriting from either LFM2-1.2B (for LFM2-VL-1.6B) or LFM2-350M (for LFM2-VL-450M). For the vision tower, LFM2-VL uses SigLIP2 NaFlex encoders to convert input images into token sequences. Two variants are implemented:\r\n* Shape-optimized (400M) for more fine-grained vision capabilities for LFM2-VL-1.6B\r\n* Base (86M) for fast image processing for LFM2-VL-450M\r\n\r\nThe encoder processes images at their native resolution up to 512√ó512 pixels, efficiently handling smaller images without upscaling and supporting non-standard aspect ratios without distortion. Larger images are split into non-overlapping square patches of 512√ó512 each, preserving detail. In LFM2-VL-1.6B, the model also receives a thumbnail (a small, downscaled version of the original image capturing the overall scene) to enhance global context understanding and alignment. Special tokens mark each patch‚Äôs position and indicate the thumbnail‚Äôs start. The multimodal connector is a 2-layer MLP connector with pixel unshuffle to reduce image token count. \r\n\r\n* Add new model LFM2-VL  by @zucchini-nlp in #40624\r\n\r\n### BLT\r\n\r\n<img width=\"1448\" height=\"1062\" alt=\"image\" src=\"https://github.com/user-attachments/assets/af1fbb09-082c-4331-9217-357adb506cbf\" />\r\n\r\nThe BLT model was proposed in [Byte Latent Transformer: Patches Scale Better Than Tokens](<https://arxiv.org/pdf/2412.09871>) by Artidoro Pagnoni, Ram Pasunuru, Pedro Rodriguez, John Nguyen, Benjamin Muller, Margaret Li1, Chunting Zhou, Lili Yu, Jason Weston, Luke Zettlemoyer, Gargi Ghosh, Mike Lewis, Ari Holtzman‚Ä†, Srinivasan Iyer.\r\nBLT is a byte-level LLM that achieves tokenization-level performance through entropy-based dynamic patching.\r\n\r\nThe abstract from the paper is the following:\r\n\r\n*We introduce the Byte Latent Transformer (BLT), a new byte-level LLM architecture that, for the first time, matches tokenization-based LLM performance at scale with significant improvements in inference\r\nefficiency and robustness. BLT encodes bytes into dynamically sized patches, which serve as the primary units of computation. Patches are segmented based on the entropy of the next byte, allocating\r\nmore compute and model capacity where increased data complexity demands it. We present the first flop controlled scaling study of byte-level models up to 8B parameters and 4T training bytes. Our results demonstrate the feasibility of scaling models trained on raw bytes without a fixed vocabulary. Both training and inference efficiency improve due to dynamically selecting long patches when data is predictable, along with qualitative improvements on reasoning and long tail generalization. Overall, for fixed inference costs, BLT shows significantly better scaling than tokenization-based models, by simultaneously growing both patch and model size.*\r\n\r\n#### Usage Tips:\r\n\r\n- **Dual Model Architecture**: BLT consists of two separate trained models:\r\n  - **Patcher (Entropy Model)**: A smaller transformer model that predicts byte-level entropy to determine patch boundaries and segment input.\r\n  - **Main Transformer Model**: The primary model that processes the patches through a Local Encoder, Global Transformer, and Local Decoder.\r\n\r\n- **Dynamic Patching**: The model uses entropy-based dynamic patching where:\r\n  - High-entropy regions (complex data) get shorter patches with more computational attention\r\n  - Low-entropy regions (predictable data) get longer patches for efficiency\r\n  - This allows the model to allocate compute resources where they're most needed\r\n\r\n- **Local Encoder**: Processes byte sequences with cross-attention to patch embeddings\r\n- **Global Transformer**: Processes patch-level representations with full attention across patches\r\n- **Local Decoder**: Generates output with cross-attention back to the original byte sequence\r\n\r\n- **Byte-Level Tokenizer**: Unlike traditional tokenizers that use learned vocabularies, BLT's tokenizer simply converts text to UTF-8 bytes and maps each byte to a token ID. There is no need for a vocabulary.\r\n\r\n* blt wip  by @itazap in #38579\r\n\r\n### Qwen3 Omni MoE\r\n\r\n<img width=\"14084\" height=\"7429\" alt=\"image\" src=\"https://github.com/user-attachments/assets/20d46a43-15f2-42bf-9703-9575f5ca4430\" />\r\n\r\nThe [Qwen2.5-Omni](https://qwenlm.github.io/blog/qwen2.5-omni/) model is a unified multiple modalities model proposed in [Qwen2.5-Omni Technical Report](https://huggingface.co/papers/2503.20215) from Qwen team, Alibaba Group.\r\n\r\n#### Notes\r\n\r\n- Use [`Qwen2_5OmniForConditionalGeneration`] to generate audio and text output. To generate only one output type, use [`Qwen2_5OmniThinkerForConditionalGeneration`] for text-only and [`Qwen2_5OmniTalkersForConditionalGeneration`] for audio-only outputs.\r\n- Audio generation with [`Qwen2_5OmniForConditionalGeneration`] supports only single batch size at the moment.\r\n- In case out out-of-memory errors hwen working with video input, decrease `processor.max_pixels`. By default the maximum is set to a very arge value and high resolution visuals will not be resized, unless resolution exceeds `processor.max_pixels`.\r\n- The processor has its own [`~ProcessorMixin.apply_chat_template`] method to convert chat messages to model inputs.\r\n\r\n* Adding support for Qwen3Omni  by @BakerBunker in #41025\r\n\r\n### Parakeet\r\n\r\n<img width=\"1431\" height=\"527\" alt=\"image\" src=\"https://github.com/user-attachments/assets/e831f451-9be3-4b5c-a222-b833a50ceb2a\" />\r\n\r\nParakeet models, [introduced by NVIDIA NeMo](https://developer.nvidia.com/blog/pushing-the-boundaries-of-speech-recognition-with-nemo-parakeet-asr-models/), are models that combine a [Fast Conformer](https://docs.nvidia.com/nemo-framework/user-guide/latest/nemotoolkit/asr/models.html#fast-conformer) encoder with connectionist temporal classification (CTC), recurrent neural network transducer (RNNT) or token and duration transducer (TDT) decoder for automatic speech recognition.\r\n\r\n**Model Architecture**\r\n- **Fast Conformer Encoder**: A linearly scalable Conformer architecture that processes mel-spectrogram features and reduces sequence length through subsampling. This is more efficient version of the Conformer Encoder found in [FastSpeech2Conformer](./fastspeech2_conformer.md) (see [`ParakeetEncoder`] for the encoder implementation and details).\r\n- [**ParakeetForCTC**](#parakeetforctc): a Fast Conformer Encoder + a CTC decoder\r\n    - **CTC Decoder**: Simple but effective decoder consisting of:\r\n        - 1D convolution projection from encoder hidden size to vocabulary size (for optimal NeMo compatibility).\r\n        - CTC loss computation for training.\r\n        - Greedy CTC decoding for inference.\r\n\r\n* Add Parakeet  by @nithinraok in #39062\r\n\r\n### EdgeTAM\r\n\r\n<img width=\"949\" height=\"537\" alt=\"image\" src=\"https://github.com/user-attachments/assets/5ca4e73d-5aa9-487d-96e1-92d4f2f4739f\" />\r\n\r\nThe EdgeTAM model was proposed in [EdgeTAM: On-Device Track Anything Model](https://huggingface.co/papers/2501.07256) Chong Zhou, Chenchen Zhu, Yunyang Xiong, Saksham Suri, Fanyi Xiao, Lemeng Wu, Raghuraman Krishnamoorthi, Bo Dai, Chen Change Loy, Vikas Chandra, Bilge Soran.\r\n\r\nEdgeTAM is an efficient adaptation of SAM 2 that introduces a 2D Spatial Perceiver architecture to optimize memory attention mechanisms for real-time video segmentation on mobile devices.\r\n\r\n* Add EdgeTAM  by @yonigozlan in #39800\r\n\r\n### OLMO3\r\n\r\nMore details to come soon :eyes:\r\n\r\n* Add Olmo3 model  by @2015aroras in #40778\r\n\r\n## Continuous batching\r\n\r\nWe are introducing Continuous Batching (CB) in this release, we consider it a stable feature. The main use case for CB is batched generation, which makes it very efficient in the context of GRPO training or evaluation. Thanks to CB, researchers or model developers are now free to use transformers in these contexts without having to spin up an additional inference engine. \r\n\r\nCB currently supports both full attention and sliding window attention: this means that the vast majority of models are supported, like llama, gemma3, gpt-oss. \r\n\r\nCB is also integrated with transformers serve, which means that you can deploy transformers as an OpenAI-compatible HTTP server.\r\nHere is a small snippet on how to use it:\r\n\r\n```python\r\nimport datasets\r\nimport torch\r\n\r\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\r\nfrom transformers.generation import GenerationConfig\r\nmodel = AutoModelForCausalLM.from_pretrained(\r\n    \"Qwen/Qwen3-4B-Instruct-2507\", dtype=torch.bfloat16, _attn_implementation=\"sdpa_paged\", device_map=\"auto\"\r\n)\r\nmodel.generation_config.max_new_tokens = 32\r\ntokenizer = AutoTokenizer.from_pretrained(\"Qwen/Qwen3-4B-Instruct-2507\", padding_side=\"left\")\r\ndataset = datasets.load_dataset(\"openai/gsm8k\", \"socratic\", split=\"test\")\r\ntokenized_datasets = dataset.map(lambda x: tokenizer(x[\"question\"]), batched=True)\r\nsimple_batch_inputs = [item[\"input_ids\"] for item in tokenized_datasets]\r\n\r\nbatch_outputs = model.generate_batch(inputs=simple_batch_inputs)\r\nfor request in batch_outputs:\r\n    print(tokenizer.decode(batch_outputs[request].generated_tokens))\r\n\"\"\"\r\n Let's break down the problem step by step:\r\n\r\n1. **Total eggs laid per day**:  \r\n   Janet‚Äôs ducks lay **16 eggs per day**\r\n Let's break down the problem step by step:\r\n\r\n1. **Blue fiber**: The robe takes **2 bolts** of blue fiber.\r\n2. **White fiber\r\n To determine Josh's profit from flipping the house, let's go step by step.\r\n\r\n---\r\n\r\n### Step 1: Initial cost of the house\r\nJosh buys the\r\n To find the total distance James runs in a week, we can break down the problem step by step:\r\n\r\n1. **Sprints per session**: James runs \r\n To determine how many cups of feed Wendi needs to give her chickens in the final meal of the day, let's go step by step.\r\n\"\"\"\r\n```\r\n\r\n## Breaking changes\r\n\r\n* üö® Remove Group Beam Search decoding strategy  by @manueldeprada in #40495\r\n* üö® Remove Constrained Beam Search decoding strategy  by @manueldeprada in #40518\r\n* üö® Allow `check_model_inputs` in core VLMs  by @zucchini-nlp in #40342\r\n* üî¥ Update Glm4V to use config values  by @zucchini-nlp in #40712\r\n* üö® Fix Inconsistant `input_feature` length and `attention_mask` length in `WhisperFeatureExtractor`  by @BakerBunker in #39221\r\n* ‚ö†Ô∏è üî¥ Add ministral model  by @manueldeprada in #40247\r\n* üî¥ Move variable output controls to `_prepare_generation_config `  by @manueldeprada in #40715\r\n* üî¥Make `center_crop` fast equivalent to slow  by @yonigozlan in #40856\r\n\r\n## Bugfixes and improvements\r\n\r\n* Fix collated reports upload filename  by @ivarflakstad in #40556\r\n* pin `pytest-rerunfailures<16.0`  by @ydshieh in #40561\r\n* remove the redundant non maintained jieba and use rjieba instead  by @divyanshsinghvi in #40383\r\n* Set `test_all_params_have_gradient=False` for `DeepseekV2ModelTest`  by @ydshieh in #40566\r\n* processor tests - use dummy videos  by @zucchini-nlp in #40537\r\n* [qwen-vl] fix position ids  by @zucchini-nlp in #40490\r\n* Fix `test_eager_matches_sdpa_inference` not run for `CLIP`  by @ydshieh in #40581\r\n* Fix CircleCI step passes in the case of pytest worker crash at test collection time  by @ydshieh in #40552\r\n* Allow `remi-or` to `run-slow`  by @ydshieh in #40590\r\n* Fix llava image processor  by @zucchini-nlp in #40588\r\n* Update `get_*_features` methods + update doc snippets  by @qubvel in #40555\r\n* Fix custom generate relative imports  by @manueldeprada in #40480\r\n* Support batch size > 1 image-text inference  by @hiyouga in #36682\r\n* Fix typos  by @cyyever in #40585\r\n* Skip `TvpImageProcessingTest::test_slow_fast_equivalence`  by @ydshieh in #40593\r\n* Fix inexistent imports  by @cyyever in #40580\r\n* Add Copilot instructions  by @Rocketknight1 in #40432\r\n* Fix `siglip` flaky `test_eager_matches_sdpa_inference`  by @ydshieh in #40584\r\n* Fix for missing default values in encoder decoder   by @remi-or in #40517\r\n* Fix quite a lot of FA tests  by @Cyrilvallez in #40548\r\n* [`Tests`] Fixup duplicated mrope logic  by @vasqu in #40592\r\n* Reduce more test data fetch  by @ydshieh in #40595\r\n* Pin torchcodec to 0.5 in AMD docker  by @remi-or in #40598\r\n* Multiple fixes to FA tests in AMD  by @remi-or in #40498\r\n* Disable cache for `TokenizerTesterMixin` temporarily  by @ydshieh in #40611\r\n* fix: continuous batching in `transformers serve`  by @McPatate in #40479\r\n* Fix processor chat template  by @zucchini-nlp in #40613\r\n* Avoid `too many request` caused by `AutoModelTest::test_dynamic_saving_from_local_repo`  by @ydshieh in #40614\r\n* Fix flaky `JambaModelTest.test_load_balancing_loss`  by @ydshieh in #40617\r\n* Add collated reports job to Nvidia CI  by @ahadnagy in #40470\r\n* Remove unnecessary pillow version check  by @cyyever in #40604\r\n* Fix invalid typing  by @cyyever in #40612\r\n* Enable more ruff UP rules  by @cyyever in #40579\r\n* Support TF32 flag for MUSA backend  by @fmo-mt in #33187\r\n* Remove random flag  by @Cyrilvallez in #40629\r\n* üåê [i18n-KO] Translated `deepseek_v3.md` to Korean   by @ssum21 in #39649\r\n* Fix `too many requests` in `TestMistralCommonTokenizer`  by @ydshieh in #40623\r\n* fix: gas for gemma fixed  by @yevvonlim in #40591\r\n* [auto-model] propagate kwargs  by @zucchini-nlp in #40491\r\n* [CP] Add attention_mask to the buffer when the mask is causal   by @kashif in #40619\r\n* Fix: PIL image load in Processing utils apply_chat_template  by @abdokaseb in #40622\r\n* Skip `test_prompt_lookup_decoding_matches_greedy_search` for `voxtral`  by @ydshieh in #40643\r\n* add DeepseekV3ForTokenClassification  by @bzantium in #40641\r\n* fix MetaCLIP 2 wrong link & wrong model names in the docstrings  by @voidism in #40565\r\n* Remove TF/Flax examples  by @Rocketknight1 in #40654\r\n* Mark `LongformerModelTest::test_attention_outputs` as flaky  by @ydshieh in #40655\r\n* fix pipeline dtype  by @jiqing-feng in #40638\r\n* feat(serving): add healthcheck  by @McPatate in #40653\r\n* Fix Metaclip modular conversion  by @Rocketknight1 in #40660\r\n* Avoid attention_mask copy in qwen2.5  by @cyyever in #40658\r\n* Allow custom args in `custom_generate` Callables and unify generation args structure  by @manueldeprada in #40586\r\n* Update `check_determinism` inside `test_determinism`  by @ydshieh in #40661\r\n* Skip `test_fast_is_faster_than_slow` for `Owlv2ImageProcessingTest`  by @ydshieh in #40663\r\n* Fix warning for output_attentions=True  by @qubvel in #40597\r\n* Skip `test_prompt_lookup_decoding_matches_greedy_search` for `qwen2_audio`  by @ydshieh in #40664\r\n* Remove overwritten `GitModelTest::test_beam_search_generate`  by @ydshieh in #40666\r\n* refactor: use `tolist` instead of list comprehension calling `.item()`  by @McPatate in #40646\r\n* Benchmarking V2: framework impl  by @ahadnagy in #40486\r\n* Even more test data cached  by @ydshieh in #40636\r\n* Skip more fast v.s slow image processor tests  by @ydshieh in #40675\r\n* Avoid night torch CI not run because of irrelevant docker image failing to build   by @ydshieh in #40677\r\n* Mark `Aimv2ModelTest::test_eager_matches_sdpa_inference_04_fp16_pad_right_sdpa_kernels` as flaky  by @ydshieh in #40683\r\n* CircleCI docker images cleanup / update / fix  by @ydshieh in #40681\r\n* Add sequence classification support for small Gemma 3 text models  by @abdokaseb in #40562\r\n* Add codebook_dim attribute to DacVectorQuantize for DacResidualVectorQuantize.from_latents()  by @flavioialongo in #40665\r\n* fix broken offline mode when loading tokenizer from hub  by @winglian in #40669\r\n* Load a tiny video to make CI faster  by @zucchini-nlp in #40684\r\n* Final test data cache - inside CI docker images  by @ydshieh in #40689\r\n* add: embedding model  by @RyanMullins in #40694\r\n* feat: support request cancellation  by @McPatate in #40599\r\n* Fixing bug in Voxtral when merging text and audio embeddings  by @rcogill in #40671\r\n* Change docker image to preview for the MI355 CI  by @ahadnagy in #40693\r\n* Fix backward compatibility with accelerate in Trainer  by @qgallouedec in #40668\r\n* Fix self.dropout_p is not defined for SamAttention/Sam2Attention  by @yonigozlan in #40667\r\n* [Glm4.5V] fix vLLM support  by @zucchini-nlp in #40696\r\n* Fix broken Llama4 accuracy in MoE part  by @nvpohanh in #40609\r\n* Avoid `T5GemmaModelTest::test_eager_matches_sdpa_inference` being flaky  by @ydshieh in #40702\r\n* Align assisted generate for unified signature in decoding methods  by @manueldeprada in #40657\r\n* Fetch one missing test data  by @ydshieh in #40703\r\n* Add Fast Image Processor for ImageGPT  by @agamjots05 in #39592\r\n* Fetch more test data with `hf_hub_download`  by @ydshieh in #40710\r\n* feat(serve): add healthcheck test  by @McPatate in #40697\r\n* Fix parent classes of ProcessingKwargs  by @cyyever in #40676\r\n* [tests] fix blip2 edge case  by @gante in #40699\r\n* [moduar] Add missing `self` in post-process methods  by @framonmar7 in #40711\r\n* [onnx] use logical `or` for grounding dino mask  by @lmarshall12 in #40625\r\n* Fix parent classes of AllKwargsForChatTemplate  by @cyyever in #40685\r\n* Fix arguments  by @cyyever in #40605\r\n* [serve] re-enable tests  by @gante in #40717\r\n* [tests] remove overwrites of removed test  by @gante in #40720\r\n* Add Optional typing  by @cyyever in #40686\r\n* [`Gemma Embedding`] Fix SWA  by @vasqu in #40700\r\n* Keypoint matching docs  by @merveenoyan in #40541\r\n* Skip `VitMatteImageProcessingTest::test_fast_is_faster_than_slow`  by @ydshieh in #40713\r\n* refactor(serve): move `request_id` to headers  by @McPatate in #40722\r\n* [Continous Batching] fix do_Sample=True in continuous batching  by @kashif in #40692\r\n* Fix order of mask functions when using `and/or_mask_function`  by @Cyrilvallez in #40753\r\n* Fix np array typing  by @cyyever in #40741\r\n* Set accepts_loss_kwargs to False for ConvNext(|V2)ForImageClassification  by @clinty in #40746\r\n* Add BF16 support check for MUSA backend  by @fmo-mt in #40576\r\n* remove gemmas eager training warning  by @August-murr in #40744\r\n* remove FSDP prefix when using save_pretrained with FSDP2  by @winglian in #40207\r\n* feat: err when unsupported attn impl is set w/ `--continuous_batching`  by @McPatate in #40618\r\n* docs: add continuous batching to serving  by @McPatate in #40758\r\n* Remove unnecessary tildes from documentation  by @st81 in #40748\r\n* Fix more typos  by @cyyever in #40627\r\n* Fix inconsistency in SeamlessM4T and SeamlessM4Tv2 docs  by @clinty in #39364\r\n* Fix `continue_final_message` in `apply_chat_template` to prevent substring matching issues  by @abdokaseb in #40732\r\n* üåê [i18n-KO] Translated 'xclip.md' to Korean  by @ssum21 in #39594\r\n* Fix Bark failing tests  by @ebezzam in #39478\r\n* Add EfficientLoFTRImageProcessorFast for GPU-accelerated image processing  by @LawJarp-A in #40215\r\n* Fix: swanlab `public.cloud.experiment_url` api error  by @Zeyi-Lin in #40763\r\n* [generate] `PromptLookupCandidateGenerator` won't generate forbidden tokens  by @gante in #40726\r\n* Support sliding window in CB  by @remi-or in #40688\r\n* [deprecations] Remove generate-related deprecations up to v4.56  by @gante in #40729\r\n* rm src/transformers/convert_pytorch_checkpoint_to_tf2.py  by @gante in #40718\r\n* [tests] update `test_past_key_values_format` and delete overwrites  by @gante in #40701\r\n* [RoPE] run RoPE tests when the model uses RoPE  by @gante in #40630\r\n* Fix crash when executing MambaCache sample code  by @torotoki in #40557\r\n* [pipeline] ASR pipeline kwargs are forwared to `generate`  by @gante in #40375\r\n* [docs] CPU install  by @stevhliu in #40631\r\n* Adding Support for Qwen3-Next  by @bozheng-hit in #40771\r\n* Fix gpt-oss router_indices in EP  by @jiqing-feng in #40545\r\n* Remove reference of video_load_backend and video_fps for processor  by @cyyever in #40719\r\n* [processors] Unbloating simple processors  by @zucchini-nlp in #40377\r\n* Enable ruff on benchmark and scripts  by @cyyever in #40634\r\n* Fix doc for PerceptionLMForConditionalGeneration forward.  by @shuminghu in #40733\r\n* Fix typos in tests and util  by @cyyever in #40780\r\n* Fix invalid PipelineParallel member  by @cyyever in #40789\r\n* Use functools.cached_property  by @cyyever in #40607\r\n* Read config pattern for Qwen3Next  by @Cyrilvallez in #40792\r\n* Fix dotted model names  by @August-murr in #40745\r\n* Fix the issue that csm model cannot work with pipeline mode.  by @yuanwu2017 in #39349\r\n* Move num_items_in_batch to correct device before accelerator.gather  by @ssharpe42 in #40773\r\n* Remove use_ipex option from Trainer  by @cyyever in #40784\r\n* fix_image_processing_fast_for_glm4v  by @lambertwjh in #40483\r\n* [Docs] Add missing class documentation for optimizer_schedules  by @jijihuny in #31870,  #23010) \r\n* Fix DeepSpeed mixed precision precedence over Accelerate defaults  by @notkisk in #39856\r\n* feature: Add robust token counting with padding exclusion   by @PrathmeshAdsod in #40416\r\n* Fix edge case for tokenize  by @wangzhen0518 in #36277) \r\n* Fix config dtype parsing for Emu3 edge case  by @Isotr0py in #40766\r\n* Align torch implementation of Gated DeltaNet in Qwen3-Next with fla library.  by @bozheng-hit in #40807\r\n* Fix typos in src  by @cyyever in #40782\r\n* add general hub test for Fast Image Processors in test_image_processing_utils  by @namgyu-youn in #40086\r\n* Push generation config along with checkpoints  by @qgallouedec in #40804\r\n* [`Jetmoe`] Fix RoPE  by @vasqu in #40819\r\n* üåê [i18n-KO] Translated clipseg.md to Korean  by @HyunZ118 in #39903\r\n* Improve torch_dtype checks  by @cyyever in #40808\r\n* Add VideoProcessors to auto-backend requirements  by @Cyrilvallez in #40843\r\n* Adds Causal Conv 1D kernel for mamba models  by @MekkCyber in #40765\r\n* Update no split modules in T5Gemma model  by @npuichigo in #40810\r\n* Replace image classification loss functions to `self.loss_function`  by @qubvel in #40764\r\n* Fix the misalignment between the l2norm in GDN of Qwen3-Next and the implementation in the FLA library.  by @bozheng-hit in #40842\r\n* Fixes for continuous batching  by @remi-or in #40828\r\n* [tests] re-enable aria fast tests  by @gante in #40846\r\n* [SAM2] Fix inconsistent results with original implementation with input boxes  by @yonigozlan in #40800\r\n* [Sam2Video] Fix video inference with batched boxes and add test  by @yonigozlan in #40797\r\n* add: differential privacy research model  by @RyanMullins in #40851\r\n* [test] Fix test_eager_matches_sdpa incorrectly skipped  by @eustlb in #40852\r\n* [tests] move generative tests away from `test_modeling_common.py`  by @gante in #40854\r\n* [generate] Always use decoder config to init cache  by @gante in #40772\r\n* Use checkpoint in auto_class_docstring  by @cyyever in #40844\r\n* Fix TrainingArguments.parallelism_config NameError with accelerate<1.10.1  by @albertvillanova in #40818\r\n* Redirect MI355 CI results to dummy dataset  by @ahadnagy in #40862\r\n* [Bug fix #40813] Fix base_model_tp_plan of Starcoder2 model.  by @greg-kwasniewski1 in #40814\r\n* [docstrings / type hints] Update outdated annotations for `past_key_values`   by @gante in #40803\r\n* fix florence kwargs   by @SunMarc in #40826\r\n* fix: XIELU act parameters not being casted to correct dtype  by @NanoCode012 in #40812\r\n* Update model tags and integration references in bug report  by @ArthurZucker in #40881\r\n* [Qwen3 Next] Use numerically stable `rsqrt`  by @thalahors in #40848\r\n* Adding Support for Qwen3-VL Series  by @JJJYmmm in #40795\r\n* [`VaultGemma`] Update expectations in integration tests  by @vasqu in #40855\r\n* Fix modular consistency  by @Cyrilvallez in #40883\r\n* Clarify passing is_causal in sdpa_attention_paged_forward  by @cyyever in #40838\r\n* Use torch.expm1 and torch.log1p for better numerical results  by @cyyever in #40860\r\n* Add Fast PromptDepthAnything Processor  by @SamuelBarryCS in #40602\r\n* Fix deta loading & dataclass  by @Cyrilvallez in #40878\r\n* Remove dict branch of attention_mask in sdpa_attention_paged_forward  by @cyyever in #40882\r\n* üåê [i18n-KO] Translated smolvlm.md to Korean  by @HyunZ118 in #40414\r\n* üåê [i18n-KO] Translated `imageprocessor.md` to Korean  by @HyunZ118 in #39557\r\n* [generate] remove docs of a feature that no longer exists  by @gante in #40895\r\n* Make debugging failing tests (check and update expect output values) easier üî•   by @ydshieh in #40727\r\n* Fixing the call to kernelize  by @MekkCyber in #40628\r\n* Fix getter  regression  by @molbap in #40824\r\n* Fix flaky `Gemma3nAudioFeatureExtractionTest::test_dither`  by @ydshieh in #40902\r\n* [cache] Merge static sliding and static chunked layer  by @Cyrilvallez in #40893\r\n* Harmonize CacheLayer names  by @Cyrilvallez in #40892\r\n* [cache] Only use scalars in `get_mask_sizes`  by @Cyrilvallez in #40907\r\n* Set seed for `Glm4vIntegrationTest`  by @ydshieh in #40905\r\n* Add Olmo3 model  by @2015aroras in #40778\r\n* remove dummy EncodingFast  by @cyyever in #40864\r\n* Improve module name handling for local custom code  by @XuehaiPan in #40809\r\n* Remove `runner_map`  by @ydshieh in #40880\r\n* disable `test_fast_is_faster_than_slow`  by @ydshieh in #40909\r\n* [gemma3] `Gemma3ForConditionalGeneration` compatible with assisted generation  by @gante in #40791\r\n* [generate] misc fixes  by @gante in #40906\r\n* Fix dtype in Paligemma  by @zucchini-nlp in #40912\r\n* [Docs] Adding documentation of MXFP4 Quantization  by @ariG23498 in #40885\r\n* Processor load with multi-processing  by @zucchini-nlp in #40786\r\n* [Llama4] Remove `image_sizes` arg and deprecate `vision_feature_layer`  by @yaswanth19 in #40832\r\n* Fix #40067: Add dedicated UMT5 support to GGUF loader (config, tokenizer, test)  by @akshay-babbar in #40218\r\n* [torchao safetensors] renaming get_state_dict function  by @liangel-02 in #40774\r\n* Adding activation kernels  by @MekkCyber in #40890\r\n* Minor fix for #40727  by @ydshieh in #40929\r\n* Add support for Florence-2 training  by @ducviet00 in #40914\r\n* Add LongCat-Flash  by @molbap in #40730\r\n* [DOC] Add missing dates in model cards  by @yonigozlan in #40922\r\n* [models] remove unused `import torch.utils.checkpoint`   by @gante in #40934\r\n* Intel CPU dockerfile  by @jiqing-feng in #40806\r\n* docs(i18n): Correct the descriptive text in the README_zh-hans.md  by @lilin-1 in #40941\r\n* Fix trainer tests  by @SunMarc in #40823\r\n* Fix `Glm4vMoeIntegrationTest`  by @ydshieh in #40930\r\n* Raise error instead of warning when using meta device in from_pretrained  by @Cyrilvallez in #40942\r\n* Consistent naming for images kwargs  by @zucchini-nlp in #40834\r\n* Remove nested import logic for torchvision  by @yonigozlan in #40940\r\n* Fix `Glm4vModelTest::test_eager_matches_fa2_generate`  by @ydshieh in #40947\r\n* Update expected values for some `test_speculative_generation`  by @ydshieh in #40949\r\n* Standardize audio embedding function name for audio multimodal models  by @jackzhxng in #40919\r\n* Add FlexOlmo model  by @2015aroras in #40921\r\n* Don't list dropout in eager_paged_attention_forward  by @cyyever in #40924\r\n\r\n## Significant community contributions\r\n\r\nThe following contributors have made significant changes to the library over the last release:\r\n\r\n* @hiyouga\r\n    * Support batch size > 1 image-text inference (#36682)\r\n* @cyyever\r\n    * Fix typos (#40585)\r\n    * Fix inexistent imports (#40580)\r\n    * Remove unnecessary pillow version check (#40604)\r\n    * Fix invalid typing (#40612)\r\n    * Enable more ruff UP rules (#40579)\r\n    * Avoid attention_mask copy in qwen2.5 (#40658)\r\n    * Fix parent classes of ProcessingKwargs (#40676)\r\n    * Fix parent classes of AllKwargsForChatTemplate (#40685)\r\n    * Fix arguments (#40605)\r\n    * Add Optional typing (#40686)\r\n    * Fix np array typing (#40741)\r\n    * Fix more typos (#40627)\r\n    * Remove reference of video_load_backend and video_fps for processor (#40719)\r\n    * Enable ruff on benchmark and scripts (#40634)\r\n    * Fix typos in tests and util (#40780)\r\n    * Fix invalid PipelineParallel member (#40789)\r\n    * Use functools.cached_property (#40607)\r\n    * Remove use_ipex option from Trainer (#40784)\r\n    * Fix typos in src (#40782)\r\n    * Improve torch_dtype checks (#40808)\r\n    * Use checkpoint in auto_class_docstring (#40844)\r\n    * Clarify passing is_causal in sdpa_attention_paged_forward (#40838)\r\n    * Use torch.expm1 and torch.log1p for better numerical results (#40860)\r\n    * Remove dict branch of attention_mask in sdpa_attention_paged_forward (#40882)\r\n    * remove dummy EncodingFast (#40864)\r\n    * Don't list dropout in eager_paged_attention_forward (#40924)\r\n    * Benchmarking V2: framework impl (#40486)\r\n    * Change docker image to preview for the MI355 CI (#40693)\r\n    * Redirect MI355 CI results to dummy dataset (#40862)\r\n* @voidism\r\n    * fix MetaCLIP 2 wrong link & wrong model names in the docstrings (#40565)\r\n* @RyanMullins\r\n    * add: embedding model (#40694)\r\n    * add: differential privacy research model (#40851)\r\n* @LawJarp-A\r\n    * Add EfficientLoFTRImageProcessorFast for GPU-accelerated image processing (#40215)\r\n* @bozheng-hit\r\n    * Adding Support for Qwen3-Next (#40771)\r\n    * Align torch implementation of Gated DeltaNet in Qwen3-Next with fla library. (#40807)\r\n    * Fix the misalignment between the l2norm in GDN of Qwen3-Next and the implementation in the FLA library. (#40842)\r\n* @wangzhen0518\r\n    * Fix edge case for tokenize (#36277) (#36555)\r\n* @HyunZ118\r\n    * üåê [i18n-KO] Translated clipseg.md to Korean (#39903)\r\n    * üåê [i18n-KO] Translated smolvlm.md to Korean (#40414)\r\n    * üåê [i18n-KO] Translated `imageprocessor.md` to Korean (#39557)\r\n* @JJJYmmm\r\n    * Adding Support for Qwen3-VL Series (#40795)\r\n* @SamuelBarryCS\r\n    * Add Fast PromptDepthAnything Processor (#40602)\r\n* @2015aroras\r\n    * Add Olmo3 model (#40778)\r\n    * Add FlexOlmo model (#40921)",
        "published_at": "2025-10-03T17:04:51Z",
        "target_commitish": "main",
        "version_tuple": [
          4,
          57,
          0
        ],
        "version_key": "4.57.0"
      },
      {
        "tag_name": "v4.56.2",
        "name": " Patch release v4.56.2",
        "body": "- Processor load with multi-processing (#40786)\r\n- [Jetmoe] Fix RoPE (#40819)\r\n- Fix getter regression (#40824)\r\n- Fix config dtype parsing for Emu3 edge case (#40766)",
        "published_at": "2025-09-17T09:13:17Z",
        "target_commitish": "main",
        "version_tuple": [
          4,
          56,
          2
        ],
        "version_key": "4.56.2"
      }
    ],
    "readme_content": "<!---\nCopyright 2020 The HuggingFace Team. All rights reserved.\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http://www.apache.org/licenses/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n-->\n\n<p align=\"center\">\n  <picture>\n    <source media=\"(prefers-color-scheme: dark)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-dark.svg\">\n    <source media=\"(prefers-color-scheme: light)\" srcset=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\">\n    <img alt=\"Hugging Face Transformers Library\" src=\"https://huggingface.co/datasets/huggingface/documentation-images/raw/main/transformers-logo-light.svg\" width=\"352\" height=\"59\" style=\"max-width: 100%;\">\n  </picture>\n  <br/>\n  <br/>\n</p>\n\n<p align=\"center\">\n    <a href=\"https://huggingface.com/models\"><img alt=\"Checkpoints on Hub\" src=\"https://img.shields.io/endpoint?url=https://huggingface.co/api/shields/models&color=brightgreen\"></a>\n    <a href=\"https://circleci.com/gh/huggingface/transformers\"><img alt=\"Build\" src=\"https://img.shields.io/circleci/build/github/huggingface/transformers/main\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/LICENSE\"><img alt=\"GitHub\" src=\"https://img.shields.io/github/license/huggingface/transformers.svg?color=blue\"></a>\n    <a href=\"https://huggingface.co/docs/transformers/index\"><img alt=\"Documentation\" src=\"https://img.shields.io/website/http/huggingface.co/docs/transformers/index.svg?down_color=red&down_message=offline&up_message=online\"></a>\n    <a href=\"https://github.com/huggingface/transformers/releases\"><img alt=\"GitHub release\" src=\"https://img.shields.io/github/release/huggingface/transformers.svg\"></a>\n    <a href=\"https://github.com/huggingface/transformers/blob/main/CODE_OF_CONDUCT.md\"><img alt=\"Contributor Covenant\" src=\"https://img.shields.io/badge/Contributor%20Covenant-v2.0%20adopted-ff69b4.svg\"></a>\n    <a href=\"https://zenodo.org/badge/latestdoi/155220641\"><img src=\"https://zenodo.org/badge/155220641.svg\" alt=\"DOI\"></a>\n</p>\n\n<h4 align=\"center\">\n    <p>\n        <b>English</b> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hans.md\">ÁÆÄ‰Ωì‰∏≠Êñá</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_zh-hant.md\">ÁπÅÈ´î‰∏≠Êñá</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ko.md\">ÌïúÍµ≠Ïñ¥</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_es.md\">Espa√±ol</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ja.md\">Êó•Êú¨Ë™û</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_hd.md\">‡§π‡§ø‡§®‡•ç‡§¶‡•Ä</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ru.md\">–†—É—Å—Å–∫–∏–π</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_pt-br.md\">Portugu√™s</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_te.md\">‡∞§‡±Ü‡∞≤‡±Å‡∞ó‡±Å</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_fr.md\">Fran√ßais</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_de.md\">Deutsch</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_it.md\">Italiano</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_vi.md\">Ti·∫øng Vi·ªát</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ar.md\">ÿßŸÑÿπÿ±ÿ®Ÿäÿ©</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_ur.md\">ÿßÿ±ÿØŸà</a> |\n        <a href=\"https://github.com/huggingface/transformers/blob/main/i18n/README_bn.md\">‡¶¨‡¶æ‡¶Ç‡¶≤‡¶æ</a> |\n    </p>\n</h4>\n\n<h3 align=\"center\">\n    <p>State-of-the-art pretrained models for inference and training</p>\n</h3>\n\n<h3 align=\"center\">\n    <img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/transformers_as_a_model_definition.png\"/>\n</h3>\n\nTransformers acts as the model-definition framework for state-of-the-art machine learning with text, computer\nvision, audio, video, and multimodal models, for both inference and training.\n\nIt centralizes the model definition so that this definition is agreed upon across the ecosystem. `transformers` is the\npivot across frameworks: if a model definition is supported, it will be compatible with the majority of training\nframeworks (Axolotl, Unsloth, DeepSpeed, FSDP, PyTorch-Lightning, ...), inference engines (vLLM, SGLang, TGI, ...),\nand adjacent modeling libraries (llama.cpp, mlx, ...) which leverage the model definition from `transformers`.\n\nWe pledge to help support new state-of-the-art models and democratize their usage by having their model definition be\nsimple, customizable, and efficient.\n\nThere are over 1M+ Transformers [model checkpoints](https://huggingface.co/models?library=transformers&sort=trending) on the [Hugging Face Hub](https://huggingface.com/models) you can use.\n\nExplore the [Hub](https://huggingface.com/) today to find a model and use Transformers to help you get started right away.\n\n## Installation\n\nTransformers works with Python 3.9+, and [PyTorch](https://pytorch.org/get-started/locally/) 2.1+.\n\nCreate and activate a virtual environment with [venv](https://docs.python.org/3/library/venv.html) or [uv](https://docs.astral.sh/uv/), a fast Rust-based Python package and project manager.\n\n```py\n# venv\npython -m venv .my-env\nsource .my-env/bin/activate\n# uv\nuv venv .my-env\nsource .my-env/bin/activate\n```\n\nInstall Transformers in your virtual environment.\n\n```py\n# pip\npip install \"transformers[torch]\"\n\n# uv\nuv pip install \"transformers[torch]\"\n```\n\nInstall Transformers from source if you want the latest changes in the library or are interested in contributing. However, the *latest* version may not be stable. Feel free to open an [issue](https://github.com/huggingface/transformers/issues) if you encounter an error.\n\n```shell\ngit clone https://github.com/huggingface/transformers.git\ncd transformers\n\n# pip\npip install '.[torch]'\n\n# uv\nuv pip install '.[torch]'\n```\n\n## Quickstart\n\nGet started with Transformers right away with the [Pipeline](https://huggingface.co/docs/transformers/pipeline_tutorial) API. The `Pipeline` is a high-level inference class that supports text, audio, vision, and multimodal tasks. It handles preprocessing the input and returns the appropriate output.\n\nInstantiate a pipeline and specify model to use for text generation. The model is downloaded and cached so you can easily reuse it again. Finally, pass some text to prompt the model.\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"text-generation\", model=\"Qwen/Qwen2.5-1.5B\")\npipeline(\"the secret to baking a really good cake is \")\n[{'generated_text': 'the secret to baking a really good cake is 1) to use the right ingredients and 2) to follow the recipe exactly. the recipe for the cake is as follows: 1 cup of sugar, 1 cup of flour, 1 cup of milk, 1 cup of butter, 1 cup of eggs, 1 cup of chocolate chips. if you want to make 2 cakes, how much sugar do you need? To make 2 cakes, you will need 2 cups of sugar.'}]\n```\n\nTo chat with a model, the usage pattern is the same. The only difference is you need to construct a chat history (the input to `Pipeline`) between you and the system.\n\n> [!TIP]\n> You can also chat with a model directly from the command line.\n> ```shell\n> transformers chat Qwen/Qwen2.5-0.5B-Instruct\n> ```\n\n```py\nimport torch\nfrom transformers import pipeline\n\nchat = [\n    {\"role\": \"system\", \"content\": \"You are a sassy, wise-cracking robot as imagined by Hollywood circa 1986.\"},\n    {\"role\": \"user\", \"content\": \"Hey, can you tell me any fun things to do in New York?\"}\n]\n\npipeline = pipeline(task=\"text-generation\", model=\"meta-llama/Meta-Llama-3-8B-Instruct\", dtype=torch.bfloat16, device_map=\"auto\")\nresponse = pipeline(chat, max_new_tokens=512)\nprint(response[0][\"generated_text\"][-1][\"content\"])\n```\n\nExpand the examples below to see how `Pipeline` works for different modalities and tasks.\n\n<details>\n<summary>Automatic speech recognition</summary>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"automatic-speech-recognition\", model=\"openai/whisper-large-v3\")\npipeline(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n{'text': ' I have a dream that one day this nation will rise up and live out the true meaning of its creed.'}\n```\n\n</details>\n\n<details>\n<summary>Image classification</summary>\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"image-classification\", model=\"facebook/dinov2-small-imagenet1k-1-layer\")\npipeline(\"https://huggingface.co/datasets/Narsil/image_dummy/raw/main/parrots.png\")\n[{'label': 'macaw', 'score': 0.997848391532898},\n {'label': 'sulphur-crested cockatoo, Kakatoe galerita, Cacatua galerita',\n  'score': 0.0016551691805943847},\n {'label': 'lorikeet', 'score': 0.00018523589824326336},\n {'label': 'African grey, African gray, Psittacus erithacus',\n  'score': 7.85409429227002e-05},\n {'label': 'quail', 'score': 5.502637941390276e-05}]\n```\n\n</details>\n\n<details>\n<summary>Visual question answering</summary>\n\n<h3 align=\"center\">\n    <a><img src=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\"></a>\n</h3>\n\n```py\nfrom transformers import pipeline\n\npipeline = pipeline(task=\"visual-question-answering\", model=\"Salesforce/blip-vqa-base\")\npipeline(\n    image=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/tasks/idefics-few-shot.jpg\",\n    question=\"What is in the image?\",\n)\n[{'answer': 'statue of liberty'}]\n```\n\n</details>\n\n## Why should I use Transformers?\n\n1. Easy-to-use state-of-the-art models:\n    - High performance on natural language understanding & generation, computer vision, audio, video, and multimodal tasks.\n    - Low barrier to entry for researchers, engineers, and developers.\n    - Few user-facing abstractions with just three classes to learn.\n    - A unified API for using all our pretrained models.\n\n1. Lower compute costs, smaller carbon footprint:\n    - Share trained models instead of training from scratch.\n    - Reduce compute time and production costs.\n    - Dozens of model architectures with 1M+ pretrained checkpoints across all modalities.\n\n1. Choose the right framework for every part of a models lifetime:\n    - Train state-of-the-art models in 3 lines of code.\n    - Move a single model between PyTorch/JAX/TF2.0 frameworks at will.\n    - Pick the right framework for training, evaluation, and production.\n\n1. Easily customize a model or an example to your needs:\n    - We provide examples for each architecture to reproduce the results published by its original authors.\n    - Model internals are exposed as consistently as possible.\n    - Model files can be used independently of the library for quick experiments.\n\n<a target=\"_blank\" href=\"https://huggingface.co/enterprise\">\n    <img alt=\"Hugging Face Enterprise Hub\" src=\"https://github.com/user-attachments/assets/247fb16d-d251-4583-96c4-d3d76dda4925\">\n</a><br>\n\n## Why shouldn't I use Transformers?\n\n- This library is not a modular toolbox of building blocks for neural nets. The code in the model files is not refactored with additional abstractions on purpose, so that researchers can quickly iterate on each of the models without diving into additional abstractions/files.\n- The training API is optimized to work with PyTorch models provided by Transformers. For generic machine learning loops, you should use another library like [Accelerate](https://huggingface.co/docs/accelerate).\n- The [example scripts](https://github.com/huggingface/transformers/tree/main/examples) are only *examples*. They may not necessarily work out-of-the-box on your specific use case and you'll need to adapt the code for it to work.\n\n## 100 projects using Transformers\n\nTransformers is more than a toolkit to use pretrained models, it's a community of projects built around it and the\nHugging Face Hub. We want Transformers to enable developers, researchers, students, professors, engineers, and anyone\nelse to build their dream projects.\n\nIn order to celebrate Transformers 100,000 stars, we wanted to put the spotlight on the\ncommunity with the [awesome-transformers](./awesome-transformers.md) page which lists 100\nincredible projects built with Transformers.\n\nIf you own or use a project that you believe should be part of the list, please open a PR to add it!\n\n## Example models\n\nYou can test most of our models directly on their [Hub model pages](https://huggingface.co/models).\n\nExpand each modality below to see a few example models for various use cases.\n\n<details>\n<summary>Audio</summary>\n\n- Audio classification with [Whisper](https://huggingface.co/openai/whisper-large-v3-turbo)\n- Automatic speech recognition with [Moonshine](https://huggingface.co/UsefulSensors/moonshine)\n- Keyword spotting with [Wav2Vec2](https://huggingface.co/superb/wav2vec2-base-superb-ks)\n- Speech to speech generation with [Moshi](https://huggingface.co/kyutai/moshiko-pytorch-bf16)\n- Text to audio with [MusicGen](https://huggingface.co/facebook/musicgen-large)\n- Text to speech with [Bark](https://huggingface.co/suno/bark)\n\n</details>\n\n<details>\n<summary>Computer vision</summary>\n\n- Automatic mask generation with [SAM](https://huggingface.co/facebook/sam-vit-base)\n- Depth estimation with [DepthPro](https://huggingface.co/apple/DepthPro-hf)\n- Image classification with [DINO v2](https://huggingface.co/facebook/dinov2-base)\n- Keypoint detection with [SuperPoint](https://huggingface.co/magic-leap-community/superpoint)\n- Keypoint matching with [SuperGlue](https://huggingface.co/magic-leap-community/superglue_outdoor)\n- Object detection with [RT-DETRv2](https://huggingface.co/PekingU/rtdetr_v2_r50vd)\n- Pose Estimation with [VitPose](https://huggingface.co/usyd-community/vitpose-base-simple)\n- Universal segmentation with [OneFormer](https://huggingface.co/shi-labs/oneformer_ade20k_swin_large)\n- Video classification with [VideoMAE](https://huggingface.co/MCG-NJU/videomae-large)\n\n</details>\n\n<details>\n<summary>Multimodal</summary>\n\n- Audio or text to text with [Qwen2-Audio](https://huggingface.co/Qwen/Qwen2-Audio-7B)\n- Document question answering with [LayoutLMv3](https://huggingface.co/microsoft/layoutlmv3-base)\n- Image or text to text with [Qwen-VL](https://huggingface.co/Qwen/Qwen2.5-VL-3B-Instruct)\n- Image captioning [BLIP-2](https://huggingface.co/Salesforce/blip2-opt-2.7b)\n- OCR-based document understanding with [GOT-OCR2](https://huggingface.co/stepfun-ai/GOT-OCR-2.0-hf)\n- Table question answering with [TAPAS](https://huggingface.co/google/tapas-base)\n- Unified multimodal understanding and generation with [Emu3](https://huggingface.co/BAAI/Emu3-Gen)\n- Vision to text with [Llava-OneVision](https://huggingface.co/llava-hf/llava-onevision-qwen2-0.5b-ov-hf)\n- Visual question answering with [Llava](https://huggingface.co/llava-hf/llava-1.5-7b-hf)\n- Visual referring expression segmentation with [Kosmos-2](https://huggingface.co/microsoft/kosmos-2-patch14-224)\n\n</details>\n\n<details>\n<summary>NLP</summary>\n\n- Masked word completion with [ModernBERT](https://huggingface.co/answerdotai/ModernBERT-base)\n- Named entity recognition with [Gemma](https://huggingface.co/google/gemma-2-2b)\n- Question answering with [Mixtral](https://huggingface.co/mistralai/Mixtral-8x7B-v0.1)\n- Summarization with [BART](https://huggingface.co/facebook/bart-large-cnn)\n- Translation with [T5](https://huggingface.co/google-t5/t5-base)\n- Text generation with [Llama](https://huggingface.co/meta-llama/Llama-3.2-1B)\n- Text classification with [Qwen](https://huggingface.co/Qwen/Qwen2.5-0.5B)\n\n</details>\n\n## Citation\n\nWe now have a [paper](https://www.aclweb.org/anthology/2020.emnlp-demos.6/) you can cite for the ü§ó Transformers library:\n```bibtex\n@inproceedings{wolf-etal-2020-transformers,\n    title = \"Transformers: State-of-the-Art Natural Language Processing\",\n    author = \"Thomas Wolf and Lysandre Debut and Victor Sanh and Julien Chaumond and Clement Delangue and Anthony Moi and Pierric Cistac and Tim Rault and R√©mi Louf and Morgan Funtowicz and Joe Davison and Sam Shleifer and Patrick von Platen and Clara Ma and Yacine Jernite and Julien Plu and Canwen Xu and Teven Le Scao and Sylvain Gugger and Mariama Drame and Quentin Lhoest and Alexander M. Rush\",\n    booktitle = \"Proceedings of the 2020 Conference on Empirical Methods in Natural Language Processing: System Demonstrations\",\n    month = oct,\n    year = \"2020\",\n    address = \"Online\",\n    publisher = \"Association for Computational Linguistics\",\n    url = \"https://www.aclweb.org/anthology/2020.emnlp-demos.6\",\n    pages = \"38--45\"\n}\n```\n",
    "ci_configs": {
      ".github/workflows/add-model-like.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/add-model-like.yml",
      ".github/workflows/assign-reviewers.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/assign-reviewers.yml",
      ".github/workflows/benchmark.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/benchmark.yml",
      ".github/workflows/benchmark_v2.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/benchmark_v2.yml",
      ".github/workflows/benchmark_v2_a10_caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/benchmark_v2_a10_caller.yml",
      ".github/workflows/benchmark_v2_mi325_caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/benchmark_v2_mi325_caller.yml",
      ".github/workflows/build-ci-docker-images.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/build-ci-docker-images.yml",
      ".github/workflows/build-docker-images.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/build-docker-images.yml",
      ".github/workflows/build-nightly-ci-docker-images.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/build-nightly-ci-docker-images.yml",
      ".github/workflows/build-past-ci-docker-images.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/build-past-ci-docker-images.yml",
      ".github/workflows/build_documentation.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/build_documentation.yml",
      ".github/workflows/build_pr_documentation.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/build_pr_documentation.yml",
      ".github/workflows/check_failed_tests.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/check_failed_tests.yml",
      ".github/workflows/check_tiny_models.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/check_tiny_models.yml",
      ".github/workflows/codeql.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/codeql.yml",
      ".github/workflows/collated-reports.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/collated-reports.yml",
      ".github/workflows/doctest_job.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/doctest_job.yml",
      ".github/workflows/doctests.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/doctests.yml",
      ".github/workflows/get-pr-info.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/get-pr-info.yml",
      ".github/workflows/get-pr-number.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/get-pr-number.yml",
      ".github/workflows/model_jobs.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/model_jobs.yml",
      ".github/workflows/model_jobs_intel_gaudi.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/model_jobs_intel_gaudi.yml",
      ".github/workflows/new_model_pr_merged_notification.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/new_model_pr_merged_notification.yml",
      ".github/workflows/pr-style-bot.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/pr-style-bot.yml",
      ".github/workflows/pr_build_doc_with_comment.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/pr_build_doc_with_comment.yml",
      ".github/workflows/pr_run_slow_ci.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/pr_run_slow_ci.yml",
      ".github/workflows/push-important-models.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/push-important-models.yml",
      ".github/workflows/release-conda.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/release-conda.yml",
      ".github/workflows/self-comment-ci.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-comment-ci.yml",
      ".github/workflows/self-nightly-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-nightly-caller.yml",
      ".github/workflows/self-nightly-past-ci-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-nightly-past-ci-caller.yml",
      ".github/workflows/self-past-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-past-caller.yml",
      ".github/workflows/self-scheduled-amd-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-scheduled-amd-caller.yml",
      ".github/workflows/self-scheduled-amd-mi250-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-scheduled-amd-mi250-caller.yml",
      ".github/workflows/self-scheduled-amd-mi325-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-scheduled-amd-mi325-caller.yml",
      ".github/workflows/self-scheduled-amd-mi355-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-scheduled-amd-mi355-caller.yml",
      ".github/workflows/self-scheduled-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-scheduled-caller.yml",
      ".github/workflows/self-scheduled-flash-attn-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-scheduled-flash-attn-caller.yml",
      ".github/workflows/self-scheduled-intel-gaudi.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-scheduled-intel-gaudi.yml",
      ".github/workflows/self-scheduled-intel-gaudi3-caller.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-scheduled-intel-gaudi3-caller.yml",
      ".github/workflows/self-scheduled.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/self-scheduled.yml",
      ".github/workflows/slack-report.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/slack-report.yml",
      ".github/workflows/ssh-runner.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/ssh-runner.yml",
      ".github/workflows/stale.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/stale.yml",
      ".github/workflows/trufflehog.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/trufflehog.yml",
      ".github/workflows/update_metdata.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/update_metdata.yml",
      ".github/workflows/upload_pr_documentation.yml": "https://raw.githubusercontent.com/huggingface/transformers/main/.github/workflows/upload_pr_documentation.yml"
    },
    "processed_at": "2025-11-05 23:46:45"
  },
  "pytorch/pytorch": {
    "full_name": "pytorch/pytorch",
    "stargazers_count": 94703,
    "size": 1154499,
    "topics": [
      "autograd",
      "deep-learning",
      "gpu",
      "machine-learning",
      "neural-network",
      "numpy",
      "python",
      "tensor"
    ],
    "releases_count": 10,
    "major_releases": [
      {
        "tag_name": "v2.9.0",
        "name": "2.9 Release Notes",
        "body": "# PyTorch 2.9.0 Release Notes\r\n- [Highlights](#highlights)\r\n- [Backwards Incompatible Changes](#backwards-incompatible-changes)\r\n- [Deprecations](#deprecations)\r\n- [New Features](#new-features)\r\n- [Improvements](#improvements)\r\n- [Bug fixes](#bug-fixes)\r\n- [Performance](#performance)\r\n- [Documentation](#documentation)\r\n- [Developers](#developers)\r\n- [Security](#security)\r\n\r\n\r\n# Highlights\r\n\r\n<table>\r\n  <tr>\r\n   <td><strong>Unstable (API-Unstable)</strong></td>\r\n  </tr>\r\n  <tr>\r\n   <td>Updates to the stable libtorch ABI for third-party C++/CUDA extensions</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Symmetric memory that enables easy programming of multi-GPU kernels</td>\r\n  </tr>\r\n  <tr>\r\n   <td>The ability to arbitrarily toggle error or resume on graph breaks in torch.compile</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Expanded wheel variant support  to include ROCm,  XPU and CUDA 13</td>\r\n  </tr>\r\n  <tr>\r\n   <td>FlexAttention enablement on Intel GPUs</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Flash decoding optimization based on FlexAttention on X86 CPU</td>\r\n  </tr>\r\n  <tr>\r\n   <td>ARM Platform improvements and optimizations</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Enablement of Linux aarch64 binary wheel builds across all supported CUDA versions</td>\r\n  </tr>\r\n</table>\r\n\r\nFor more details about these highlighted features, you can look at the [release blogpost](https://pytorch.org/blog/pytorch-2-9/). Below are the full release notes for this release.\r\n\r\n\r\n# Backwards Incompatible Changes\r\n\r\n## Min supported Python version is now 3.10 (#162310)\r\n\r\nThe minimum version of Python required for PyTorch 2.9.0 is 3.10. We also have 3.14 and 3.14t available as preview with this release.\r\n\r\n## Undefined behavior when an output of a custom operator shares storage with an input\r\n\r\nThis is a reminder that outputs of PyTorch custom operators (that are registered using the `torch.library` or `TORCH_LIBRARY` APIs) are not allowed to return Tensors that share storage with input tensors. The violation of this condition leads to undefined behavior: sometimes the result will be correct, sometimes it will be garbage.\r\n\r\nAfter [#163227](https://github.com/pytorch/pytorch/pull/163227), custom operators that violated this condition that previously returned correct results under `torch.compile` may now return silently incorrect results under `torch.compile`. Because this is changing the behavior of undefined behavior, we do not consider this to be a bug, but we are still documenting it in this section as a \"potentially unexpected behavior change\".\r\n\r\nThis is one of the conditions checked for by [`torch.library.opcheck`](https://docs.pytorch.org/docs/stable/library.html#testing-custom-ops) and is mentioned in [The Custom Operators Manual](https://docs.google.com/document/d/1_W62p8WJOQQUzPsJYa7s701JXt0qf2OfLub2sbkHOaU/edit?tab=t.0#bookmark=id.4c0um7xkba6e)\r\n\r\n### More details\r\n\r\nOutputs of PyTorch custom operators are not allowed to return Tensors that share storage with input tensors\r\n\r\nFor example, the following two custom operators are not valid custom operators:\r\n\r\n```py\r\n@torch.library.custom_op(\"mylib::foo\", mutates_args=())\r\ndef foo(x: torch.Tensor) -> torch.Tensor:\r\n    # the result of `foo` must not directly be an input to foo.\r\n    return x\r\n\r\n@torch.library.custom_op(\"mylib::bar\", mutates_args=())\r\ndef bar(x: torch.Tensor) -> torch.Tensor:\r\n    # the result of bar must not be a view of an input of bar\r\n    return x.view(-1)\r\n```\r\nThe easiest workaround is to add an extra `.clone()` to the outputs:\r\n```py\r\n@torch.library.custom_op(\"mylib::foo\", mutates_args=())\r\ndef foo(x: torch.Tensor) -> torch.Tensor:\r\n    return x.clone()\r\n\r\n@torch.library.custom_op(\"mylib::bar\", mutates_args=())\r\ndef bar(x: torch.Tensor) -> torch.Tensor:\r\n    return x.view(-1).clone()\r\n```\r\n\r\nA common way to get into this situation is for a user to want to create a custom operator that sometimes mutates the input in-place and sometimes returns a new Tensor, like in the following example.\r\n\r\n```py\r\n@torch.library.custom_op(\"mylib::baz\", mutates_args=[\"x\"])\r\ndef baz(x: torch.Tensor) -> torch.Tensor:\r\n    if inplace:\r\n        x.sin_()\r\n        return x\r\n    else:\r\n        return x.sin()\r\n```\r\nThis dynamism is not supported and leads to undefined behavior. The workaround is to split the custom operator into two custom operators, one that always mutates the input in-place, and another that always returns a new Tensor.\r\n```py\r\n@torch.library.custom_op(\"mylib::baz_outplace\", mutates_args=())\r\ndef baz_outplace(x: torch.Tensor) -> torch.Tensor:\r\n    return x.sin()\r\n\r\n@torch.library.custom_op(\"mylib::baz_inplace\", mutates_args=[\"x\"])\r\ndef baz_inplace(x: torch.Tensor) -> torch.Tensor:\r\n    x.sin_()\r\n\r\ndef baz(x):\r\n    if inplace:\r\n        baz_inplace(x)\r\n        return x\r\n    else:\r\n        return baz_outplace(x)\r\n```\r\n\r\n## Build metal kernels of MacOS-14+ and remove all pre-MacOS-14 specific logic, requires MacOS-14+ going forward (#159733, #159912)\r\n\r\nPyTorch MPS is only supported on MacOS-14 or later. If you need to use MPS on MacOS Ventura, please avoid updating to Python-3.9 or above\r\n\r\n## Upgrade to DLPack 1.0 (#145000)\r\n\r\nThis upgrade is doing the same BC-breaking changes as the DLPack release. Objects in `torch.utils.dlpack` have been updated to reflect these changes, such as `DLDeviceType`.\r\n\r\nSee the PR for details on the exact changes and how to update your code.\r\n\r\n## Raise appropriate errors in `torch.cat` (#158249)\r\n\r\n`torch.cat` now raises `ValueError`, `IndexError` or `TypeError` where appropriate instead of the generic `RuntimeError`. If you code was catching these errors, you can update to catch the new error type.\r\n\r\n\r\n## Default to `dynamo=True` for ONNX exporter (#159646, #162726)\r\n\r\nPreviously `torch.onnx.export(...)` used the legacy TorchScript exporter if no arguments were provied. The ONNX exporter now uses the newer `torch.export.export` pipeline by default (`dynamo=True`). This change improves graph fidelity and future-proofs exports, but may surface graph capture errors that were previously masked or handled differently.\r\n\r\nPreviously in torch 2.8.0:\r\n\r\n```python\r\n# API calls the legacy exporter with dynamo=False\r\ntorch.onnx.export(...)\r\n```\r\n\r\nNow in torch 2.9.0:\r\n\r\n```python\r\n# To preserve the original behavior\r\ntorch.onnx.export(..., dynamo=False)\r\n\r\n# Export onnx model through torch.export.export\r\ntorch.onnx.export(...)\r\n```\r\n\r\nRecommendation: first try the new default; only fall back if you hit blocking issues and report them upstream.\r\nLong term solution: fix the root cause instead of relying on fallback or TorchScript exporter.\r\n\r\n## Switch off runtime asserts by default in Export in favor of a shape guards function (#160111, #161178, #161794)\r\n\r\n\r\nTo enable runtime asserts, use `export(..., prefer_deferred_runtime_asserts_over_guards=True)`. Also kills the `allow_complex_guards_as_runtime_asserts` flag, merging it into the former option.\r\n\r\n\r\nAdditionally, `exported_program.module()` will generate a call to a `_guards_fn` submodule that will run additional checks on inputs. Users who do not want this behavior can either remove this call in the graph, or do `exported_program.module(check_guards=False)` to avoid the generation.\r\n\r\n## Set default opset to 20 in ONNX (#158802)\r\n\r\nOpset 20 enables newer operator definitions. If your tooling or downstream runtime only supports opset 18, pin it explicitly. For the latest ONNX operators, you can experiment with opset 23.\r\n\r\nPreviously in torch 2.8.0:\r\n\r\n```python\r\n# opset_version=18\r\ntorch.onnx.export(...)\r\n```\r\n\r\nNow in torch 2.9.0:\r\n\r\n```python\r\n# To preserve the original behavior\r\ntorch.onnx.export(..., opset_version=18)\r\n\r\n# New: opset_version=20\r\ntorch.onnx.export(...)\r\n\r\n# Use the latest supported opset: opset_version=23\r\ntorch.onnx.export(..., opset_version=23)\r\n```\r\n\r\n## Drop `draft_export` in exporter API (#161454, #162225)\r\n\r\nRemove implicit draft tracing from the default exporter path, achieving clearer behaviour and faster failures.\r\nThe expensive `torch.export.draft_export` diagnostic path is no longer auto-invoked (which could take hours on large models). You can still opt in for deep diagnostics:\r\n\r\nPreviously in torch 2.8.0:\r\n\r\n```bash\r\n# If both torch.export.export(..., strict=False) and\r\n# torch.export.export(..., strict=True) fail to capture\r\n# the model graph, torch.export.draft_export(...) will be triggered,\r\n# and uses real tensor to trace/export the model.\r\n#\r\n# Inside export_to_onnx.py:\r\n#  ... torch.onnx.export(..., dynamo=True)\r\npython export_to_onnx.py\r\n```\r\n\r\nNow in torch 2.9.0:\r\n\r\n```bash\r\n# To trigger torch.export.draft_export once\r\n# torch.export.export strict=False/True both\r\n# fail:\r\n\r\nTORCH_ONNX_ENABLE_DRAFT_EXPORT=True python export_to_onnx.py\r\n```\r\n\r\n## Remove `torch.onnx.dynamo_export` and the `onnxrt` torch compile backend (#158130, #158258)\r\n\r\n`torch.onnx.dynamo_export` is removed. Please use `torch.onnx.export` instead.\r\nThe experimental ONNX Runtime compile backend (`torch.compile(backend=\"onnxrt\")`) is no longer supported.\r\n\r\n## Remove `torch.onnx.enable_fake_mode` (#161222)\r\n\r\nThe `dynamo=True` mode uses `FakeTensor`s by default which is memory efficient.\r\n\r\n## Some public facing ONNX utility APIs for the TorchScript based exporter are now private (#161323)\r\n\r\nDeprecated members in `torch.onnx.verification` are removed. Previously private `torch.onnx.symbolic_opsets*` functions will no longer be accessible. Consider making a copy of the source code if you need to access any private functions for compatibility with the TorchScript based exporter.\r\n\r\n## Remove `torch.onnx.symbolic_caffe2` (#157102)\r\n\r\nSupport for `caffe2` in the ONNX exporter has ended and is removed.\r\n\r\n## Remove `/d2implyavx512upperregs` flag that slows build (#159431)\r\n\r\nRe-introduced AVX512 optimizations for Windows VS2022 builds, may cause issues with specific versions of VS2022, see #145702\r\n\r\n## Add `ScalarType` to shim conversion and `stable::Tensor.scalar_type` (#160557)\r\n\r\nBefore, user extensions could only in abstract pass around obfuscated dtypes appearing as `int32_ts`. Now, users can confidently use `torch::headeronly::ScalarType` in their extensions for major scalar types. This PR enables ABI stability by adding a translation layer through the shim, so that even if the `ScalarType` enum values change in the future, user extensions need not fear.\r\n\r\nThis change adds ScalarType support for user extensions and is only narrowly BC breaking for unpopular dtypes: `quint*`s, `qint*`s, `Bits*`, `dummy_uint*`s, `dummy_int*`s, `Float8_e8m0fnu`, and `Float4_e2m1fn_x2` in the use case where an extension retrieves a Tensor dtype of the above and passes it into `aoti_torch_call_dispatcher`.\r\n\r\n# Deprecations\r\n## Deprecate `pin_memory_device` param in `torch.utils.data.DataLoader` (#158323)\r\n\r\nWe move enabling `pin_memory` back inside `BaseDataLoaderIter`. This is required for `StatefulDataloader` which leveraged `BaseDataLoaderIter` direclty rather than the `Dataloader` class init\r\n\r\n## Deprecate `torch.export.export_for_training` API in favor of equivalent `torch.export.export` API (#158203)\r\n\r\n`torch.export.export_for_training` exists because we couldn't migrate internal usages of export to the final IR. Now that we have completed the migration, we deprecated and deleted this API.\r\n\r\n# New Features\r\n## Python Frontend\r\n- Add utility to get the kernel currently registered on the dispatcher (#158393)\r\n- Extend `__torch_function__` handler to be triggered by elements within a list (#160256)\r\n- Add `torch.hash_tensor` reduction function (#154149)\r\n\r\n## FX\r\n- Extend torch function support to ALL arguments instead of just scalar type (but not inside of list, #145089)\r\n- Add `is_fx_symbolic_tracing` flag (#161385)\r\n\r\n## Dynamo\r\n- Experimental API for ahead-of-time compiling models in fullgraph mode (#161383)\r\n- Add a hook for recompilations (#157961)\r\n- DynamicInts prototype (#162194)\r\n\r\nIntroduces an API for annotating dynamic integer inputs & attributes for `torch.compile`, by wrapping plain ints with `DynamicInt()`.\r\nDynamicInt objects also work in eager mode, acting as their underlying values when passed as scalar inputs.\r\n\r\n```python\r\na = DynamicInt(4)\r\ny = a + 2  # DynamicInt(6)\r\nz = torch.ones(a)  # torch.ones(4)\r\n\r\nfn = torch.compile(torch.ones)\r\nfn(a)  # compiled fn takes a dynamic integer input\r\nfn(2)  # returns torch.ones(2) without recompiling\r\n```\r\n\r\n\r\n## Optimizer\r\n- Introduce Muon optimizer to PyTorch (#160213)\r\n\r\n## Profiler\r\n- Add GC Events to Python Stack Tracer (#161209)\r\n- Add a custom profiler configuration option (#151656)\r\n\r\n## Inductor\r\n- Allow user to pass in custom partitioner function (#157580)\r\n\r\n## Export\r\n- Add support for param mutation under inference mode (#159661)\r\n\r\n## AOTDispatcher\r\n- Add AOTDispatcher config to set backward autocast behavior (#156356)\r\n\r\n## Quantization\r\n- Enable cpu fp8 qlinear and cpu fp8 qconv (#155678, #157076)\r\n\r\n## ONNX\r\n- RMS Norm support in opset 23 (#159377)\r\n\r\n## C++ Extensions\r\n- Build out a stable set of ATen ops in `torch/csrc/stable/ops.h`:  `amax`, `narrow`, `new_empty` + `new_zeros` dtype variant, `pad`, (#159328, #158974, #159508, #161597, #160214)\r\n- Add `torch::stable::Tensor()` default constructor,  `is_cpu`, and `get_device_index`(#159507, #160212, #160143)\r\n- Add beginnings of `torch::stable::accelerator` with support for DeviceGuard and Stream (#159679, #160453)\r\n- Start building out `torch/headeronly`: c10 Macros, STD_TORCH_CHECK, ScalarTypes (like BFloat16 and Half, #158035, #158365, #157912, #158377, #159302, #159414, #159412, #159415, #159411, #159911)\r\n- Remove cmake cache and reconfigure again if it is invalid (#156958)\r\n- Cut a version of `TORCH_ERROR_CODE_CHECK` in `headeronly` from AOTI (#159604)\r\n- Remove `wheel` from build requirements (#158027)\r\n- Error when `TORCH_STABLE_ONLY` is defined in `TensorBase.h` (#161658)\r\n\r\n## Build Frontend\r\n- Add transpose to `torch/csrc/stable` (#158160)\r\n- Add `zero_()` and `empty_like(t)` to `torch/csrc/stable/ops.h` (#158866)\r\n\r\n## Release Engineering\r\n- Add support for CUDA 13.0 in CI/CD builds. Enable CUDA compression mode for binary size reduction for CUDA 13.0 builds (#160956, #161073, #161257, #161663, #161316, #160201, #160770, #161013, #161916, #162268, #162322, #162383, #161833)\r\n\r\n- Enable CUDA 12.6, 12.8 and 13.0 support for Linux ARM64 CD builds (#162364, #160720, #159481)\r\n\r\n- Add support for Python 3.14 in CI/CD builds (#156889, #157559, #159261, #159869, #160593, #160788, #161255, #159725)\r\n\r\n- Enable NVSHMEM integration (#151261, #153010, #154538, #155506, #156685, #158938, #161321, #160778, #159907, #160465)\r\n\r\n## CUDA\r\n- Add getter for CUDA graph exec to allow mutation of captured kernel params (#161294)\r\n- Implement support for `cudnn_batch_norm_out` kernel to replace the autogen approach (#123020)\r\n\r\n## CPU\r\n- Support GQA for flash attention (#157893)\r\n\r\n## MPS\r\n- Partial sparse support for MPS backend (#159729, #160254, #160223, #161846, #162007, #157238)\r\n- Add `avg_pool3d`, `max_unpool1d/2d/3d`, `max_pool3d`, `max_pool3d` bwd pass, and `avg_pool3d` bwd pass for MPS (#158877,#159789, #156467, #157498, #159089)\r\n\r\n## ROCm\r\n- OCP Micro-scaling Format (mx-fp8/mx-fp4) Support (#151360)\r\n\r\n## XPU\r\n- Enable `FlexAttention` on Intel GPU (#143553)\r\n\r\n# Improvements\r\n## Python Frontend\r\n- Speed up `torch.load` under `FakeTensorMode` by reducing random reads (#157931)\r\n- Make `torch.utils.benchmark.utils.timer` accelerator agnostic (#157131)\r\n- Improve error message for weight-only load errors (#159935)\r\n\r\n## torch.nn\r\n- Allow `register_buffer` with `Tensor`-like objects (#159455)\r\n- Improve error message for unsupported padding configurations (#160866)\r\n- Validate target is 0D when input is 1D in `NLLLoss` (#161412)\r\n\r\n## Optimizer\r\n- Resolve warning in LBFGS when converting a tensor with `requires_grad=True` to a scalar (#160389)\r\n- Resolve `SequentialLR` deprecation warning about invoking `step(epoch)` (#149392)\r\n\r\n## Autograd\r\n- Support deterministic `torch.nn.Upsample` `mode=\"trilinear\"` backward (#154239)\r\n\r\n## Distributed\r\n### c10d\r\n  - Add improvements to eager init of `ProcessGroupNCCL` (#156748)\r\n  - Simplify unique hash management of `ProcessGroupNCCL` (#156790)\r\n  - Support per operation timeouts in `ProcessGroupGloo` (#158128)\r\n  - Allow ping to be retried in `TCPStore` (#159165)\r\n  - Support scalar tensor for functional `all_gather` (#149913)\r\n  - Expos `unsafe_get_ptr` for dist.ProcessGroupNCCL.NCCLConfig (#161136)\r\n  - Add batch option for `send/recv_object_list` (#160342)\r\n  - Make FakeStore optional to be passed into fake backend (#162164)\r\n  - Enable complex datatype support in `ProcessGroupGloo` (#156633)\r\n  - Move thread-local capture mode guard to include `work.isStarted` (#160398)\r\n### DistributedDataParallel (DDP)\r\n  - Support ddp zero hook XCCL path (#159240)\r\n### DTensor\r\n  - Relax `device_mesh` argument constraint in `local_map` (#157049)\r\n  - Support complex numbers in DTensor redistribute (#157329)\r\n  - Rework partial propagation in point-wise op and support mul (#157340)\r\n  - Allow dynamic shapes for `DTensor` slice (#157953)\r\n  - Implement `histc` op (#158298)\r\n  - Made dispatch to sharding prop over decomps (#159324)\r\n  - Support user-supplied Generator for random ops (#159933)\r\n  - Add `propagate_tensor_meta` function that skips cache if `_are_we_tracing` (#161334)\r\n  - Support `local_map` as a decorator (#161353)\r\n### Device Mesh\r\n  - Enable the use of user set backend and pg option even for the global mesh (#157501)\r\n  - Enable slicing a submesh with warnings (#158899)\r\n  - Allow controlling PG backend and options via `init_device_mesh` (#159371)\r\n### FullyShardedDataParallel2 (FSDP2)\r\n  - Support custom `all_gather` and `reduce_scatter` comms (#155189)\r\n  - Made it fail `set_allocate_memory_from_process_group` if used together with custom comm hooks (#157487)\r\n  - Use `reduceOpSum` when world size is 1 (#157529)\r\n  - Skipp `allgather` when world size is 1 (#160135)\r\n  - Use `post_reduce_stream.record_event()` on hsdp+cpuoffload (#160481)\r\n### Tensor Parallel (TP)\r\n  - Improve `parallelize_module` API to support more cases (#157182)\r\n### TensorPipe\r\n  - Update TensorPipe pinned dependency version (#159834)\r\n### TorchElastic\r\n  - Enable NUMA binding integration with elastic agent and `torchrun` (#149334)\r\n  - Support NUMA Binding for Callable Entrypoints (#160163, #161183)\r\n### Pipeline Parallelism (PP)\r\n  - Add `eval()` API to schedule (#157795)\r\n  - Allow intermediate nodes in zero bubble to have multiple grads (#159084)\r\n  - Support `OVERLAP_F_B` computation type (#158978)\r\n  - Initializ P2P communicators on first step (#160210)\r\n  - Add `DualPipeV` schedule (#159591)\r\n\r\n## Linear Algebra Frontend\r\n- Use rocSOLVER for Cholesky inversion on AMD. (#157154)\r\n- Add option for using TF32 as fp32 internal precision for matmul/linear/conv on MKLDNN (#157520)\r\n- Make einsum produce contiguous outputs in more cases (#161755)\r\n\r\n## Profiler\r\n- Add more CUDA API for kernel launcher (#156016)\r\n- Allow Custom Time Unit When Printing Profiler Table (#157913)\r\n- Update CUDA runtime kernel identification logic (#157890)\r\n\r\n## FX\r\n- Fix DCE eliminating random operations by improving `is_impure()` (#151524, #157981)\r\n- Support converting a float32 tensor to a scalar in FX trace. (#158216)\r\n- Correctly copy `self.module_stack` in ModuleStackTracer (#159956)\r\n- Add tool to track events in graph split (#159795)\r\n- Add `node_name_match` to subgraph rewriter (#157574)\r\n\r\n## Dynamo\r\n- Improve tracing support for various Python builtin data structures/modules:\r\n  - `list`s (e.g. #153969)\r\n  - `set`s (e.g. #153150)\r\n  - `dict`s (e.g. #154794)\r\n  - `iter` (e.g. #156371)\r\n  - `itertools` (e.g. #159693)\r\n  - `collections` (e.g. #159365)\r\n  - `collections.NamedTuple` (#159367)\r\n  - frozen `dataclasses.dataclass` (#159529)\r\n- Graph break error messages link to a website with more information (#159011)\r\n- Add option for `TorchDispatchMode` to ignore `torch.compile` internals (#161648)\r\n\r\n## Inductor\r\n- Add Inductor support for MTIA backend (#159211)\r\n- Share default device context when all graph partitions and cudagraph-unsafe ops are on the same device(#162873)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Enable AOTI for CPU on Windows (#158915)\r\n- Re-enable TMA templates w/ AOTI (#157819)\r\n- Don't allow int32 indices if `{non-inf, > int32_max}` upper bound is provided (#159433)\r\n- Add RecordFunction to C shim so that profiling works with AOTI (#159842)\r\n- Add AOTI C shim functions for collective ops (#154492)\r\n- Add missing ops to set of C-shim ops which can have nullptr returns (#158073)\r\n\r\n## Export\r\n- Handle `None` & ellipsis slicing/select in non-strict (#157821)\r\n- Extend FP8 types in serialization (#158430)\r\n- Improve error messages for deserialization (#159881)\r\n- Support serialization for `triton_kernel_wrapper_functional` HOP (#161314)\r\n- Support serialization for complex constants (#161517)\r\n- Add runtime asserts to `while_loop` HOP subgraphs (#158467)\r\n- Warn on side-effectful code in strict mode (#160060)\r\n- Support for vmap in pre-dispatch export (#154650)\r\n- Support vmap and custom autograd function/improve DTensor constructor inefficiency (#162240)\r\n\r\n## AOTDispatcher\r\n- Skip logging in fp8 activation quantization if there are no nodes to be quantized (#158129)\r\n- Add `aot_export_joint_with_descriptors` and `aot_compile_joint_with_descriptors` (#158715)\r\n- Extract out `prepare_aot_module_simplified` for use in next PR (#158319)\r\n- Rename modules in AOTAutograd (#158449)\r\n- Track descriptors for all inputs/outputs of AOTAutograd traced graph (#158624)\r\n- Improve graph output alias with subclass error message (#159619)\r\n- Pass fw/bw compilers to `aot_export_joint_with_descriptors` (#159814)\r\n\r\n## Composability\r\n- Meta implementation for `aten.add.Scalar` (#161332)\r\n- `aten.expand_copy` decomp (#161688)\r\n- Fix result dtype cast in decomp for `aten.linalg_vector_norm` (#155111)\r\n- Add dtype checks in meta implementation for several ordering ops (#159556)\r\n- Fix meta function for `aten.complex` (#160894)\r\n- Improve unbacked symint (dynamic shape) support for several decompositions (#148815, #156902, #157008, #158894, #159184, #160683, #160253, #162084, #162099, #162109, #160462)\r\n\r\n## Quantization\r\n- Avoid getting model device once per node for pt2e quantization flow (#159901)\r\n- Fixes bug in implementation of `HistogramObserver` (#156457)\r\n- Support `bias=None` for `fbgemm_linear_fp16_weight` CPU op (#158535)\r\n- Add Static Dispatch Kernel for `wrapped_fbgemm_linear_fp16_weight` for Sigmoid (#160451)\r\n\r\n## Nested Tensor (NJT)\r\n- Added initial `log_softmax()` support (#159662)\r\n\r\n## Foreach\r\n- Invoke `vector.reserve()` consistently for non-inplace foreach operations (#161128)\r\n- Faster and safer lambda expression capture in `has_integral_tensor()` (#161042)\r\n\r\n## ONNX\r\n- Support symbolic arguments in ONNX exporter (#157734)\r\n- Fix `torch.tensor` warning in ONNX `symbolic_opset10` export  (#158835)\r\n\r\n## C++ Frontend\r\n- Generalized `AllocatorConfig` to be device-agnostic via new `AcceleratorAllocatorConfig` (#149601, #150312)\r\n- Added `Scalar::isUnsigned()` method (#159877)\r\n- Exposed `ModelRunner` from nativert as public (#159989)\r\n- Improve error message for `torch.binomial` enforcing float inputs (#157658)\r\n\r\n## Build Frontend\r\n- Fix dev warning in `Dependencies.cmake` (#159702)\r\n- Fix building system gloo with CUDA/HIP (#146637)\r\n- Build `libtorch` without NVSHMEM (#160910)\r\n- Improve BLAS feature detection (#143846)\r\n\r\n## Release Engineering\r\n- Enable vLLM testing workflow (#160583, #161565, #162292, #162000, #161797)\r\n- Enable Windows ARM64 CI testing (#148753, #161504)\r\n- Enable PyTorch ROCm CI for MI355X testing. (#158889)\r\n\r\n## CUDA\r\n- Make cublaslt/hipblaslt workspaces persistent (#156495)\r\n- Remove unnecessary warnings during the ATen compilation process (#157703)\r\n- Slightly improve error message from `repeat_interleave` kernel (#157996)\r\n- Add framework for explanations for common CUDA errors (#158395)\r\n- Upgrade KernelLauncher `kernelLaunchCheck` to print help string (#158896)\r\n- Prep for cutlass upgrade by ignoring `Wunused-but-set-variable` (#159276)\r\n- Workaround ATen SFINAE under `libc++` (#161101)\r\n- Implement changes to CCCL (CUB/Thrust/LibCUDACXX) usage in ATen (#153373)\r\n- Add maybe unused flag to remove warning (#157655)\r\n- Use new CCCL API in v2.8 (#160554)\r\n- Improve cupy device placement when device is provided with explicit index (#158529)\r\n\r\n## CPU (AArch64)\r\n- Made PyTorch compilable with gcc-14 on ARM (#157867)\r\n\r\n## MPS\r\n- Add `shifted_chebyshev_polynomial_[tuvw]`, `igamma/igammac,grid_sampler_3d, native_dropout`/`native_dropout_backward`  (#157488, #161927, #160541, #162108)\r\n- Extend atomic operations to all int types (#158179)\r\n- Extend `index_put` to complex types (#160159)\r\n- Extend `addmm` to integral types (#160270)\r\n- Add support for unsigned types (#159094)\r\n- Add API to query GPU core count (#160414)\r\n- Add `kthvalue` (#161817)\r\n- Type-promote tensor-iterator common dtype (#160334)\r\n- Implement `logcumsumexp` metal kernel (#156858)\r\n- Enable `dlpack` integration (#158888)\r\n- Dynamic reductions (#159355)\r\n- Update `avg_pool2d` to use Metal kernel when `ceil_mode=True` (#161011)\r\n\r\n## ROCm\r\n- Additional hipify mappings (#158056, #158352, #161992)\r\n- Refactor `composable_kernel` (CK) backend user interface to improve user experience (#152951)\r\n- Allow use of `rocSOLVER` for Cholesky inversion. (#157154)\r\n- AOT Inductor enable gfx950 for max autotune using CK (#159195)\r\n- Add flag `torch.backends.miopen.immediate` to toggle MIOpen Immediate Mode instead of relying on `deterministic=True` and `benchmark=False` (#158951)\r\n- MIOpen convolutions no longer call `reshape_` or unexpectedly change memory formats (#161687)\r\n\r\n## XPU\r\n- Support Intel GPU quantization ops in AOTInductor (#156572)\r\n- Add `device_id` to Intel GPU properties to distinguish iGPUs with identical names (#156481)\r\n\r\n# Bug Fixes\r\n## Python Frontend\r\n- Add option in `torch.utils.cpp_extension.load_inline` to override gencode (#156850)\r\n- Fix `max_width` computation in Tensor printing (#126859)\r\n- Improve `pin_memory` error message on CPU-only systems (#159994)\r\n- Making batching rule for `F.embedding` DTensor-aware (#162117)\r\n\r\n## Autograd\r\n- Fix `torch.autograd.Function` memory leak due to `torch.utils.checkpiont` early stopping (#161171)\r\n- Fix `torch.autograd.graph.GradientEdge` for `torch.autograd.Function` (#160098)\r\n- Match 0-dim gradients device type regardless of subclass-ness (#160165)\r\n\r\n## Distributed\r\n### c10d\r\n  - Fix slow init due to repeated dns resolution failure in socket (#159596)\r\n  - Fix `setGroupName` and `setGroupDesc` in `group_split` and `merge_remote_group` (#159429)\r\n  - Fix a bug of distributed 'gather' with noncontiguous tensors on the Gloo backend (#158903)\r\n  - Fix a bug of distributed 'gather' with noncontiguous tensors on the NCCL backend (#159549)\r\n  - Fix data inconsistencies when using `batch_isend_irecv` with 2D tensor views by making P2P tensors dense (#163719)\r\n  - Handle discontiguous `allgather`/`reducescatter` inputs (#163712)\r\n### Device Mesh\r\n  - Fix the not incorrectly chained each of the strings as iterables (#160709)\r\n### DistributedDataParallel (DDP)\r\n  - Fix incorrect interaction between `DDPOptimizer` and donated buffers (#160745)\r\n### DTensor\r\n  - Fix DTensor handling of conjugate bit (#158030)\r\n  - Fix `OpSchema` equality check (#161231)\r\n  - Fix `grouped_mm` strategy for invalid stride cases (#158245)\r\n  - Fix `F.one_hot` in DTensor (#162307)\r\n  - Always disabled `ShardingPropagation` cache if compiling (#156868)\r\n### FullyShardedDataParallel (FSDP)\r\n  - Fix the bug in FSDP offload `pin_memory` (#157147)\r\n  - Fix to ensure writeback handles `NO_SHARD` correctly by flattening tensors before copying (#154369)\r\n### FullyShardedDataParallel2 (FSDP2)\r\n  - Fix error message for `fsdp_pre_all_gather` (#160817)\r\n  - Fix the issue with `set_reduce_scatter_divide_factor` errors and `MixedPrecisionPolicy`  (#155964)\r\n### Pipeline Parallelism (PP)\r\n  - Fix eval step under `no_grad()` (#159293)\r\n  - Fix zero bubble schedules for `eval()` (#159475)\r\n### TensorPipe\r\n  - Fix `import torch` if compiled without `TensorPipe` (#159461)\r\n### TorchElastic\r\n  - Fix wrong log file name in the docs of `torch.distributed.elastic.multiprocessing.start_processes()` (#160396)\r\n\r\n## Linear Algebra Frontend\r\n- Avoid downcasts for fp16 matmul on the BLAS backend (#161999)\r\n\r\n## Profiler\r\n- Fix Linter for Global Annotations flag in Snapshot (#157858)\r\n\r\n## FX\r\n- Fix `split_module` with symint (#160093)\r\n- Fix `getattr_recursive` with ModuleList (#161204)\r\n- Skip const folding with symbolic expression (#161437)\r\n- Fix qualified name for methods of `torch.Tensor` (#162224)\r\n\r\n## Dynamo\r\n- Fix segfault due to interaction between Dynamo backends and `torch.compiler.reset()` (#156527)\r\n- Fix crash due to bad interaction with recompilations and with blocks in Python 3.11+ (#162318)\r\n\r\n## torch.nn\r\n- Fix silent correctness w/ backpropping grads for `FlexAttention` (#163677)\r\n- Fix `return_lse` warning message in `FlexAttention` (#163578)\r\n- Fix `FlexAttention` head broadcast (#163426)\r\n\r\n## Inductor\r\n- Fix wrong meta function for `constant_pad_nd` (#159878)\r\n- Fix learnable bias assertion error in Inductor (#161170)\r\n- Fix int64 from `MutationOutput` Buffer (#162020)\r\n- Fix Inductor CUDA sort `NaN` behavior (#159308)\r\n- Fix layout for local buf in outer loop fusion (#160857)\r\n- Fix slice scatter `dtype` consistency (#160851)\r\n- Fix 3d tiled online softmax (#162341)\r\n- Fix unsafe collective reorder past wait in Inductor (#157489)\r\n- Fix `FallbackKernel` alias function to avoid incorrect aliasing for custom ops (#163227)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Fix a bug from `load_constants` (#161887)\r\n- Fix wrong propagation of fallback_ops_dict in `gen_aoti_c_shim` (#159904)\r\n- Fix unbacked symint and memory leak in Inductor memory planning (#159839)\r\n- Fix memory leak in AOTI when calling `aoti_torch_as_strided` (#162118)\r\n- Explicitly delete `wait_tensor` returned tensor (#159502)\r\n- Fix memory leak from `all_reduce` (#159818)\r\n\r\n## Composability\r\n- Make functionalization ViewMeta serializable with pickle (#163769)\r\n\r\n## Export\r\n- Fix bug in constants lifting pass (#157719)\r\n- Fix `from_node` provenance in unlift pass (#157943)\r\n- Fix `NaN` serialization (#155359)\r\n- Fix deserialization for unbacked symbol ranges (#158681)\r\n- Fix runtime assert handling in deserialization (#159060)\r\n- Fix for FQN handling in unflattener (#159418)\r\n- Fix `nn_module_stack` for `assert_tensor_metadata` nodes (#159625)\r\n- Fix usage for `move_to_device_pass` (#159992, #160528, #162301)\r\n- Avoid name overwrites for aliased exported module parameters (#160600)\r\n- Avoid inling `dynamo.disables` in unflattening (#161306)\r\n- Fix deserialization issue for storage offset (#162172)\r\n- Remove `.contiguous()` when saving weights to raw bytes to preserve original storage size of tensor (#163587)\r\n\r\n## Quantization\r\n- Avoid `NaN` in fp8 output of CPU `qlinear` and `qconv` ops (#160957)\r\n- Fix segmentation fault when `choose_qparams_optimized` (#161966)\r\n\r\n## Foreach\r\n- `chunk_size` should always be `int64_t` for Foreach functors (#156872)\r\n\r\n## ONNX\r\n- Make onnx export SDPA match ATen behavior (#159973)\r\n- Fix `rotary_embedding_23` implementation (#162865)\r\n- Fix export behavior when model has `None` as output (#160200)\r\n- Fix lower opset version support in `dynamo=True` (#161056)\r\n- Fix `index_put_` usage (#161263)\r\n\r\n## C++ Extensions\r\n- Fix CPP extension distributed warning for `TORCH_CUDA_ARCH_LIST` to only log when running on non-distributed or on rank 0 (#162764)\r\n\r\n## C++ Frontend\r\n- Fix `torch.utils.cpp_extension` parser for clang version 20.1.7+libcxx (#157666)\r\n- Fix `MakeTensor::computeStorageSize()` calculation (#158690)\r\n- Fix static initialization order issue with `AllocatorConfig` (#159629)\r\n\r\n## Build Frontend\r\n- Turn on `BUILD_BUNDLEPTXAS=1` to allow compile on newer GPUs(#163988)\r\n\r\n## CUDA\r\n- Handle uninitialized `torch.backends.cuda.matmul.fp32_precision` (#161102)\r\n- Fix nansum in non-JIT build (#158633)\r\n- Decrease launch bounds of CTCLoss backward for blackwell to avoid crash (#159522)\r\n- Implement workaround for `cudaErrorNotSupported` (#162412)\r\n- Fix missing `__syncthreads` in MultiMarginLoss backward (#158994)\r\n- Roll-back cuDNN frontend upgrade and update Meta registration due to compile issues (#163104)\r\n- Disable cuDNN for 3D convolutions with `kernel size != 1` for cuDNN 9.8+ (#163581)\r\n\r\n## CPU\r\n- Add check so non-aarch64 platforms can hit `MKLDNN` path (#162168)\r\n\r\n## MPS\r\n- Fix batch norm incorrect gradient (#156867)\r\n- Do not crash if `tensor dim > INT_MAX` (#158824)\r\n- Avoid outputing zeros from `exponential_` for MPS (#159386)\r\n- Fix MPS autocast for `ConvTranspose3d` (#160345)\r\n- Fix MPS `conv3d` autocast bias dtype mismatch (#160423)\r\n- Fix error check for `torch.var` on scalar (#160889)\r\n- Fix `index_add` for complex + int64, int64 input + zerodim index (#160926, #161511)\r\n- Fix `constant_pad_nd_mps` bug when pad is empty (#161149)\r\n- Fix `index_select` for `scalar_types` (#161206)\r\n- Fix `index_copy` for scalars and `index_copy` for strided indices (#161267, #161333)\r\n- Ensure that tensors are contiguous before using MPS linear kernel (#161641)\r\n- Address `NaN`s if SDPA is called with all values masked from query (#157727)\r\n- Fix invalid formatting (#158436)\r\n- Fix empty input in posneg functions (#161824)\r\n- Migrate round unary op to Metal (#161712)\r\n- Type-promote tensor-iterator common dtype (#160334)\r\n- Fix regression in 2.8.0 for `scaled_dot_product_attention` using MPS (#163598)\r\n- Chunk `fillBuffer` into 4Gb slices to avoid regression on MacOS 26 (#164108)\r\n- Fix latent bug that can result in segfault in CPP extensions (#164093)\r\n\r\n## ROCm\r\n- Fix Inductor with cudagraph trees `hip:0` device error (#161221)\r\n- Fix some build failures and support some BLAS calls on Windows (#161981)\r\n- Fix undefined symbol linker error after exposing MIOpen symbols on Windows (#156479)\r\n- Fix finding ROCm/HIP version on Windows (#156486)\r\n- Fix LoadHIP handling of environment variable paths on Windows (#159080)\r\n- Add hipcc compatibility flags to `cpp_extension.py` on Windows (#159790)\r\n- In SDPA via AOTriton, `logsumexp` needs scaling back to natural base (#156903)\r\n- Check stream graph capture status in `memcpy_and_sync` inline function (#158165)\r\n\r\n## XPU\r\n- Fix `cpp_extension` compatibility with `intel-deep-learning-essentials-2025.2` (#161012)\r\n\r\n## JIT\r\n- Make `ErrorReport::CallStack` thread-safe (#160386)\r\n- Fix `RemoveProfileNodesAndSpecializeTypes` handling for `Tensor?` that is resolved to `None` (#161538)\r\n\r\n# Performance\r\n## Optimizer\r\n- Use `addmm` to improve Newton‚ÄìSchulz orthogonalization in Muon (#161379)\r\n- Avoid stream sync in SWA `AveragedModel.update_parameters()` (#157705)\r\n\r\n## Autograd\r\n- Fix SVD forward-mode AD multiplication priority (#161027)\r\n\r\n## Dynamo\r\n- Recursive `dict` tag optimization for faster guard evaluation (#159183)\r\n\r\n## Inductor\r\n- Improve performance of A16W4 and A16W8 `GEMM` template (#159127, #161148)\r\n- More aggressive persistent reduction (#161055)\r\n- Add a few outer dimension reduction cases for LOAF (#162028)\r\n- Fuse two RoPE kernels into a single kernel and improving runtime efficiency (#161420)\r\n\r\n## Export\r\n- Caching optimizations for placeholder naming pass (#158594)\r\n- Add Static Dispatch Kernel for `fmod.Scalar` and `scale_gradient` (#160654, #160454)\r\n\r\n## CUDA\r\n- Use a nonblocking copy to avoid stream synchronization for GPU tensor indexing with CPU mask (#156384)\r\n- Disable cudagraph GCs by default to improve capture performance (#158649)\r\n\r\n## Release Engineering\r\n- Upgrade to ROCm 6.4.1 and 6.4.2 patch releases (#156636, #158887, #158886, #158651, #159001)\r\n- Migrate RPyTorch ROCm CI to MI325 capacity (#159059, #159649, #161184)\r\n- Enable B200 PyTorch benchmark testing (#158011, #157341)\r\n\r\n## MPS\r\n- Optimize cummin/cummax metal kernels (#156794)\r\n- Speedup `torch.full` for 1-byte types (#158874)\r\n- Speedup `argmax`/`argmin` (#159524)\r\n- Improve performance of `max_pool3d` (#157875)\r\n- Avoid calling tensor ops in `max_pool3d` impl (#157874)\r\n- Move `max_pool2d` to Metal for `stride != 1` (#157876)\r\n\r\n## ROCm\r\n- SDPA now uses AOTriton to 0.11b (#161754)\r\n- `hipblaslt` is used by default on gfx908 for ROCm >= 6.3 (#159092)\r\n- Enable miopen channels last 3d for conv and batchnorm (#160529)\r\n- Remove extra transposes in NHWC convolutions on MIOpen (#160435)\r\n- Remove extra sync in `tensor.item()` (#158486)\r\n- Elementwise and reduction kernel perf improvements (#159430, #159652, #160444, #160466, #161054, #161180, #161181)\r\n- Enable build of `fbgemm_gpu genai` sources for grouped GEMM support (#160676)\r\n\r\n## XPU\r\n- Enable tensor memory descriptor Triton template for Intel GPU (#161600)\r\n\r\n# Documentation\r\n## Python Frontend\r\n- Improve documentation for `torch.lobpcg`, `torch.clone`, `torch.matmul`, `torch.max`, `torch.gather`, `torch.Tensor.scatter_`, `torch.empty_like`, `torch.randint`, `torch.mul`, `torch.min`, `torch.max`. `torch.sort`, `torch.full_like`, `torch.histogramdd`, `torch.hamming_window` (#156139, #157007, #161424, #156153, #157929, #157920, #158050, #158731, #160312, #161539, #162051, #158275, #152682)\r\n- Remove torchscript related sections in serialization docs (#156648)\r\n- Fix typo in `torch.set_float32_matmul_precision` docs (#158191)\r\n- Fix docstring for `torch.nn.utils.clip_grads_with_norm_` to reflect clamping behavior (#158200)\r\n- Fix the Doc issue on the description of edge_order in `torch.gradient` (#159130)\r\n- Add `torch.segment_reduce` docs (#154352)\r\n- Add examples to `torch.is_floating_point` and `torch.is_complex` docs (#161951)\r\n## torch.nn\r\n- Improve description of `padding` for `avg_poolnd` (#159142)\r\n- Improve `CrossEntropyLoss` docs with example of incorrect target specification (#155649)\r\n- Remove redundant dtype conversion in `scaled_dot_product_attention` example (#161613)\r\n\r\n## Optimizer\r\n- Document specific optimizer modules APIs e.g., `torch.optim.adam.Adam`, properly (#158483, #158669, #160194)\r\n- Add note for clarity in Adafactor doc #154862 (#155248)\r\n- Minorly improve `zero_grad` description (#161239)\r\n\r\n## Autograd\r\n- Improve `torch.inference_mode` docs and error message (#161164)\r\n\r\n## Distributed\r\n### c10d\r\n  - Documented barrier collective's interaction with `device_id` (#159389)\r\n  - Fix comment to match logic in `distributed_c10d.py` (#162158)\r\n### DTensor\r\n  - Rewrote doc of `TupleStrategy` (#158132)\r\n  - Documented `redistribute_costs` (#158495)\r\n### FullyShardedDataParallel (FSDP)\r\n  - Removed FSDP1 developer note (#158991)\r\n\r\n## Profiler\r\n- Update PT2 Profiler Torch-Compiled Region Image (#158066)\r\n- Fix Experimental Config Documentatation(#156586)\r\n- Update README (#159816)\r\n\r\n## FX\r\n- Fix typos in `torch/` (`torch/fx/`, #156604)\r\n- Add typing (#158450)\r\n- Fix typo in FX interpreter class docs (#162055)\r\n- Remove allow-untyped-defs from `torch/fx/experimental/migrate_gradual_types/util.py` (#157236)\r\n\r\n## Inductor\r\n- Add documentation for CUDAGraph partition (#159450)\r\n\r\n## Export\r\n- Update docs around draft export, dynamism, and PT2 Archive (#157750)\r\n\r\n## ONNX\r\n- Update export docstring (#162622)\r\n- Delete deprecated tutorial page link (#157310)\r\n- Filter out torchscript sentences (#158850)\r\n- Fix doc typo for `symbolic_multi_out` (#160702)\r\n- `onnx.md` to simplify deprecated entities (#159312)\r\n- Update export docstring and set `fallback=False` by default (#162622, #162726)\r\n- Fix typo in error message: summit -> submit (#162587)\r\n\r\n## Release Engineering\r\n- Add decorator to create deprecation warnings (#155127)\r\n- Add runnable code examples to export documentation (#158506)\r\n- Add developer notes for integrating new backends into PyTorch (#158644)\r\n\r\n## XPU\r\n- Update supported OS to Windows 11 & Ubuntu 24.04/25.04 for Intel client GPU (#161699)\r\n\r\n# Security\r\n## Python Frontend\r\n- Don't store flamegraph to tmp folder (#157374)\r\n\r\n# Developers\r\n## Python Frontend\r\n- Better sample inputs for addmm OpInfo (#160234)\r\n\r\n## Distributed\r\n### c10d\r\n  - Add `waitcounter` for watchdog and heartbeat monitoring thread (#157480)\r\n  - Made `torch.distributed.breakpoint` set a long timeout (#158481)\r\n  - Add `check_rng_sync` util (#160283)\r\n  - Add `FlightRecorder` support for `ProcessGroupXCCL` (#158568)\r\n  - Add `early_stop` kwarg to `torch.utils.checkpoint` (#160781)\r\n### DTensor\r\n  - Wrap sharding prop error with contextual exception (#161574)\r\n  - Add check if tracing for sharding propagation to handle un-hashable keys in DTensor (#160798)\r\n### Device Mesh\r\n  - Add error when users try to slice non contiguous flattened dim submesh (#157523)\r\n  - Make the repr shorter when debug ENV not set (#158822)\r\n### ShardedTensor\r\n  - Make error message descriptive in ShardedTensor creation (#150627, #159423)\r\n### Pipeline Parallelism (PP)\r\n  - Add profiling to schedule execution (#160753)\r\n\r\n## FX\r\n- Consolidate stack trace in Tracer (#156257, #157302, #158266)\r\n- Separate provenance tracking to different levels (#160383, #158399, #158796, #159484)\r\n- Fix `register_foward_pre_hook not supported on ScriptModule` error (#156904)\r\n- Add `__eq__` function to NodeSource (#158170)\r\n- Add `__hash__` function to NodeSource (#158322)\r\n- Cache dict and string rep for better perf in NodeSource (#158372)\r\n- Recover node source from dict (#158373, #158473)\r\n- Include error stacktrace and graph module in `tlparse` error (#158469)\r\n- Add `expanded_def` option for FX printing, render descriptor, update tests (#158708)\r\n- Remove `co_lnotab` in favor of `co_linetable` (#159227)\r\n- Remove duplicate imports (#161685)\r\n- Include Output tensor metadata for `CompiledFxGraph` (#159311)\r\n\r\n## Inductor\r\n- Deprecate `allow_tf32` in `tl.dot(..., allow_tf32=...)`, use `tl.dot(..., input_precision=...)` (#160711)\r\n- Log autotune choices and benchmark result to scuba/chrome trace (#159496)\r\n- Add TLParse artifact for logging runtime of collective and compute ops (#159730)\r\n- Call `jit_post_compile_hook` within Inductor Triton Kernel compile path (#161443)\r\n- Prune configs that require more shared memory than the hardware limit (#161996)\r\n- Runtime estimations using nccl estimator on mm only benchmark mode (#161405)\r\n- Don't use `torch.backends.cuda.matmul.allow_tf32` in Inductor cache key (#159480)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Better error message when no .so/cpp files are found (#156863)\r\n- Clean up old APIs in AOTI c shim (#158400)\r\n- Add Inductor provenance mapping for cpp extern kernel (#161656, #162069)\r\n- Print out error msg when nvcc compiler fails (#157203)\r\n- Add kernel information JSON generation for AOTI packages (#160540)\r\n\r\n## Composability\r\n- Stop suggesting to use `guard_size_oblivious` on data dependent errors (#160510)\r\n- Avoid unnecessary slices resulting in data-dependent errors (#157528)\r\n\r\n## Quantization\r\n- Revamp dtype documentation (#156087)\r\n- Use new type statement to fix public API of types (#158487)\r\n\r\n## Dataloader Frontend\r\n- Add `torch.utils.data` samplers benchmark script (#156974)\r\n- Add `torch.utils.data.Dataloader` benchmark script (#159432)\r\n\r\n## Release Engineering\r\n- Replace `setup.py develop` with `pip install -e` for development builds (#155998, #156027, #156710)  (#156709)\r\n\r\n## XPU\r\n- Upgrade Intel GPU software stack package to intel-deep-learning-essentials-2025.2 (#158733)\r\n",
        "published_at": "2025-10-15T17:12:27Z",
        "target_commitish": "main",
        "version_tuple": [
          2,
          9,
          0
        ],
        "version_key": "2.9.0"
      },
      {
        "tag_name": "v2.8.0",
        "name": "PyTorch 2.8.0 Release",
        "body": "# PyTorch 2.8.0 Release Notes\r\n- [Highlights](#highlights)\r\n- [Backwards Incompatible Changes](#backwards-incompatible-changes)\r\n- [Deprecations](#deprecations)\r\n- [New Features](#new-features)\r\n- [Improvements](#improvements)\r\n- [Bug fixes](#bug-fixes)\r\n- [Performance](#performance)\r\n- [Documentation](#documentation)\r\n- [Developers](#developers)\r\n\r\n\r\n# Highlights\r\n<table>\r\n  <tr>\r\n   <td><strong>Unstable</strong></td>\r\n  </tr>\r\n  <tr>\r\n   <td>torch::stable::Tensor</td>\r\n  </tr>\r\n  <tr>\r\n   <td>High-performance quantized LLM inference on Intel CPUs with native PyTorch</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Experimental Wheel Variant Support</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Inductor CUTLASS backend support</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Inductor Graph Partition for CUDAGraph</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Control Flow Operator Library</td>\r\n  </tr>\r\n  <tr>\r\n   <td>HuggingFace SafeTensors support in PyTorch Distributed Checkpointing</td>\r\n  </tr>\r\n  <tr>\r\n   <td>SYCL support in PyTorch CPP Extension API</td>\r\n  </tr>\r\n  <tr>\r\n   <td>A16W4 on XPU Device</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Hierarchical compilation with torch.compile</td>\r\n  </tr>\r\n  <tr>\r\n   <td>Intel GPU distributed backend (XCCL) support</td>\r\n  </tr>\r\n</table>\r\n\r\nFor more details about these highlighted features, you can look at the [release blogpost](https://pytorch.org/blog/pytorch-2-8/).\r\nBelow are the full release notes for this release.\r\n\r\n# Tracked Regressions\r\n### Windows wheel builds with CUDA 12.9.1 stack overflow during build (#156181)\r\nDue to a bug introduced in CUDA 12.9.1, we are unable to complete full Windows wheel builds with this\r\nversion, as compilation of `torch.segment_reduce()` crashes the build. Thus, we provide a wheel\r\nwithout `torch.segment_reduce()` included in order to sidestep the issue. If you need support\r\nfor `torch.segment_reduce()`, please utilize a different version.\r\n\r\n# Backwards Incompatible Changes\r\n\r\n## CUDA Support\r\n### Removed support for Maxwell and Pascal architectures with CUDA 12.8 and 12.9 builds (#157517, #158478, #158744)\r\nDue to binary size limitations, support for sm50 - sm60 architectures with CUDA 12.8 and 12.9 has\r\nbeen dropped for the 2.8.0 release. If you need support for these architectures, please utilize\r\nCUDA 12.6 instead.\r\n\r\n## Python Frontend\r\n### Calling an op with an input dtype that is unsupported now raises `NotImplementedError` instead of `RuntimeError` (#155470)\r\nPlease update exception handling logic to reflect this.\r\n\r\nIn 2.7.0\r\n```\r\ntry:\r\n    torch.nn.Hardshrink()(torch.randint(0, 5, (10,)))\r\nexcept RuntimeError:\r\n    ...\r\n```\r\n\r\nIn 2.8.0\r\n```\r\ntry:\r\n    torch.nn.Hardshrink()(torch.randint(0, 5, (10,)))\r\nexcept NotImplementedError:\r\n    ...\r\n```\r\n\r\n### Added missing in-place on view check to custom `autograd.Function` (#153094)\r\n\r\nIn 2.8.0, if a custom `autograd.Function` mutates a view of a leaf requiring grad,\r\nit now properly raises an error. Previously, it would silently leak memory.\r\n```\r\n   class Func(torch.autograd.Function):\r\n        @staticmethod\r\n        def forward(ctx, inp):\r\n            inp.add_(1)\r\n            ctx.mark_dirty(inp)\r\n            return inp\r\n\r\n        @staticmethod\r\n        def backward(ctx, gO):\r\n            pass\r\n\r\n    a = torch.tensor([1.0, 2.0], requires_grad=True)\r\n    b = a.view_as(a)\r\n    Func.apply(b)\r\n```\r\nOutput:\r\n\r\nVersion 2.7.0\r\n```\r\nRuns without error, but leaks memory\r\n```\r\nVersion 2.8.0\r\n```\r\nRuntimeError: a view of a leaf Variable that requires grad is being used in an in-place operation\r\n```\r\n\r\n### An error is now properly thrown for the out variant of `tensordot` when called with a `requires_grad=True` tensor (#150270)\r\n\r\nPlease avoid passing an out tensor with `requires_grad=True` as gradients cannot be\r\ncomputed for this tensor.\r\n\r\nIn 2.7.0\r\n```\r\na = torch.empty((4, 2), requires_grad=True)\r\nb = torch.empty((2, 4), requires_grad=True)\r\nc = torch.empty((2, 2), requires_grad=True)\r\n# does not error, but gradients for c cannot be computed\r\ntorch.tensordot(a, b, dims=([1], [0]), out=c)\r\n```\r\n\r\nIn 2.8.0\r\n```\r\na = torch.empty((4, 2), requires_grad=True)\r\nb = torch.empty((2, 4), requires_grad=True)\r\nc = torch.empty((2, 2), requires_grad=True)\r\ntorch.tensordot(a, b, dims=([1], [0]), out=c)\r\n# RuntimeError: tensordot(): the 'out' tensor was specified and requires gradients, and\r\n# its shape does not match the expected result. Either remove the 'out' argument, ensure\r\n# it does not require gradients, or make sure its shape matches the expected output.\r\n```\r\n\r\n## torch.compile\r\n### Specialization of a tensor shape with `mark_dynamic` applied now correctly errors (#152661)\r\n\r\nPrior to 2.8, it was possible for a guard on a symbolic shape to be incorrectly\r\nomitted if the symbolic shape evaluation was previously tested with guards\r\nsuppressed (this often happens within the compiler itself). This has been fixed\r\nin 2.8 and usually will just silently \"do the right thing\" and add the correct\r\nguard. However, if the new guard causes a tensor marked with `mark_dynamic` to become\r\nspecialized, this can result in an error. One workaround is to use\r\n`maybe_mark_dynamic` instead of `mark_dynamic`.\r\n\r\nSee the discussion in issue #157921 for more\r\ncontext.\r\n\r\nVersion 2.7.0\r\n```python\r\nimport torch\r\n\r\nembed = torch.randn(2, 8192)\r\nx = torch.zeros(8192)\r\n\r\ntorch._dynamo.mark_dynamic(x, 0)\r\n\r\n@torch.compile\r\ndef f(embedding_indices, x):\r\n    added_tokens_mask = torch.where(x > 10000, 1, 0)\r\n    ei = torch.narrow(embedding_indices, 1, 0, x.size(0))\r\n    return ei.clone()\r\n\r\nf(embed, x)\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nimport torch\r\n\r\nembed = torch.randn(2, 8192)\r\nx = torch.zeros(8192)\r\n\r\ntorch._dynamo.maybe_mark_dynamic(x, 0)\r\n\r\n@torch.compile\r\ndef f(embedding_indices, x):\r\n    added_tokens_mask = torch.where(x > 10000, 1, 0)\r\n    ei = torch.narrow(embedding_indices, 1, 0, x.size(0))\r\n    return ei.clone()\r\n\r\nf(embed, x)\r\n```\r\n\r\n### Several config variables related to `torch.compile` have been renamed or removed\r\n- Dynamo config variable `enable_cpp_framelocals_guard_eval` has changed to no longer have any effect (#151008).\r\n- Inductor config variable `rocm.n_max_profiling_configs` is deprecated (#152341).\r\nInstead, use ck-tile based configs `rocm.ck_max_profiling_configs` and\r\n`rocm.ck_tile_max_profiling_configs`.\r\n- Inductor config variable `autotune_fallback_to_aten` is deprecated (#154331).\r\nInductor will no longer silently fall back to `ATen`. Please add `\"ATEN\"` to\r\n`max_autotune_gemm_backends` for the old behavior.\r\n- Inductor config variables `use_mixed_mm` and `mixed_mm_choice` are deprecated (#152071). Inductor now supports prologue fusion, so there is no need for\r\nspecial cases now.\r\n- Inductor config setting `descriptive_names = False` is deprecated (#151481). Please use one of the other available\r\noptions: `\"torch\"`, `\"original_aten\"`, or `\"inductor_node\"`.\r\n- `custom_op_default_layout_constraint` has moved from inductor config to functorch config (#148104). Please reference it via\r\n`torch._functorch.config.custom_op_default_layout_constraint` instead of\r\n`torch._inductor.config.custom_op_default_layout_constraint`.\r\n- AOTI config variable `emit_current_arch_binary` is deprecated (#155768).\r\n- AOTI config variable `aot_inductor.embed_cubin` has been renamed to `aot_inductor.embed_kernel_binary` (#154412).\r\n- AOTI config variable `aot_inductor.compile_wrapper_with_O0` has been renamed to `compile_wrapper_opt_level` (#148714).\r\n\r\n### Added a stricter aliasing/mutation check for `HigherOrderOperator`s (e.g. `cond`), which will explicitly error out if alias/mutation among inputs and outputs is unsupported (#148953, #146658).\r\n\r\nFor affected `HigherOrderOperator`s, add `.clone()` to aliased outputs to address this.\r\n\r\nVersion 2.7.0\r\n```python\r\nimport torch\r\n\r\n@torch.compile(backend=\"eager\")\r\ndef fn(x):\r\n    return torch.cond(x.sum() > 0, lambda x: x, lambda x: x + 1, [x])\r\n\r\nfn(torch.ones(3))\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nimport torch\r\n\r\n@torch.compile(backend=\"eager\")\r\ndef fn(x):\r\n    return torch.cond(x.sum() > 0, lambda x: x.clone(), lambda x: x + 1, [x])\r\n\r\nfn(torch.ones(3))\r\n```\r\n\r\n### `guard_or_x` and `definitely_x` have been consolidated (#152463)\r\nWe removed `definitely_true` / `definitely_false` and associated APIs, replacing them with\r\n`guard_or_true` / `guard_or_false`, which offer similar functionality and can be used to\r\nachieve the same effect. Please migrate to the latter.\r\n\r\nVersion 2.7.0\r\n```python\r\nfrom torch.fx.experimental.symbolic_shapes import definitely_false, definitely_true\r\n\r\n...\r\nif definitely_true(x):\r\n  ...\r\n\r\nif definitely_false(y):\r\n  ...\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nfrom torch.fx.experimental.symbolic_shapes import guard_or_false, guard_or_true\r\n\r\n...\r\nif guard_or_false(x):\r\n  ...\r\n\r\n# alternatively: if guard_or_false(torch.sym_not(y))\r\nif not guard_or_true(y):\r\n  ...\r\n```\r\n\r\n## torch.export\r\n### `torch.export.export_for_inference` has been removed in favor of `torch.export.export_for_training().run_decompositions()` (#149078)\r\n\r\nVersion 2.7.0\r\n```python\r\nimport torch\r\n\r\n...\r\nexported_program = torch.export.export_for_inference(mod, args, kwargs)\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nimport torch\r\n\r\n...\r\nexported_program = torch.export.export_for_training(\r\n    mod, args, kwargs\r\n).run_decompositions(decomp_table=decomp_table)\r\n```\r\n\r\n### Switched default to `strict=False` in `torch.export.export` and `export_for_training` (#148790, #150941)\r\n\r\nThis differs from the previous release default of `strict=True`. To revert to the old default\r\nbehavior, please explicitly pass `strict=True`.\r\n\r\nVersion 2.7.0\r\n```python\r\nimport torch\r\n\r\n# default behavior is strict=True\r\ntorch.export.export(...)\r\ntorch.export.export_for_training(...)\r\n```\r\n\r\nVersion 2.8.0\r\n```python\r\nimport torch\r\n\r\n# strict=True must be explicitly passed to get the old behavior\r\ntorch.export.export(..., strict=True)\r\ntorch.export.export_for_training(..., strict=True)\r\n```\r\n\r\n## ONNX\r\n### Default opset in `torch.onnx.export` is now 18 (#156023)\r\n\r\nWhen `dynamo=False`, the default ONNX opset version has been updated from 17 to 18. Users can set `opset_version` to explicitly select an opset version.\r\n\r\nVersion 2.7\r\n\r\n```py\r\n# opset_version=17\r\ntorch.onnx.export(...)\r\n```\r\n\r\nVersion 2.8\r\n\r\n```py\r\n# To preserve the original behavior\r\ntorch.onnx.export(..., opset_version=17)\r\n\r\n# New: opset_version=18\r\ntorch.onnx.export(...)\r\n```\r\n\r\n### The `JitTraceConvertStrategy` has been removed (#152556)\r\n\r\nSupport for JIT traced and scripted modules in the ONNX exporter when `dynamo=True` has been removed. You are encouraged to export an nn.Module directly, or create an `ExportedProgram` using `torch.export` before exporting to ONNX.\r\n\r\n### `onnxscript>=0.3.1` is required for the `dynamo=True` option (#157017)\r\n\r\nYou must upgrade `onnxscript` to version 0.3.1 or higher for it to be compatible with PyTorch 2.8.\r\n\r\n## Build Frontend\r\n### Removed the `torch/types.h` include from `Dispatcher.h` (#149557)\r\nThis can cause build errors in C++ code that implicitly relies on this include (e.g. very old versions of `torchvision`).\r\n\r\nNote that `Dispatcher.h` does not belong as an include from `torch/types.h` and was only present as a\r\nshort-term hack to appease `torchvision`. If you run into `torchvision` build errors, please\r\nupdate to a more recent version of `torchvision` to resolve this.\r\n\r\n### Upgraded `DLPack` to 1.0 (#145000)\r\nAs part of the upgrade, some of the `DLDeviceType` enum values have been renamed. Please switch\r\nto the new names.\r\n\r\nVersion 2.7.0\r\n```\r\nfrom torch.utils.dlpack import DLDeviceType\r\n\r\nd1 = DLDeviceType.kDLGPU\r\nd2 = DLDeviceType.kDLCPUPinned\r\n...\r\n```\r\n\r\nVersion 2.8.0\r\n```\r\nfrom torch.utils.dlpack import DLDeviceType\r\n\r\nd1 = DLDeviceType.kDLCUDA  # formerly kDLGPU\r\nd2 = DLDeviceType.kDLCUDAHost  # formerly kDLCPUPinned\r\n...\r\n```\r\n\r\n### NVTX3 code has been moved from `cmake/public/cuda.cmake` to `cmake/Dependencies.cmake` (#151583)\r\n\r\nThis is a BC-breaking change for the build system interface. Downstream projects that previously got NVTX3 through `cmake/public/cuda.cmake`\r\n(i.e.. calling `find_package(TORCH REQUIRED)`) will now need to explicitly configure NVTX3 support in the library itself (i.e. use `USE_SYSTEM_NVTX=1`).\r\nThe change is to fix the broken behavior where downstream projects couldn't find NVTX3 anyway due to the `PROJECT_SOURCE_DIR` mismatch.\r\n\r\nVersion 2.7.0:\r\n- A downstream project using `-DUSE_SYSTEM_NVTX` would be able to find NVTX3 and `torch::nvtx3` via PyTorch's `cmake/public/cuda.cmake` logic.\r\n- A downstream project NOT using `-DUSE_SYSTEM_NVTX` would encounter build errors with CUDA 12.8 or above.\r\n\r\nVersion 2.8.0:\r\n- A downstream project using `-DUSE_SYSTEM_NVTX` will not be able to find NVTX3 or `torch::nvtx3` via PyTorch's `cmake/public/cuda.cmake`. The downstream project now needs to explicitly find NVTX3 and torch::nvtx3 by implementing the same logic in PyTorch's `cmake/Dependences.cmake`.\r\n- A downstream project NOT using `-DUSE_SYSTEM_NVTX` will proceed building without NVTX unless another part of the build process re-enables NVTX.\r\n\r\n# Deprecations\r\n### MPS support for MacOS Ventura will be removed in 2.9\r\nPyTorch 2.8 is the last release that will support GPU acceleration on MacOS Ventura. In the next\r\nrelease (2.9), MacOS Sonoma (released in Sept. 2023) or above will be required to use the MPS\r\nbackend.\r\n\r\n### `torch.ao.quantization` is deprecated and will be removed in 2.10 (#153892)\r\nTo migrate:\r\n- Eager mode quantization (`torch.ao.quantization.quantize`, `torch.ao.quantization.quantize_dynamic`)\r\n  - Weight-only and dynamic quantization: use `torchao` eager mode `quantize_`.\r\n  - Static quantization: use `torchao` PT2E quantization.\r\n- FX graph mode quantization (`torch.ao.quantization.quantize_fx.prepare_fx`, `torch.ao.quantization.quantize_fx.convert_fx`): use `torchao` PT2E quantization (`torchao.quantization.quantize_pt2e.prepare_pt2e`, `torchao.quantization.quantize_pt2e.convert_pt2e`).\r\n\r\nNote that PT2E quantization has been migrated to `torchao` (https://github.com/pytorch/ao/tree/main/torchao/quantization/pt2e). See https://github.com/pytorch/ao/issues/2259 and https://docs.pytorch.org/ao/main/quick_start.html#pytorch-2-export-quantization for more details.\r\n\r\n### The `dynamo=False` (current default) option for `torch.onnx.export` is deprecated (#152478, #155580)\r\n\r\nThe default will be `dynamo=True` starting from PyTorch 2.9. You are encouraged to migrate to use the `dynamo=True` option in `torch.onnx.export`. This flag makes `torch.export.export` the default export path, replacing `TorchScript`.\r\n\r\nTo maintain the old behavior, set `dynamo=False` explicitly. You are encouraged to also experiment with the `fallback=True` option that will make the exporter fall back to the `dynamo=False` path if there are errors.\r\n\r\n# New Features\r\n## CUDA\r\n- Support capture of event record and wait in CUDAGraphs for timing (#155372)\r\n\r\n## torch.compile\r\n#### Dynamo\r\n- Added support for hierarchical compilation via `nested_compile_region` (#156449)\r\n- Allow guards to be dropped with custom filter functions via `guard_filter_fn` (#150936)\r\n- Added `dont_skip_tracing` decorator to skip over most Dynamo `skipfiles` rules (#150586)\r\n\r\n#### Inductor\r\n- Added support for mapping a Dynamo graph to multiple different Inductor graphs, which can be optimized separately (#147648, #147038)\r\n\r\n## torch.export\r\n- Introduced [`draft-export`](https://docs.pytorch.org/docs/main/export/draft_export.html), an export variant designed to consistently produce a graph and generate a debugging report of issues encountered during tracing (#152637, #153219, #149465, #153627, #154190, #155744, #150876, #150948, #151051, #151065, #150809, #151797)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Added support for `TorchBind` objects (#150196, #154265)\r\n- Added config variable `aot_inductor.model_name_for_generated_files` for specifying model name (#154129)\r\n\r\n## MPS\r\n- `MPSInductor`: `torch.compile` for Apple GPUs (#150121, #149342, #151449, #151754, #149687, #149180, #149221, #153598, #152788, #153787, #152214, #151152, #155891, #154578, #151272, #151288, #153997, #151871, #153362, #156566, #150661, #153582)\r\n\r\n## ONNX\r\n- Added new strategy `draft_export` (#147529, [docs](https://docs.pytorch.org/docs/main/draft_export.html)) to provide debugging information upon data-dependent / constraint errors when obtaining an `ExportedProgram` with `torch.onnx.export`\r\n\r\n- Added support for symbolic operators in the `dynamo=True` export path (#148905, #149678, #150038, [docs](https://docs.pytorch.org/docs/main/onnx_ops.html#symbolic-operators)). Two operators `torch.onnx.ops.symbolic` and `torch.onnx.ops.symbolic_multi_out` are defined to allow you to create symbolic ONNX operators directly in your PyTorch models. You can use them in a `forward` method:\r\n\r\n```python\r\ndef forward(self, x: torch.Tensor) -> torch.Tensor:\r\n    # Optionally use is_in_onnx_export to control the behavior during onnx export\r\n\r\n    if torch.onnx.is_in_onnx_export():\r\n        # Create a symbolic ONNX operator with the name \"CustomOp\" in the \"custom_domain\" domain.\r\n        # The output tensor will have the specified dtype and shape\r\n        return torch.onnx.ops.symbolic(\r\n            \"custom_domain::CustomOp\",\r\n            (x,),\r\n            dict(attr_key=\"attr_value\"),\r\n            dtype=x.dtype,\r\n            shape=x.shape,\r\n            version=1,\r\n        )\r\n    else:\r\n        return x\r\n```\r\n\r\n## Python Frontend\r\n- Added Generalized Pareto Distribution (GPD) (#135968)\r\n\r\n## Quantization\r\n- Introduced `torch.float4_e2m1fn_x2` dtype (#148791)\r\n\r\n## XPU\r\n- Support Intel distributed backend (XCCL) (#141856)\r\n- Support SYCL kernels through C++ extension (#132945)\r\n\r\n# Improvements\r\n## Build Frontend\r\n- Removed outdated warning about `TORCH_CUDA_ARCH_LIST` (#152715, #155314)\r\n- Made Eigen an optional build dependency (#155955)\r\n- Updated CUTLASS to 3.9.2 (#152779)\r\n\r\n## Composability\r\n- Enhanced custom op support with serializable op profiles and fake registration overrides (#151817, #150807, #150806)\r\n\r\n## C++ Frontend\r\n- Exposed `bicubic` mode for `torch::nn::functional::grid_sample` (#150817)\r\n\r\n## CUDA\r\n- Introduced `no_implicit_headers` mode for `load_inline()` on custom CUDA extensions (#149480)\r\n- Support large batch sizes in SDPA memory-efficient attention backend (#154029, #154663)\r\n- Fixed invalid indexing in SDPA memory-efficient attention backward (#155397)\r\n- Support SDPA attention backends on sm121 (DGX Spark) (#152314)\r\n- Added FP8 row-wise scaled-mm for sm12x (GeForce Blackwell) (#155991)\r\n\r\n## cuDNN\r\n- Updated cuDNN frontend version to 1.12 (#153888)\r\n\r\n## Distributed\r\n#### c10d\r\n- Enhanced `TCPStore` with clone and queuing features (#150966, #151045, #150969, #151485)\r\n- Added a collective time estimator for NCCL comms (#149343)\r\n- Made `getDefaultBackend` more fault tolerant without relying on exceptions (#149152)\r\n- Specified the default PyTorch Distributed backend for MPS (#149538)\r\n- Supported `masterListenFd` in `TCPStoreLibUvBackend` (#150215)\r\n- Used shared stores in gloo (#150230)\r\n- Improved FR dump robustness with all watchdog broadcast wait, reduce dump timeout and shrinked mutex range (#150652, #151329, #155949)\r\n- Added the record of each individual collective being coalesced in FR (#151238)\r\n- Implemented safer book-keeping of NCCL communicators (#150681)\r\n- Clarified behavior of `TORCH_NCCL_USE_TENSOR_REGISTER_ALLOCATOR_HOOK` (#150682)\r\n- Registered also future allocations in mempool with NCCL (#150684)\r\n- Avoided computing `global_rank` when `group_rank` is used (#151373)\r\n- Exposed NCCL communicator from `ProcessGroupNCCL` via an unsafe API (#152496)\r\n- Added split sizes info dump for uneven all2all bw calculation (#151438)\r\n- Made FR vendor neutral so that other backends can use it and integrated into gloo. (#152585, #152563, #154929, #152614)\r\n- Added `needs_contiguous_strides` tag in functional collective (#153399, #153523)\r\n- Allowed `split_group` to work with non-nccl backends (#152175)\r\n- Simplified `new_subgroups()` by using `new_subgroups_by_enumeration()` (#153843)\r\n- Made only current thread allocate to pool in `ProcessGroupNCCL` (#153990)\r\n- Enabled using `c10::Half` for gloo (#153862)\r\n- Released GIL in PG destructor (#154976)\r\n- Enhanced `get_process_group_ranks()` to accept `group=None` (#154902)\r\n- Skipped updating the default device distributed backend if already registered (#155320)\r\n- Enabled querying the build and runtime NCCL versions (#156305)\r\n- Disabled NCCL NVLS when using deterministic mode (#156381)\r\n- Made `init_process_group` support index-only device id (#156214)\r\n- Support enabling / disabling NaN detector per-`ProcessGroup` (#151723)\r\n- Added support for `reduce_scatter` and `ReduceOp::AVG` in `ProcessGroupGloo` (#149781, #149869)\r\n- Added FP8 support in `ProcessGroupNCCL` (#152706)\r\n- Added `ibverbs` backend in gloo and enabled gloo CUDA when used with a backend that supports `GPUDirect` (#153015, #153425, #153406)\r\n\r\n#### DeviceMesh\r\n- Improved device selection logic (#150897)\r\n\r\n#### DistributedDataParallel (DDP)\r\n- Added one option to allow skipping all reduce unused parameters (#151503)\r\n- Added check on received data to avoid segfault in the DDP reducer (#152143)\r\n- Propagated `use_python_reducer` to C++ reducer (#152735)\r\n`DistributedStateDict` (DSD)\r\n- Supported non-tensor-data `write_size` in planner write items (#149699)\r\n- Skip popping meta device tensors (#153185)\r\n\r\n#### DTensor\r\n- Made `StridedShard` support uneven sharding (#150490)\r\n- Added op support for `torch.cumsum` (#151071)\r\n- Added `DTensor` `redistribute` fwd/bwd datatype conversion to enable `SimpleFSDP` mixed precision training (#150740)\r\n- Added rich support to `torch.distributed.tensor.debug.visualize_sharding` (#152027)\r\n\r\n#### FullyShardedDataParallel2 (FSDP2)\r\n- Added `PrivateUse1` backend in FSDP collectives and device type to pre forward hook (#147260, #149487)\r\n- Added `set_reshard_after_forward` (#149103)\r\n- Allowed different dtypes for no grad model params (#154103)\r\n- Respected `reshard_after_forward=True` for root model and kept root unsharded when not specifying `reshard_after_forward` (#154704, #155319)\r\n- Allowed forcing FSDP2 to always use SUM reductions (#155915)\r\n- Made assert on `all_reduce_event` only if it's not CPU device (#150316)\r\n- Enabled NCCL zero-copy (user buffer registration) for FSDP2 (#150564)\r\n\r\n#### Pipeline Parallelism\r\n- Added schedule visualizer (#150347)\r\n- Allowed unused kwargs in ZB path (#153498)\r\n- Added `get_pipeline_order()` for Gpipe and 1F1B (#155935)\r\n\r\n#### ShardedTensor\r\n- Added support for 0-size `ShardedTensor` and recalculated metadata from `all_gather` (#152583)\r\n\r\n#### TensorParallel\r\n- Added a `ParallelStyle PrepareModuleInputOutput` (#150372)\r\n\r\n#### torchelastic\r\n- No shutdown of rendezvous on leaving workers (#152525)\r\n\r\n## torch.compile\r\n#### Dynamo\r\n- Improved tracing support for python sets, tensor subclasses with `__torch_function__`, and `namedtuple` subclasses (#153150, #149792, #153982)\r\n- Eliminated all Compiled Autograd dynamic shapes recompiles for compile time reduction (#151962, #152119,\r\n#151962, #149707, #149709,\r\n#148799, #148801)\r\n- Added `reason` field to `torch.compiler.disable` (#150341)\r\n- Removed `lru_cache` warnings for functions in the top-level `torch` namespace (#157718)\r\n\r\n#### Inductor\r\n- Added block sparse support for FlexAttention on CPU (#147196)\r\n- Introduced new config settings:\r\n  - `aot_inductor.custom_ops_to_c_shims` and `aot_inductor.custom_op_libs`: allow for specifying custom op C shim (#153968)\r\n  - `max_fusion_buffer_group_pairwise_attempts`: limits fusions to specified node distance (#154688)\r\n  - `cuda.cutlass_enabled_ops`: controls CUTLASS operation selection (#155770)\r\n  - `triton.cudagraph_capture_sizes`: allows specifying certain shapes for which to capture CUDAGraphs; skips CUDAGraphs for other shapes (#156551)\r\n  - `use_static_cuda_launcher`: enables launching compiled triton statically to improve cold start times (#148890)\r\n  - `assume_unaligned_fallback_output`: allows inductor to track unaligned outputs (#150777)\r\n  - `cuda.cutlass_tma_only`: controls whether or not to only use TMA-compatible kernels in CUTLASS (#152815)\r\n  - `static_launch_user_defined_triton_kernels`: enables statically launching user defined triton kernels (#153725)\r\n  - `precompilation_timeout_seconds`: controls the timeout on precompilation (#153788)\r\n  - `disable_decompose_k`: disables new `DecomposeK` GEMM Kernels (#154421)\r\n  - `min_num_split`: sets the minimum number of splits in a split reduction (#155941)\r\n  - `max_autotune_flex_search_space`: allows specifying the size of the search space for flex attention autotuning (#156307)\r\n- Introduced environment variable `LOG_AUTOTUNE_RESULTS` for autotune log (#156254)\r\n- Improved numerical stability of CPU Welford reduction for normalizations (#145061)\r\n\r\n## torch.export\r\n- Improved handling of builtin ops (`min`, `max`, `math.pow`) (#151348)\r\n- Added min/max ranges for dim hints (#149590)\r\n- Allow registering normal classes to `pytree.register_dataclass` (#147752)\r\n- Allow specifying integer inputs as dynamic (#151842)\r\n- Inline `jit.script`ed functions in export (#155180)\r\n- Pretty printing for graph signature (#149710)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Support for device-side TMA (#157241)\r\n- Added `num_runners` to `AOTIModelPackageLoader` (#149364)\r\n\r\n## FX\r\n- Updated codegen compare op to `==` (#150611)\r\n- Map names to operand indices when const folding submodules (#150692)\r\n- Improved stacktrace when tracing (#151029, #155486)\r\n- Support edge dialect ops in `normalize_function` (#143689)\r\n- Fixed path naming in minifier (#153130)\r\n- Added `graph_code_verbose_log` artifact for FX passes (#153775)\r\n- Improved cache key graph printing performance (#151928)\r\n- Added flag to `fx.passes.split_module` to normalize input names (#157793)\r\n\r\n## Linear Algebra Frontend\r\n- Add tensor overlap check for `cross` (#154999)\r\n\r\n## MPS\r\n- Added support for a number of `torch.special` operations as well as `index_copy`, `hardshrink`, `rsub`, `col2im`, and `isin` (#149174, #149203 #149123, #149368, #149378, #149563, #149687, #149705, #149783, #149407/#149680, #150279, #151754, #153786, #154326, #155304, #156263, #155382, #154010, #149816, #152282, #156090, #150060, #151600, #155002, #154671)\r\n- Extended dtype support for:\r\n  * `index_put` with half precision floats (#151869)\r\n  * `ConvTranspose3D` with FP32 and complex (#154696)\r\n  * `log1p` and `sigmoid` with int64 (#151791)\r\n- Compute activation kernels at float precision (#155735)\r\n\r\n## Nested Tensor (NJT)\r\n- Fixed contiguity in NJT string representation (#153529)\r\n\r\n## torch.nn\r\n- Added warning for module full backward hook when no input requires gradient (#155339)\r\n- Added Half support for `weight_norm` on CPU (#148878)\r\n\r\n## ONNX\r\n- Updated ONNX to 1.18 (#152200)\r\n- Added support for opsets (18-23) when `dynamo=True` (#149901, #154596)\r\n- Added float4 support (#151069, #156353)\r\n- Added support for ONNX operators `Attention-23` and `RotaryEmbedding-23` as native PyTorch ops (#156431, #156367, #154745)\r\n- Added support for `torch.scan` (#154513)\r\n- Added support for 0/1-sized example inputs on dynamic dimensions (#155717)\r\n- Add `group_norm` support from opset 21 (#152138)\r\n- Added `asdict` method to `VerificationInfo` class (#151024)\r\n- Support running bfloat16 models with ONNX Runtime (#149646)\r\n- Updated ONNX program doc formatting and improve robustness (#151623)\r\n- Updated `dynamic_shapes` behavior to use `torch.export.dim.DYNAMIC` (#153065)\r\n- Set the name of the producing node using the value name (#155413)\r\n- Improved support for symbolic operators `sym_float`, `sym_not`, `sym_min`, `sym_max` (#153200, #152111, #152196)\r\n\r\n## Optimizer\r\n- Added `TensorLR` variant for fused Adagrad on CPU (#153078)\r\n- Convert tensor lr to 0-dim as needed for the optimizer to normally work (#145674)\r\n- Added `lr_lambda` type check in `MultiplicativeLR` (#151973)\r\n\r\n## Profiler\r\n- Added support for on-demand memory snapshot (#150559)\r\n- Added PT2 compile context to visualizer (#152862)\r\n- Added PT2 to memory snapshot (#152707)\r\n- Added flag to toggle global and local callbacks for annotations (#154932)\r\n- Pass overload names to Kineto (#149333)\r\n- Set duration to -1 for unfinished CPU events (#150131)\r\n- Start at index with most events (#154571)\r\n\r\n## Python Frontend\r\n- Introduced `torch.AcceleratorError` (#152023)\r\n- Implemented `Size.__radd__()` (#152554)\r\n- Updated `get_default_device()` to also respect `torch.device` context manager (#148621)\r\n\r\n## Quantization\r\n- Improved x86 PT2E quantization support with new uint8 ops (pointwise `mul` / `add` / `add_relu` and `batch_norm2d`), qconv1d-relu fusion, and lowering pass (#151112, #152411, #152811, #150751, #149708)\r\n- Support boolean tensor for `torch.fused_moving_avg_obs_fake_quant` on CUDA (#153699)\r\n\r\n## Release Engineering\r\n- Updated gcc11 to gcc13 in manylinux images (#152825, #152825, #150635, #158445)\r\n- Updated to cmake 3.27.2 (#154783, #150549, #153380)\r\n\r\n## ROCm\r\n- Allow user to override default flags for `cpp_extension` (#152432)\r\n- Enabled support for sparse compressed `mm`/`bmm`/`addmm` (#153262)\r\n\r\n## Sparse Frontend\r\n- Enabled sparse compressed tensor invariant checks for `PrivateUse1` extension (#149374)\r\n\r\n## torch.func\r\n- Add batching rules for ops: `torch.Tensor.scatter_add_` (#150543), `torch.matrix_exp` (#155202)\r\n\r\n## XPU\r\n- Support safe softmax, GQA, fp32 causal mask for SDP and increase maximum head dim from 256 to 576 on Intel GPU (#151999, #150992, #152091)\r\n- Add memory reporting to Memory Profiler for Intel GPU (#152842)\r\n- Support Intel GPU profiler toggle functionality (#155135)\r\n- Support distributed memory tracker integration for Intel GPU (#150703)\r\n- Improved error handling and reporting in Intel GPU CMake files (#149353)\r\n- Support `embed_cubin` and `multi_arch_kernel_binary` options in AOTI for Intel GPU (#154514, #153924)\r\n- Added generic and Intel GPU specific Stream and Event in `UserDefineClass` (#155787)\r\n- Support int4 WOQ GEMM on Intel GPU (#137566)\r\n\r\n# Bug Fixes\r\n## Build Frontend\r\n- Support builds with `CMake-4.x` (#150203)\r\n- Fixed fbgemm build with `gcc-12+` (#150847)\r\n- Force build to conform to C++ standard on Windows by adding `/permissive-` flag (#149035)\r\n\r\n## Composability\r\n- Fixed support for 1-element tuple returns from custom ops (#155447)\r\n- Avoid overflow in `torch.norm` for scalar input (#144073)\r\n\r\n## CPU (x86)\r\n- Fixed apparent copy-paste bug in `log_softmax` reduced-precision fp kernel (#156379)\r\n\r\n## CUDA\r\n- Fixed deterministic indexing with broadcast (#154296)\r\n- Fixed `torch.backends.cuda.matmul.allow_fp16_accumulation` crash when using cuBLASLt (#153083)\r\n- Enable `AsyncMM` on Blackwell (#153519)\r\n- Fixed `torch.cuda.MemPool` for multithreaded use-cases (#153356)\r\n- Fix to avoid calling `sum()` on a default-constructed gamma / beta in `layer_norm` (#156600)\r\n- Avoid hangs by erroring out for negative offsets or K=0 in grouped GEMMs (#153226)\r\n- Don't error out in `empty_cache` under mempool context (#158180)\r\n\r\n## Distributed\r\n#### c10d\r\n- Fixed extra CUDA context created by barrier (#149144)\r\n- Fixed the logic to use group rank instead of global rank when possible (#149488)\r\n- Fixed ET trace collection of `all_to_all` (#149485)\r\n- Disabled start event recording for coalesced col and improved profile title (#150863)\r\n- Fixed connection reset in tcp store (#150987, #151052)\r\n- Fixed unused `group` input argument in `new_subgroups()` (#152765, #153798)\r\n- Fixed tcp init when using port 0 (#154156)\r\n- Adopted a vector to temporarily keep the reference to future object to avoid blocking inside Flight Recorder (#156653)\r\n\r\n#### Distributed Checkpointing (DCP)\r\n- Fixed to use global coordinator rank in `broadcast_object` util function (#155912)\r\n\r\n#### DistributedDataParallel (DDP)\r\n- Fixed `DDPOptimizer` issue on static tensor index (#155746)\r\n\r\n#### DTensor\r\n- Fixed `local_map` with multi-threading (#149070)\r\n- Fixed `new_local_tensor` in `redistribute` be None case (#152303)\r\n- Fixed bug visualizing 1D Tensor using rich (#152871)\r\n\r\n#### Pipeline Parallelism\r\n- Optimized memory usage by releasing output memory earlier (#153383)\r\n\r\n#### RPC\r\n- Made torch importable if compiled without `TensorPipe` (#154382)\r\n\r\n#### ShardedTensor\r\n- Fixed sharded tensor `gather` when a local tensor on certain ranks has zero elements (#150914)\r\n\r\n#### TensorParallel\r\n- Turn async-TP applicability asserts back into silent skips (#158736)\r\n\r\n## torch.compile\r\n#### Dynamo\r\n- Eliminated silent incorrectness issues in the Compiled Autograd initial trace (#149014, #155521, #155289, #149336)\r\n- Fixed various tracing errors involving einops, `dict(mapping_proxy)`, and the FlexAttention HOP (#157754, #157515, #157519)\r\n- Fixed unpack hook semantics for memory savings in checkpointing and offloading for Compiled Autograd (#147242, #153300)\r\n- Fixed sources for dataclass defaults and the `lru_cache` method (#158689, #157308)\r\n- Fixed spammy errors when an invalid `TORCH_LOGS` argument is passed (#151678)\r\n\r\n#### Inductor\r\n- Support special kwargs in AMD triton configs (#154605)\r\n- Fixed minifier when one has multiple Python runtimes (#155918)\r\n- Bug fix for int8 GEMM compensation epilogue (#152408)\r\n\r\n## torch.export\r\n- Fixed tracing of the following: `aten.is_nonzero` (#149637), `torch.bincount()` (#152497), `aten.div` (#150874) slicing (#150104), and `attn_mask` (#158618), `aten.to` (#153972), scalar tensor construction (#154661)\r\n- Fixed `dynamic_shapes` spec for kwargs (#148772, #149528, #150103)\r\n- Fixed input bugs in unflattener (#149206, #153474, #153000)\r\n- Fix nonstrict tracing of `functools.partial` (#153408), and higher order ops (#149295)\r\n- Fixed serialization/deserialization of `None` inputs (#150515), `math` module (#154643), `call_torchbind` (#155647), and enums (#154821)\r\n- Fixed state dict modification in run_decompositions (#151436)\r\n- Fixed subclass access custom op bug (#149698)\r\n\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Fixed AOTI `update_constant_buffer` issue (#149243)\r\n- Fixed a memory leak in `model_package_loader` (#152334)\r\n- Don't alloc weights in `AOTIModel` if they don't exist (#152692)\r\n- Fixed state of `ConstantFolding` (#153152)\r\n- Fixed index offset for optional tensor return (#155073)\r\n- Fixed float8 type printing for min/max value printing (#154466)\r\n\r\n## Linear Algebra Frontend\r\n- Fix to workaround LAPACK workspace size being returned as a floating point value (#149682)\r\n- Fixed the accumulation type for `dot` and `gemv` (#152676)\r\n- Fixed `torch.lobpcg` to compute same largest eigenvalue as scipy and `np.linalg.eig` (#152789)\r\n- Fixed 32-bit indexing overflows in `ReducedPrecisionGemV` (#150949)\r\n\r\n## MPS\r\n- Fixed various op support issues: unary/binary ops with `2**32`+ element inputs, binary ops with inputs with different dtypes, ops with complex scalar inputs, `cholesky` decomp, `floor_divide` type promotion, `index_kernel` with large inputs, `lerp` with complex inputs, `logit` with half/bfloat16 inputs, SDPA memory leak, `torch.special.entr`, `tri[ul]`, matrix inversion with `N>1024`, and `where` with non-contiguous `cond` (#152479, #155183, #149233, #151176, #151282, #158239, #152371, #149974, #158237, #146754, #158867, #155184, #152204)\r\n\r\n## torch.nn\r\n- Fixed `load_state_dict` behavior for `nn.LazyLinear` (#147599)\r\n\r\n## ONNX\r\n- Fixed bfloat16 support in `onnx_program` callable (#151121)\r\n- Produce correct dtypes for bf16/f8 in IR TorchTensor (#151259)\r\n- Preserve all legacy exporter params in fallback (#156659)\r\n- Fixed 4D tensor conversion for SDPA (#157509)\r\n\r\n## Optimizer\r\n- Fixed bug where `lr_scheduler` unexpectedly calls `step()` when init argument `last_epoch > -1` (#149312)\r\n- Fixed `CosineAnnealingWarmRestarts` resetting `T_cur` (#151289)\r\n\r\n## Profiler\r\n- Fixed empty C call queue in python tracer (#150370)\r\n- Removed decref from python context in python tracer (#151625)\r\n- Enable all configured activities in CUPTI Range Profiler mode (#154749)\r\n\r\n## Python Frontend\r\n- Fixed segfault during numpy string tensor conversion (#155364)\r\n- Added checks for empty tensor list (#155383)\r\n- Fixed sample validation for `MixtureSameFamily` distribution (#151317)\r\n- Fixed bug where creating a second `Wishart` or `Uniform` distribution modifies constraints on the first (#154361)\r\n- Fix to properly export `torch::utils::tensor_to_numpy` symbol (#154178)\r\n- Fixed `torch.[con]cat[enate]` to avoid crashing on empty inputs (#155460)\r\n- Unify `torch.tensor` and `torch.ops.aten.scalar_tensor` behavior (#158655)\r\n\r\n## Release Engineering\r\n- Checkout optional submodules when publishing a release tarball (#156615)\r\n- Fixed MacOS MP hang in Python-3.12+ (#155698)\r\n- Fixed static functions when using module in MSVC (#148675)\r\n- Fixed VS2022-caused AVX512 illegal instruction issue (#153480)\r\n\r\n## ROCm\r\n- Fixed build error for opportunistic fastatomics with newer compilers (#152841)\r\n\r\n#### TunableOp\r\n- More TF32 support (#149088)\r\n- Fixed offline tuning for `ScaledGEMM` (#149677)\r\n- Fixed row-wise `ScaledGEMM` (#152403)\r\n- Support submatrices in offline tuning for ROCm (#151138)\r\n\r\n## Vulkan\r\n- Fixed `torch.is_vulkan_available()` on Mac (#155595)\r\n\r\n## XPU\r\n- Fixed matmul accuracy when `offset > 0` (#154495)\r\n- Fixed `torch.xpu.is_bf16_supported` to correctly report presence of Intel GPU (#152317)\r\n- Fixed AOT compilation in SYCL C++ extension (#156364)\r\n\r\n# Performance\r\n## Autograd\r\n- Improved autograd streams synchronization (#151079, #157914)\r\n\r\n## CPU (AArch64)\r\n- Compute `ELU(0)` with the cheaper definition (#155765)\r\n\r\n## CUDA\r\n- Improved performance of `cat` and `index_select` (#150233, #152380, #151715)\r\n\r\n## Dataloader Frontend\r\n- Reduced memory usage of `SubsetRandomSampler` by iterating over list instead of tensor (#149126)\r\n\r\n## torch.compile\r\n#### Inductor\r\n- Improved performance of GEMMs (#147315, #151530, #149373, #156174, #155444)\r\n- Added a config option `cpp.use_small_dequant_buffer` to use a small dequant buffer for WOQ int4 GEMM (#156395)\r\n- Support graph partitioning on custom ops (#149782)\r\n- Optimized the heuristics of parallel reduction on CPU (#149614)\r\n\r\n## torch.export\r\n- Cache unflattened graph module (#150030)\r\n\r\n## JIT\r\n- Improved Dead Code Elimination (DCE) compile times for large graphs (#153645)\r\n\r\n## Linear Algebra Frontend\r\n- Introduced fast path for `torch.dot` with float16/bfloat16 (#152799)\r\n\r\n## MPS\r\n- Improved performance of `LayerNorm`, `mm` / `bmm`, `sum` / `prod` reductions, arithmetic ops,\r\nbinary kernels, SDPA, `linear`, and `cumsum` / `cumprod` (#152010, #150541, #150566, #147644, #149730, #152781, #152210, #157494)\r\n\r\n## Python Frontend\r\n- Optimized SVE embedding performance (#150176)\r\n- Improved performance for `torch.tensordot` when contracting to a scalar (#145936)\r\n\r\n## ROCm\r\n- Improved performance of `softmax`, `NLLLoss`, in-place sum, max pooling backward / reductions on NHWC\r\ninputs, max pooling, multi-dimensional reductions, and non-vectorized elementwise kernels (#149076, #149779, #149548, #151230, #152267, #154522, #154619, #155806, #153184)\r\n- Improved scatter add performance on MI250X (#151724)\r\n- Extended vectorized elementwise kernel to more heterogenous tensor types (#149738)\r\n- Use `HipSparseLT` to further accelerate semi-structured (e.g. 2:4) sparsity (#150578)\r\n\r\n## Sparse Frontend\r\n- Skip sparse tensor invariant validation when loading sparse Tensors from external storage (#154610, #154759, #154638)\r\n\r\n## XPU\r\n- Enabled post-op fusion for oneDNN convolution on Intel GPU (#150287)\r\n- Reduced host overhead for Intel GPU by eliminating meaningless API calls (#151111)\r\n- Improved INT4 WOQ GEMM for Intel GPU by introducing a cache mechanism to reduce the oneDNN integration overhead further (#147693)\r\n- Improved scalar tensor case handling in `addmm`, `baddmm` to reduce oneDNN integration overhead on Intel GPU (#153051)\r\n\r\n# Documentation\r\n## Autograd\r\n- Added more details on why `ctx.save_for_backward` is important in note about extending autograd (#153005)\r\n- Updated docs of `torch.autograd.graph.saved_tensors_hooks` to avoid refcycle (#153049)\r\n- Updated gradient behavior note in `torch.amin` and `torch.amax` (#155071)\r\n\r\n## CUDA\r\n- Fixed deprecated amp APIs in docs (#154553)\r\n- Documented device memory apis in correct module (#155126)\r\n- Documented non-pytorch CUDA memory allocation and how to query it (#150880)\r\n\r\n## Distributed\r\n#### c10d\r\n- Documented object collectives limitations (#150815)\r\n- Updated `NCCLConfig` with QOS variable (#151821)\r\n- Documented `get_default_backend_for_device` (#158236)\r\n\r\n#### FullyShardedDataParallel2 (FSDP2)\r\n- Updated `ignored_params` docstring and added unit tests (#149074)\r\n- Added pointer to torchtitan (#153079)\r\n- Added warning for incorrected grad results at world size 1 (#154928)\r\n\r\n## torch.export\r\n- Added mini tutorial for provenance tracking (#152211)\r\n- Updated docs for `Dims` and `ExportGraphSignature` (#156262, #156244)\r\n\r\n## Linear Algebra Frontend\r\n- Addressed ambiguity in docs for `torch.linalg.norm()`'s ord argument of +2 & -2 (#155148)\r\n\r\n## torch.nn\r\n- Improved documentation for transformer-related layers, `nn.RNN`, `nn.functional` loss functions, `interpolate` saturate cast behavior, `ConvTranspose2d` `stride` / `output_size` arguments, and `register_full_backward_hook` (#155123, #153620, #148436, #151304, #150819, #150609, #151785)\r\n- Fixed examples for `nn.Sequential` and `nn.LazyModuleMixin` (#147304, #150596)\r\n- Documented padding size limitations in `nn.modules.padding` and `AvgPoolND` (#155618, #152680)\r\n\r\n## ONNX\r\n- Convert .rst doc files to markdown (#155228, #155556)\r\n- Improved docstring of ONNX symbolic ops (#149668)\r\n- Added note for attention op symbolic function (#156441)\r\n- Added ONNX Dynamo metadata documentation (#155816)\r\n\r\n## Optimizer\r\n- Added scripts to generate plots of `LRScheduler`s (#149189)\r\n- Included other accelerators in capturable docstr for optimizers (#149770)\r\n- Updated SGD documentation to match implementation and document that dampening is skipped in SGD first step (#149884, #152833)\r\n- Fixed doc for `CosineAnnealingLR` to accurately reflect its recursive learning rate schedule (#152936)\r\n- Fixed incorrect citation of authors in `Adafactor` documentation (#145209)\r\n- Added `load_state_dict` hint doc about invoke order work with `lr_scheduler` (#149942)\r\n\r\n## Python Frontend\r\n- Make `torch.Library`'s `kind` have no default value to be consistent with the code (#149390)\r\n- Added 32-bit complex to the list of dtypes (#144590)\r\n- Clarified behavior when integer dtype is used with `requires_grad=True` in `tensor.to()` (#150913)\r\n- Optimized `cdist` param description (#151178)\r\n- Updated serialization docs (#153631)\r\n- Render `Example:` and not `Example::` in docs (#153978)\r\n- Added docstring indicating undefined behavior for converting inf to int (#154781)\r\n- Updated `as_strided()` docs (#149146)\r\n- Fixed `keepdim` param optional description (#151197)\r\n- Clarify that x and dx are mutually exclusive in `torch.trapezoid` docs (#151190)\r\n- Documented `out_dtype` arg for torch GEMM operations (#151704)\r\n- Fixed the basic description of `torch.min()`, `torch.max()`, `torch.all()`, and `torch.any()` (#152658)\r\n- Added `torch.triu_indices`, `torch.tril_indices` dtype description (#150749)\r\n- Optimized `torch.equal` description (#149618)\r\n\r\n## Quantization\r\n- Fixed incorrect `get_default_qat_qconfig` in `prepare_qat_fx` docs (#155100)\r\n\r\n## Release Engineering\r\n- Migrated to new theme (#149331)\r\n\r\n## XPU\r\n- Improved \"Getting Started on Intel GPU\" hardware requirements and notes (#151886)\r\n\r\n# Developers\r\n## Distributed\r\n#### c10d\r\n- Added param recording for uniqueID broadcasting and allgather (#149166)\r\n- Added logger config and more loggings, e.g. `nccl_version` and thread name/id, for flight record in PGNCCL (#150356, #150513, #151048, #152648, #155142, #155754)\r\n- Surfaced error type when we unlink and create named pipe for DumpPipe (#150648)\r\n- Improved the logs on remote shutdown of tcpstore (#153586)\r\n- Enhanced Error Logging in `new_subgroups()` for Non-Divisible World Sizes (#154124)\r\n- Added a logger for all nccl collectives with its time duration when completed (#156008)\r\n- Updated error message in `get_backend()` with more details (#141796)\r\n\r\n#### FullyShardedDataParallel (FSDP1)\r\n- Print FQNs when debugging `FlatParamHandle` (#151336)\r\n\r\n#### FullyShardedDataParallel2 (FSDP2)\r\n- Added FSDP2 logging (#155826)\r\n\r\n#### RPC\r\n- Correctly pass exceptions raised from `rpc_init` to CPython (#154325)\r\n\r\n#### torchelastic\r\n- Added the logging of start of torch elastic workers (#150849)\r\n- Passed event log handler to record function calls (#155457)\r\n- Added `torch.distributed.run` option to provide destination for event logging (#155268)\r\n\r\n## torch.export\r\n- Add `TracingContext` (#149294)\r\n- Monkeypatch fake mode so it errors on invalid custom ops (#149410)\r\n- Fixed torch export docs for preserve_module_call_signature (#151140)\r\n- Improved error message for deserializing custom triton op (#152029)\r\n- Better type annotation for lift_constants_pass (#152072)\r\n- Fixed bug in `detect_attr_assignment` (#151824)\r\n\r\n## Ahead-Of-Time Inductor (AOTI)\r\n- Refactor `AOTInductor` runtime API for Intel GPU (#153929)\r\n- Improve stable library APIs (#152040)\r\n- Add a basic shim and `stable::Tensor is_contiguous` API (#156228)\r\n\r\n## FX\r\n- Gracefully exit minimizer when there is no discrepancy in block mode (#154076)\r\n\r\n## Optimizer\r\n- Improve decorator typing for Optimizer subclasses (#153374)\r\n- Optimize typing in `lr_scheduler.py` (#151219)\r\n- Fixed the type hint of `step()` with default value (#153367)\r\n\r\n## Release Engineering\r\n- Added support for CUDA 12.9 in CI/CD (#154980, #156630, #155895, #155799, #155496, #155340, #155819, #156108)\r\n- Added support for ROCm 6.4 in CI/CD (#151236, #151345, #151355, #153253, #156112)\r\n- Moved CI from ubuntu 20.04 images to ubuntu 22.04 and 24.04 (#154437, #154153, #149142)\r\n- Moved CI to CUDA 12.8 (#154004, #152810, #155087, #148963)\r\n- Enabled CI on MI300 (#150667, #152133, #148394, #153134)\r\n- Enabled CI on H100 (#153900, #154562, #153170, #155861, #155719, #156429)\r\n- Enabled CD for Windows Arm64 (#150310, #152109, #149850, #152099)\r\n- Enabled testing of binary Docker builds in CI/CD (#151483, #151488, #151489, #151706)\r\n- Added smoke test to validate NCCL and cuDNN versions in PyPI packages (#149885, #150194)\r\n- Enabled monitoring for performance tests (#153452, #153453, #153454, #153456)\r\n- Improved benchmarking and performance testing on MacOS (#151721, #151747, #151748, #153897, #155493, #153897, #155493)\r\n- Use `setup-python` from for Mac tests (#155698)\r\n- Removed CUDA 11.8 and 12.4 support in CI/CD (#155509, #154169, #152362, #155555, #154893)\r\n- Removed Anaconda support in CI/CD (#147789, #152338, #152431, #152377, #152433, #147476, #151035, #152860, #152702, #154303, #154309)",
        "published_at": "2025-08-06T17:06:10Z",
        "target_commitish": "main",
        "version_tuple": [
          2,
          8,
          0
        ],
        "version_key": "2.8.0"
      },
      {
        "tag_name": "v2.7.1",
        "name": "PyTorch 2.7.1 Release, bug fix release",
        "body": "This release is meant to fix the following issues (regressions / silent correctness):\r\n\r\n### Torch.compile\r\nFix Excessive cudagraph re-recording for HF LLM models ([#152287](https://github.com/pytorch/pytorch/pull/152287)) \r\nFix torch.compile on some HuggingFace models  ([#151154](https://github.com/pytorch/pytorch/pull/151154))\r\nFix crash due to Exception raised inside torch.autocast ([#152503](https://github.com/pytorch/pytorch/pull/152503))\r\nImprove Error logging in torch.compile ([#149831](https://github.com/pytorch/pytorch/pull/149831))\r\nMark mutable custom operators as cacheable in torch.compile ([#151194](https://github.com/pytorch/pytorch/pull/151194))\r\nImplement workaround for a graph break with older version einops ([#153925](https://github.com/pytorch/pytorch/pull/153925))\r\nFix an issue with tensor.view(dtype).copy_(...) ([#151598](https://github.com/pytorch/pytorch/pull/151598))\r\n\r\n### Flex Attention\r\nFix assertion error due to inductor permuting inputs to flex attention ([#151959](https://github.com/pytorch/pytorch/pull/151959))\r\nFix performance regression on nanogpt speedrun ([#152641](https://github.com/pytorch/pytorch/pull/152641))\r\n\r\n### Distributed\r\nFix extra CUDA context created by barrier ([#149144](https://github.com/pytorch/pytorch/pull/149144))\r\nFix an issue related to Distributed Fused Adam in Rocm/APEX when using nccl_ub feature ([#150010](https://github.com/pytorch/pytorch/pull/150010))\r\nAdd a workaround random hang in non-blocking API mode in NCCL 2.26 ([#154055](https://github.com/pytorch/pytorch/pull/154055))\r\n\r\n### MacOS\r\nFix MacOS compilation error with Clang 17 ([#151316](https://github.com/pytorch/pytorch/pull/151344))\r\nFix binary kernels produce incorrect results when one of the tensor arguments is from a wrapped scalar on MPS devices ([#152997](https://github.com/pytorch/pytorch/pull/152997))\r\n\r\n### Other\r\nImprove PyTorch Wheel size due to introduction of addition of 128 bit vectorization ([#148320](https://github.com/pytorch/pytorch/pull/148320)) ([#152396](https://github.com/pytorch/pytorch/pull/152396))\r\nFix fmsub function definition ([#152075](https://github.com/pytorch/pytorch/pull/152075))\r\nFix Floating point exception in torch.mkldnn_max_pool2d ([#151848](https://github.com/pytorch/pytorch/pull/151848))\r\nFix abnormal inference output with XPU:1 device ([#153067](https://github.com/pytorch/pytorch/pull/153067))\r\nFix Illegal Instruction Caused by grid_sample on Windows ([#152613](https://github.com/pytorch/pytorch/pull/152613))\r\nFix ONNX decomposition does not preserve custom CompositeImplicitAutograd ops ([#151826](https://github.com/pytorch/pytorch/pull/151826))\r\nFix error with dynamic linking of libgomp library ([#150084](https://github.com/pytorch/pytorch/pull/150084))\r\nFix segfault in profiler with Python 3.13 ([#153848](https://github.com/pytorch/pytorch/pull/153848))",
        "published_at": "2025-06-04T18:13:15Z",
        "target_commitish": "main",
        "version_tuple": [
          2,
          7,
          1
        ],
        "version_key": "2.7.1"
      }
    ],
    "readme_content": "![PyTorch Logo](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/pytorch-logo-dark.png)\n\n--------------------------------------------------------------------------------\n\nPyTorch is a Python package that provides two high-level features:\n- Tensor computation (like NumPy) with strong GPU acceleration\n- Deep neural networks built on a tape-based autograd system\n\nYou can reuse your favorite Python packages such as NumPy, SciPy, and Cython to extend PyTorch when needed.\n\nOur trunk health (Continuous Integration signals) can be found at [hud.pytorch.org](https://hud.pytorch.org/ci/pytorch/pytorch/main).\n\n<!-- toc -->\n\n- [More About PyTorch](#more-about-pytorch)\n  - [A GPU-Ready Tensor Library](#a-gpu-ready-tensor-library)\n  - [Dynamic Neural Networks: Tape-Based Autograd](#dynamic-neural-networks-tape-based-autograd)\n  - [Python First](#python-first)\n  - [Imperative Experiences](#imperative-experiences)\n  - [Fast and Lean](#fast-and-lean)\n  - [Extensions Without Pain](#extensions-without-pain)\n- [Installation](#installation)\n  - [Binaries](#binaries)\n    - [NVIDIA Jetson Platforms](#nvidia-jetson-platforms)\n  - [From Source](#from-source)\n    - [Prerequisites](#prerequisites)\n      - [NVIDIA CUDA Support](#nvidia-cuda-support)\n      - [AMD ROCm Support](#amd-rocm-support)\n      - [Intel GPU Support](#intel-gpu-support)\n    - [Get the PyTorch Source](#get-the-pytorch-source)\n    - [Install Dependencies](#install-dependencies)\n    - [Install PyTorch](#install-pytorch)\n      - [Adjust Build Options (Optional)](#adjust-build-options-optional)\n  - [Docker Image](#docker-image)\n    - [Using pre-built images](#using-pre-built-images)\n    - [Building the image yourself](#building-the-image-yourself)\n  - [Building the Documentation](#building-the-documentation)\n    - [Building a PDF](#building-a-pdf)\n  - [Previous Versions](#previous-versions)\n- [Getting Started](#getting-started)\n- [Resources](#resources)\n- [Communication](#communication)\n- [Releases and Contributing](#releases-and-contributing)\n- [The Team](#the-team)\n- [License](#license)\n\n<!-- tocstop -->\n\n## More About PyTorch\n\n[Learn the basics of PyTorch](https://pytorch.org/tutorials/beginner/basics/intro.html)\n\nAt a granular level, PyTorch is a library that consists of the following components:\n\n| Component | Description |\n| ---- | --- |\n| [**torch**](https://pytorch.org/docs/stable/torch.html) | A Tensor library like NumPy, with strong GPU support |\n| [**torch.autograd**](https://pytorch.org/docs/stable/autograd.html) | A tape-based automatic differentiation library that supports all differentiable Tensor operations in torch |\n| [**torch.jit**](https://pytorch.org/docs/stable/jit.html) | A compilation stack (TorchScript) to create serializable and optimizable models from PyTorch code  |\n| [**torch.nn**](https://pytorch.org/docs/stable/nn.html) | A neural networks library deeply integrated with autograd designed for maximum flexibility |\n| [**torch.multiprocessing**](https://pytorch.org/docs/stable/multiprocessing.html) | Python multiprocessing, but with magical memory sharing of torch Tensors across processes. Useful for data loading and Hogwild training |\n| [**torch.utils**](https://pytorch.org/docs/stable/data.html) | DataLoader and other utility functions for convenience |\n\nUsually, PyTorch is used either as:\n\n- A replacement for NumPy to use the power of GPUs.\n- A deep learning research platform that provides maximum flexibility and speed.\n\nElaborating Further:\n\n### A GPU-Ready Tensor Library\n\nIf you use NumPy, then you have used Tensors (a.k.a. ndarray).\n\n![Tensor illustration](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/tensor_illustration.png)\n\nPyTorch provides Tensors that can live either on the CPU or the GPU and accelerates the\ncomputation by a huge amount.\n\nWe provide a wide variety of tensor routines to accelerate and fit your scientific computation needs\nsuch as slicing, indexing, mathematical operations, linear algebra, reductions.\nAnd they are fast!\n\n### Dynamic Neural Networks: Tape-Based Autograd\n\nPyTorch has a unique way of building neural networks: using and replaying a tape recorder.\n\nMost frameworks such as TensorFlow, Theano, Caffe, and CNTK have a static view of the world.\nOne has to build a neural network and reuse the same structure again and again.\nChanging the way the network behaves means that one has to start from scratch.\n\nWith PyTorch, we use a technique called reverse-mode auto-differentiation, which allows you to\nchange the way your network behaves arbitrarily with zero lag or overhead. Our inspiration comes\nfrom several research papers on this topic, as well as current and past work such as\n[torch-autograd](https://github.com/twitter/torch-autograd),\n[autograd](https://github.com/HIPS/autograd),\n[Chainer](https://chainer.org), etc.\n\nWhile this technique is not unique to PyTorch, it's one of the fastest implementations of it to date.\nYou get the best of speed and flexibility for your crazy research.\n\n![Dynamic graph](https://github.com/pytorch/pytorch/raw/main/docs/source/_static/img/dynamic_graph.gif)\n\n### Python First\n\nPyTorch is not a Python binding into a monolithic C++ framework.\nIt is built to be deeply integrated into Python.\nYou can use it naturally like you would use [NumPy](https://www.numpy.org/) / [SciPy](https://www.scipy.org/) / [scikit-learn](https://scikit-learn.org) etc.\nYou can write your new neural network layers in Python itself, using your favorite libraries\nand use packages such as [Cython](https://cython.org/) and [Numba](http://numba.pydata.org/).\nOur goal is to not reinvent the wheel where appropriate.\n\n### Imperative Experiences\n\nPyTorch is designed to be intuitive, linear in thought, and easy to use.\nWhen you execute a line of code, it gets executed. There isn't an asynchronous view of the world.\nWhen you drop into a debugger or receive error messages and stack traces, understanding them is straightforward.\nThe stack trace points to exactly where your code was defined.\nWe hope you never spend hours debugging your code because of bad stack traces or asynchronous and opaque execution engines.\n\n### Fast and Lean\n\nPyTorch has minimal framework overhead. We integrate acceleration libraries\nsuch as [Intel MKL](https://software.intel.com/mkl) and NVIDIA ([cuDNN](https://developer.nvidia.com/cudnn), [NCCL](https://developer.nvidia.com/nccl)) to maximize speed.\nAt the core, its CPU and GPU Tensor and neural network backends\nare mature and have been tested for years.\n\nHence, PyTorch is quite fast ‚Äî whether you run small or large neural networks.\n\nThe memory usage in PyTorch is extremely efficient compared to Torch or some of the alternatives.\nWe've written custom memory allocators for the GPU to make sure that\nyour deep learning models are maximally memory efficient.\nThis enables you to train bigger deep learning models than before.\n\n### Extensions Without Pain\n\nWriting new neural network modules, or interfacing with PyTorch's Tensor API was designed to be straightforward\nand with minimal abstractions.\n\nYou can write new neural network layers in Python using the torch API\n[or your favorite NumPy-based libraries such as SciPy](https://pytorch.org/tutorials/advanced/numpy_extensions_tutorial.html).\n\nIf you want to write your layers in C/C++, we provide a convenient extension API that is efficient and with minimal boilerplate.\nNo wrapper code needs to be written. You can see [a tutorial here](https://pytorch.org/tutorials/advanced/cpp_extension.html) and [an example here](https://github.com/pytorch/extension-cpp).\n\n\n## Installation\n\n### Binaries\nCommands to install binaries via Conda or pip wheels are on our website: [https://pytorch.org/get-started/locally/](https://pytorch.org/get-started/locally/)\n\n\n#### NVIDIA Jetson Platforms\n\nPython wheels for NVIDIA's Jetson Nano, Jetson TX1/TX2, Jetson Xavier NX/AGX, and Jetson AGX Orin are provided [here](https://forums.developer.nvidia.com/t/pytorch-for-jetson-version-1-10-now-available/72048) and the L4T container is published [here](https://catalog.ngc.nvidia.com/orgs/nvidia/containers/l4t-pytorch)\n\nThey require JetPack 4.2 and above, and [@dusty-nv](https://github.com/dusty-nv) and [@ptrblck](https://github.com/ptrblck) are maintaining them.\n\n\n### From Source\n\n#### Prerequisites\nIf you are installing from source, you will need:\n- Python 3.10 or later\n- A compiler that fully supports C++17, such as clang or gcc (gcc 9.4.0 or newer is required, on Linux)\n- Visual Studio or Visual Studio Build Tool (Windows only)\n\n\\* PyTorch CI uses Visual C++ BuildTools, which come with Visual Studio Enterprise,\nProfessional, or Community Editions. You can also install the build tools from\nhttps://visualstudio.microsoft.com/visual-cpp-build-tools/. The build tools *do not*\ncome with Visual Studio Code by default.\n\nAn example of environment setup is shown below:\n\n* Linux:\n\n```bash\n$ source <CONDA_INSTALL_DIR>/bin/activate\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n```\n\n* Windows:\n\n```bash\n$ source <CONDA_INSTALL_DIR>\\Scripts\\activate.bat\n$ conda create -y -n <CONDA_NAME>\n$ conda activate <CONDA_NAME>\n$ call \"C:\\Program Files\\Microsoft Visual Studio\\<VERSION>\\Community\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64\n```\n\nA conda environment is not required.  You can also do a PyTorch build in a\nstandard virtual environment, e.g., created with tools like `uv`, provided\nyour system has installed all the necessary dependencies unavailable as pip\npackages (e.g., CUDA, MKL.)\n\n##### NVIDIA CUDA Support\nIf you want to compile with CUDA support, [select a supported version of CUDA from our support matrix](https://pytorch.org/get-started/locally/), then install the following:\n- [NVIDIA CUDA](https://developer.nvidia.com/cuda-downloads)\n- [NVIDIA cuDNN](https://developer.nvidia.com/cudnn) v8.5 or above\n- [Compiler](https://gist.github.com/ax3l/9489132) compatible with CUDA\n\nNote: You could refer to the [cuDNN Support Matrix](https://docs.nvidia.com/deeplearning/cudnn/backend/latest/reference/support-matrix.html) for cuDNN versions with the various supported CUDA, CUDA driver, and NVIDIA hardware.\n\nIf you want to disable CUDA support, export the environment variable `USE_CUDA=0`.\nOther potentially useful environment variables may be found in `setup.py`.  If\nCUDA is installed in a non-standard location, set PATH so that the nvcc you\nwant to use can be found (e.g., `export PATH=/usr/local/cuda-12.8/bin:$PATH`).\n\nIf you are building for NVIDIA's Jetson platforms (Jetson Nano, TX1, TX2, AGX Xavier), Instructions to install PyTorch for Jetson Nano are [available here](https://devtalk.nvidia.com/default/topic/1049071/jetson-nano/pytorch-for-jetson-nano/)\n\n##### AMD ROCm Support\nIf you want to compile with ROCm support, install\n- [AMD ROCm](https://rocm.docs.amd.com/en/latest/deploy/linux/quick_start.html) 4.0 and above installation\n- ROCm is currently supported only for Linux systems.\n\nBy default the build system expects ROCm to be installed in `/opt/rocm`. If ROCm is installed in a different directory, the `ROCM_PATH` environment variable must be set to the ROCm installation directory. The build system automatically detects the AMD GPU architecture. Optionally, the AMD GPU architecture can be explicitly set with the `PYTORCH_ROCM_ARCH` environment variable [AMD GPU architecture](https://rocm.docs.amd.com/projects/install-on-linux/en/latest/reference/system-requirements.html#supported-gpus)\n\nIf you want to disable ROCm support, export the environment variable `USE_ROCM=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n##### Intel GPU Support\nIf you want to compile with Intel GPU support, follow these\n- [PyTorch Prerequisites for Intel GPUs](https://www.intel.com/content/www/us/en/developer/articles/tool/pytorch-prerequisites-for-intel-gpus.html) instructions.\n- Intel GPU is supported for Linux and Windows.\n\nIf you want to disable Intel GPU support, export the environment variable `USE_XPU=0`.\nOther potentially useful environment variables may be found in `setup.py`.\n\n#### Get the PyTorch Source\n\n```bash\ngit clone https://github.com/pytorch/pytorch\ncd pytorch\n# if you are updating an existing checkout\ngit submodule sync\ngit submodule update --init --recursive\n```\n\n#### Install Dependencies\n\n**Common**\n\n```bash\n# Run this command from the PyTorch directory after cloning the source code using the ‚ÄúGet the PyTorch Source‚Äú section above\npip install --group dev\n```\n\n**On Linux**\n\n```bash\npip install mkl-static mkl-include\n# CUDA only: Add LAPACK support for the GPU if needed\n# magma installation: run with active conda environment. specify CUDA version to install\n.ci/docker/common/install_magma_conda.sh 12.4\n\n# (optional) If using torch.compile with inductor/triton, install the matching version of triton\n# Run from the pytorch directory after cloning\n# For Intel GPU support, please explicitly `export USE_XPU=1` before running command.\nmake triton\n```\n\n**On MacOS**\n\n```bash\n# Add this package on intel x86 processor machines only\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed\nconda install pkg-config libuv\n```\n\n**On Windows**\n\n```bash\npip install mkl-static mkl-include\n# Add these packages if torch.distributed is needed.\n# Distributed package support on Windows is a prototype feature and is subject to changes.\nconda install -c conda-forge libuv=1.51\n```\n\n#### Install PyTorch\n\n**On Linux**\n\nIf you're compiling for AMD ROCm then first run this command:\n\n```bash\n# Only run this if you're compiling for ROCm\npython tools/amd_build/build_amd.py\n```\n\nInstall PyTorch\n\n```bash\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\npython -m pip install --no-build-isolation -v -e .\n```\n\n**On macOS**\n\n```bash\npython -m pip install --no-build-isolation -v -e .\n```\n\n**On Windows**\n\nIf you want to build legacy python code, please refer to [Building on legacy code and CUDA](https://github.com/pytorch/pytorch/blob/main/CONTRIBUTING.md#building-on-legacy-code-and-cuda)\n\n**CPU-only builds**\n\nIn this mode PyTorch computations will run on your CPU, not your GPU.\n\n```cmd\npython -m pip install --no-build-isolation -v -e .\n```\n\nNote on OpenMP: The desired OpenMP implementation is Intel OpenMP (iomp). In order to link against iomp, you'll need to manually download the library and set up the building environment by tweaking `CMAKE_INCLUDE_PATH` and `LIB`. The instruction [here](https://github.com/pytorch/pytorch/blob/main/docs/source/notes/windows.rst#building-from-source) is an example for setting up both MKL and Intel OpenMP. Without these configurations for CMake, Microsoft Visual C OpenMP runtime (vcomp) will be used.\n\n**CUDA based build**\n\nIn this mode PyTorch computations will leverage your GPU via CUDA for faster number crunching\n\n[NVTX](https://docs.nvidia.com/gameworks/content/gameworkslibrary/nvtx/nvidia_tools_extension_library_nvtx.htm) is needed to build Pytorch with CUDA.\nNVTX is a part of CUDA distributive, where it is called \"Nsight Compute\". To install it onto an already installed CUDA run CUDA installation once again and check the corresponding checkbox.\nMake sure that CUDA with Nsight Compute is installed after Visual Studio.\n\nCurrently, VS 2017 / 2019, and Ninja are supported as the generator of CMake. If `ninja.exe` is detected in `PATH`, then Ninja will be used as the default generator, otherwise, it will use VS 2017 / 2019.\n<br/> If Ninja is selected as the generator, the latest MSVC will get selected as the underlying toolchain.\n\nAdditional libraries such as\n[Magma](https://developer.nvidia.com/magma), [oneDNN, a.k.a. MKLDNN or DNNL](https://github.com/oneapi-src/oneDNN), and [Sccache](https://github.com/mozilla/sccache) are often needed. Please refer to the [installation-helper](https://github.com/pytorch/pytorch/tree/main/.ci/pytorch/win-test-helpers/installation-helpers) to install them.\n\nYou can refer to the [build_pytorch.bat](https://github.com/pytorch/pytorch/blob/main/.ci/pytorch/win-test-helpers/build_pytorch.bat) script for some other environment variables configurations\n\n```cmd\ncmd\n\n:: Set the environment variables after you have downloaded and unzipped the mkl package,\n:: else CMake would throw an error as `Could NOT find OpenMP`.\nset CMAKE_INCLUDE_PATH={Your directory}\\mkl\\include\nset LIB={Your directory}\\mkl\\lib;%LIB%\n\n:: Read the content in the previous section carefully before you proceed.\n:: [Optional] If you want to override the underlying toolset used by Ninja and Visual Studio with CUDA, please run the following script block.\n:: \"Visual Studio 2019 Developer Command Prompt\" will be run automatically.\n:: Make sure you have CMake >= 3.12 before you do this when you use the Visual Studio generator.\nset CMAKE_GENERATOR_TOOLSET_VERSION=14.27\nset DISTUTILS_USE_SDK=1\nfor /f \"usebackq tokens=*\" %i in (`\"%ProgramFiles(x86)%\\Microsoft Visual Studio\\Installer\\vswhere.exe\" -version [15^,17^) -products * -latest -property installationPath`) do call \"%i\\VC\\Auxiliary\\Build\\vcvarsall.bat\" x64 -vcvars_ver=%CMAKE_GENERATOR_TOOLSET_VERSION%\n\n:: [Optional] If you want to override the CUDA host compiler\nset CUDAHOSTCXX=C:\\Program Files (x86)\\Microsoft Visual Studio\\2019\\Community\\VC\\Tools\\MSVC\\14.27.29110\\bin\\HostX64\\x64\\cl.exe\n\npython -m pip install --no-build-isolation -v -e .\n```\n\n**Intel GPU builds**\n\nIn this mode PyTorch with Intel GPU support will be built.\n\nPlease make sure [the common prerequisites](#prerequisites) as well as [the prerequisites for Intel GPU](#intel-gpu-support) are properly installed and the environment variables are configured prior to starting the build. For build tool support, `Visual Studio 2022` is required.\n\nThen PyTorch can be built with the command:\n\n```cmd\n:: CMD Commands:\n:: Set the CMAKE_PREFIX_PATH to help find corresponding packages\n:: %CONDA_PREFIX% only works after `conda activate custom_env`\n\nif defined CMAKE_PREFIX_PATH (\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library;%CMAKE_PREFIX_PATH%\"\n) else (\n    set \"CMAKE_PREFIX_PATH=%CONDA_PREFIX%\\Library\"\n)\n\npython -m pip install --no-build-isolation -v -e .\n```\n\n##### Adjust Build Options (Optional)\n\nYou can adjust the configuration of cmake variables optionally (without building first), by doing\nthe following. For example, adjusting the pre-detected directories for CuDNN or BLAS can be done\nwith such a step.\n\nOn Linux\n\n```bash\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\nCMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build\n```\n\nOn macOS\n\n```bash\nexport CMAKE_PREFIX_PATH=\"${CONDA_PREFIX:-'$(dirname $(which conda))/../'}:${CMAKE_PREFIX_PATH}\"\nMACOSX_DEPLOYMENT_TARGET=11.0 CMAKE_ONLY=1 python setup.py build\nccmake build  # or cmake-gui build\n```\n\n### Docker Image\n\n#### Using pre-built images\n\nYou can also pull a pre-built docker image from Docker Hub and run with docker v19.03+\n\n```bash\ndocker run --gpus all --rm -ti --ipc=host pytorch/pytorch:latest\n```\n\nPlease note that PyTorch uses shared memory to share data between processes, so if torch multiprocessing is used (e.g.\nfor multithreaded data loaders) the default shared memory segment size that container runs with is not enough, and you\nshould increase shared memory size either with `--ipc=host` or `--shm-size` command line options to `nvidia-docker run`.\n\n#### Building the image yourself\n\n**NOTE:** Must be built with a docker version > 18.06\n\nThe `Dockerfile` is supplied to build images with CUDA 11.1 support and cuDNN v8.\nYou can pass `PYTHON_VERSION=x.y` make variable to specify which Python version is to be used by Miniconda, or leave it\nunset to use the default.\n\n```bash\nmake -f docker.Makefile\n# images are tagged as docker.io/${your_docker_username}/pytorch\n```\n\nYou can also pass the `CMAKE_VARS=\"...\"` environment variable to specify additional CMake variables to be passed to CMake during the build.\nSee [setup.py](./setup.py) for the list of available variables.\n\n```bash\nmake -f docker.Makefile\n```\n\n### Building the Documentation\n\nTo build documentation in various formats, you will need [Sphinx](http://www.sphinx-doc.org)\nand the pytorch_sphinx_theme2.\n\nBefore you build the documentation locally, ensure `torch` is\ninstalled in your environment. For small fixes, you can install the\nnightly version as described in [Getting Started](https://pytorch.org/get-started/locally/).\n\nFor more complex fixes, such as adding a new module and docstrings for\nthe new module, you might need to install torch [from source](#from-source).\nSee [Docstring Guidelines](https://github.com/pytorch/pytorch/wiki/Docstring-Guidelines)\nfor docstring conventions.\n\n```bash\ncd docs/\npip install -r requirements.txt\nmake html\nmake serve\n```\n\nRun `make` to get a list of all available output formats.\n\nIf you get a katex error run `npm install katex`.  If it persists, try\n`npm install -g katex`\n\n> [!NOTE]\n> If you installed `nodejs` with a different package manager (e.g.,\n> `conda`) then `npm` will probably install a version of `katex` that is not\n> compatible with your version of `nodejs` and doc builds will fail.\n> A combination of versions that is known to work is `node@6.13.1` and\n> `katex@0.13.18`. To install the latter with `npm` you can run\n> ```npm install -g katex@0.13.18```\n\n> [!NOTE]\n> If you see a numpy incompatibility error, run:\n> ```\n> pip install 'numpy<2'\n> ```\n\nWhen you make changes to the dependencies run by CI, edit the\n`.ci/docker/requirements-docs.txt` file.\n\n#### Building a PDF\n\nTo compile a PDF of all PyTorch documentation, ensure you have\n`texlive` and LaTeX installed. On macOS, you can install them using:\n\n```\nbrew install --cask mactex\n```\n\nTo create the PDF:\n\n1. Run:\n\n   ```\n   make latexpdf\n   ```\n\n   This will generate the necessary files in the `build/latex` directory.\n\n2. Navigate to this directory and execute:\n\n   ```\n   make LATEXOPTS=\"-interaction=nonstopmode\"\n   ```\n\n   This will produce a `pytorch.pdf` with the desired content. Run this\n   command one more time so that it generates the correct table\n   of contents and index.\n\n> [!NOTE]\n> To view the Table of Contents, switch to the **Table of Contents**\n> view in your PDF viewer.\n\n\n### Previous Versions\n\nInstallation instructions and binaries for previous PyTorch versions may be found\non [our website](https://pytorch.org/get-started/previous-versions).\n\n\n## Getting Started\n\nThree pointers to get you started:\n- [Tutorials: get you started with understanding and using PyTorch](https://pytorch.org/tutorials/)\n- [Examples: easy to understand PyTorch code across all domains](https://github.com/pytorch/examples)\n- [The API Reference](https://pytorch.org/docs/)\n- [Glossary](https://github.com/pytorch/pytorch/blob/main/GLOSSARY.md)\n\n## Resources\n\n* [PyTorch.org](https://pytorch.org/)\n* [PyTorch Tutorials](https://pytorch.org/tutorials/)\n* [PyTorch Examples](https://github.com/pytorch/examples)\n* [PyTorch Models](https://pytorch.org/hub/)\n* [Intro to Deep Learning with PyTorch from Udacity](https://www.udacity.com/course/deep-learning-pytorch--ud188)\n* [Intro to Machine Learning with PyTorch from Udacity](https://www.udacity.com/course/intro-to-machine-learning-nanodegree--nd229)\n* [Deep Neural Networks with PyTorch from Coursera](https://www.coursera.org/learn/deep-neural-networks-with-pytorch)\n* [PyTorch Twitter](https://twitter.com/PyTorch)\n* [PyTorch Blog](https://pytorch.org/blog/)\n* [PyTorch YouTube](https://www.youtube.com/channel/UCWXI5YeOsh03QvJ59PMaXFw)\n\n## Communication\n* Forums: Discuss implementations, research, etc. https://discuss.pytorch.org\n* GitHub Issues: Bug reports, feature requests, install issues, RFCs, thoughts, etc.\n* Slack: The [PyTorch Slack](https://pytorch.slack.com/) hosts a primary audience of moderate to experienced PyTorch users and developers for general chat, online discussions, collaboration, etc. If you are a beginner looking for help, the primary medium is [PyTorch Forums](https://discuss.pytorch.org). If you need a slack invite, please fill this form: https://goo.gl/forms/PP1AGvNHpSaJP8to1\n* Newsletter: No-noise, a one-way email newsletter with important announcements about PyTorch. You can sign-up here: https://eepurl.com/cbG0rv\n* Facebook Page: Important announcements about PyTorch. https://www.facebook.com/pytorch\n* For brand guidelines, please visit our website at [pytorch.org](https://pytorch.org/)\n\n## Releases and Contributing\n\nTypically, PyTorch has three minor releases a year. Please let us know if you encounter a bug by [filing an issue](https://github.com/pytorch/pytorch/issues).\n\nWe appreciate all contributions. If you are planning to contribute back bug-fixes, please do so without any further discussion.\n\nIf you plan to contribute new features, utility functions, or extensions to the core, please first open an issue and discuss the feature with us.\nSending a PR without discussion might end up resulting in a rejected PR because we might be taking the core in a different direction than you might be aware of.\n\nTo learn more about making a contribution to Pytorch, please see our [Contribution page](CONTRIBUTING.md). For more information about PyTorch releases, see [Release page](RELEASE.md).\n\n## The Team\n\nPyTorch is a community-driven project with several skillful engineers and researchers contributing to it.\n\nPyTorch is currently maintained by [Soumith Chintala](http://soumith.ch), [Gregory Chanan](https://github.com/gchanan), [Dmytro Dzhulgakov](https://github.com/dzhulgakov), [Edward Yang](https://github.com/ezyang), [Alban Desmaison](https://github.com/albanD), [Piotr Bialecki](https://github.com/ptrblck) and [Nikita Shulga](https://github.com/malfet) with major contributions coming from hundreds of talented individuals in various forms and means.\nA non-exhaustive but growing list needs to mention: [Trevor Killeen](https://github.com/killeent), [Sasank Chilamkurthy](https://github.com/chsasank), [Sergey Zagoruyko](https://github.com/szagoruyko), [Adam Lerer](https://github.com/adamlerer), [Francisco Massa](https://github.com/fmassa), [Alykhan Tejani](https://github.com/alykhantejani), [Luca Antiga](https://github.com/lantiga), [Alban Desmaison](https://github.com/albanD), [Andreas Koepf](https://github.com/andreaskoepf), [James Bradbury](https://github.com/jekbradbury), [Zeming Lin](https://github.com/ebetica), [Yuandong Tian](https://github.com/yuandong-tian), [Guillaume Lample](https://github.com/glample), [Marat Dukhan](https://github.com/Maratyszcza), [Natalia Gimelshein](https://github.com/ngimel), [Christian Sarofeen](https://github.com/csarofeen), [Martin Raison](https://github.com/martinraison), [Edward Yang](https://github.com/ezyang), [Zachary Devito](https://github.com/zdevito). <!-- codespell:ignore -->\n\nNote: This project is unrelated to [hughperkins/pytorch](https://github.com/hughperkins/pytorch) with the same name. Hugh is a valuable contributor to the Torch community and has helped with many things Torch and PyTorch.\n\n## License\n\nPyTorch has a BSD-style license, as found in the [LICENSE](LICENSE) file.\n",
    "ci_configs": {
      ".github/workflows/_bazel-build-test.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_bazel-build-test.yml",
      ".github/workflows/_binary-build-linux.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_binary-build-linux.yml",
      ".github/workflows/_binary-test-linux.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_binary-test-linux.yml",
      ".github/workflows/_binary-upload.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_binary-upload.yml",
      ".github/workflows/_docs.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_docs.yml",
      ".github/workflows/_get-changed-files.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_get-changed-files.yml",
      ".github/workflows/_link_check.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_link_check.yml",
      ".github/workflows/_linux-build.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_linux-build.yml",
      ".github/workflows/_linux-test-stable-fa3.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_linux-test-stable-fa3.yml",
      ".github/workflows/_linux-test.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_linux-test.yml",
      ".github/workflows/_mac-build.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_mac-build.yml",
      ".github/workflows/_mac-test.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_mac-test.yml",
      ".github/workflows/_rocm-test.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_rocm-test.yml",
      ".github/workflows/_runner-determinator.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_runner-determinator.yml",
      ".github/workflows/_win-build.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_win-build.yml",
      ".github/workflows/_win-test.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_win-test.yml",
      ".github/workflows/_xpu-test.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/_xpu-test.yml",
      ".github/workflows/assigntome-docathon.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/assigntome-docathon.yml",
      ".github/workflows/auto_request_review.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/auto_request_review.yml",
      ".github/workflows/b200-distributed.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/b200-distributed.yml",
      ".github/workflows/b200-symm-mem.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/b200-symm-mem.yml",
      ".github/workflows/build-almalinux-images.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/build-almalinux-images.yml",
      ".github/workflows/build-libtorch-images.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/build-libtorch-images.yml",
      ".github/workflows/build-magma-linux.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/build-magma-linux.yml",
      ".github/workflows/build-magma-rocm-linux.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/build-magma-rocm-linux.yml",
      ".github/workflows/build-magma-windows.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/build-magma-windows.yml",
      ".github/workflows/build-manywheel-images-s390x.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/build-manywheel-images-s390x.yml",
      ".github/workflows/build-manywheel-images.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/build-manywheel-images.yml",
      ".github/workflows/build-triton-wheel.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/build-triton-wheel.yml",
      ".github/workflows/build-vllm-wheel.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/build-vllm-wheel.yml",
      ".github/workflows/check-labels.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/check-labels.yml",
      ".github/workflows/check_mergeability_ghstack.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/check_mergeability_ghstack.yml",
      ".github/workflows/cherry-pick.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/cherry-pick.yml",
      ".github/workflows/close-nonexistent-disable-issues.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/close-nonexistent-disable-issues.yml",
      ".github/workflows/create_release.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/create_release.yml",
      ".github/workflows/delete_old_branches.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/delete_old_branches.yml",
      ".github/workflows/docathon-sync-label.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/docathon-sync-label.yml",
      ".github/workflows/docker-builds.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/docker-builds.yml",
      ".github/workflows/docker-cache-mi300.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/docker-cache-mi300.yml",
      ".github/workflows/docker-release.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/docker-release.yml",
      ".github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-linux-aarch64-binary-manywheel-nightly.yml",
      ".github/workflows/generated-linux-binary-libtorch-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-linux-binary-libtorch-nightly.yml",
      ".github/workflows/generated-linux-binary-manywheel-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-linux-binary-manywheel-nightly.yml",
      ".github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-linux-s390x-binary-manywheel-nightly.yml",
      ".github/workflows/generated-macos-arm64-binary-libtorch-release-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-macos-arm64-binary-libtorch-release-nightly.yml",
      ".github/workflows/generated-macos-arm64-binary-wheel-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-macos-arm64-binary-wheel-nightly.yml",
      ".github/workflows/generated-windows-arm64-binary-libtorch-debug-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-windows-arm64-binary-libtorch-debug-nightly.yml",
      ".github/workflows/generated-windows-arm64-binary-libtorch-release-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-windows-arm64-binary-libtorch-release-nightly.yml",
      ".github/workflows/generated-windows-arm64-binary-wheel-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-windows-arm64-binary-wheel-nightly.yml",
      ".github/workflows/generated-windows-binary-libtorch-debug-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-windows-binary-libtorch-debug-nightly.yml",
      ".github/workflows/generated-windows-binary-libtorch-release-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-windows-binary-libtorch-release-nightly.yml",
      ".github/workflows/generated-windows-binary-wheel-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/generated-windows-binary-wheel-nightly.yml",
      ".github/workflows/h100-cutlass-backend.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/h100-cutlass-backend.yml",
      ".github/workflows/h100-distributed.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/h100-distributed.yml",
      ".github/workflows/h100-symm-mem.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/h100-symm-mem.yml",
      ".github/workflows/inductor-micro-benchmark-x86.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-micro-benchmark-x86.yml",
      ".github/workflows/inductor-micro-benchmark.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-micro-benchmark.yml",
      ".github/workflows/inductor-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-nightly.yml",
      ".github/workflows/inductor-perf-compare.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-compare.yml",
      ".github/workflows/inductor-perf-test-b200.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-b200.yml",
      ".github/workflows/inductor-perf-test-nightly-aarch64.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-nightly-aarch64.yml",
      ".github/workflows/inductor-perf-test-nightly-h100.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-nightly-h100.yml",
      ".github/workflows/inductor-perf-test-nightly-macos.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-nightly-macos.yml",
      ".github/workflows/inductor-perf-test-nightly-rocm-mi300.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-nightly-rocm-mi300.yml",
      ".github/workflows/inductor-perf-test-nightly-rocm-mi355.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-nightly-rocm-mi355.yml",
      ".github/workflows/inductor-perf-test-nightly-x86-zen.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-nightly-x86-zen.yml",
      ".github/workflows/inductor-perf-test-nightly-x86.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-nightly-x86.yml",
      ".github/workflows/inductor-perf-test-nightly-xpu.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-nightly-xpu.yml",
      ".github/workflows/inductor-perf-test-nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-perf-test-nightly.yml",
      ".github/workflows/inductor-periodic.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-periodic.yml",
      ".github/workflows/inductor-rocm-mi300.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-rocm-mi300.yml",
      ".github/workflows/inductor-rocm.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-rocm.yml",
      ".github/workflows/inductor-unittest.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor-unittest.yml",
      ".github/workflows/inductor.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/inductor.yml",
      ".github/workflows/lint-autoformat.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/lint-autoformat.yml",
      ".github/workflows/lint-bc.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/lint-bc.yml",
      ".github/workflows/lint.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/lint.yml",
      ".github/workflows/linux-aarch64.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/linux-aarch64.yml",
      ".github/workflows/llm_td_retrieval.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/llm_td_retrieval.yml",
      ".github/workflows/mac-mps.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/mac-mps.yml",
      ".github/workflows/nightly-s3-uploads.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/nightly-s3-uploads.yml",
      ".github/workflows/nightly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/nightly.yml",
      ".github/workflows/nitpicker.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/nitpicker.yml",
      ".github/workflows/operator_benchmark.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/operator_benchmark.yml",
      ".github/workflows/operator_microbenchmark.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/operator_microbenchmark.yml",
      ".github/workflows/periodic-rocm-mi200.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/periodic-rocm-mi200.yml",
      ".github/workflows/periodic-rocm-mi300.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/periodic-rocm-mi300.yml",
      ".github/workflows/periodic.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/periodic.yml",
      ".github/workflows/pull.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/pull.yml",
      ".github/workflows/quantization-periodic.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/quantization-periodic.yml",
      ".github/workflows/revert.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/revert.yml",
      ".github/workflows/riscv64.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/riscv64.yml",
      ".github/workflows/rocm-mi300.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/rocm-mi300.yml",
      ".github/workflows/rocm-mi355.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/rocm-mi355.yml",
      ".github/workflows/rocm-navi31.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/rocm-navi31.yml",
      ".github/workflows/rocm.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/rocm.yml",
      ".github/workflows/runner-determinator-validator.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/runner-determinator-validator.yml",
      ".github/workflows/runner_determinator_script_sync.yaml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/runner_determinator_script_sync.yaml",
      ".github/workflows/s390.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/s390.yml",
      ".github/workflows/s390x-periodic.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/s390x-periodic.yml",
      ".github/workflows/scorecards.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/scorecards.yml",
      ".github/workflows/slow.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/slow.yml",
      ".github/workflows/stale.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/stale.yml",
      ".github/workflows/target-determination-indexer.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/target-determination-indexer.yml",
      ".github/workflows/target_determination.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/target_determination.yml",
      ".github/workflows/test-b200.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/test-b200.yml",
      ".github/workflows/test-check-binary.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/test-check-binary.yml",
      ".github/workflows/test-h100.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/test-h100.yml",
      ".github/workflows/tools-unit-tests.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/tools-unit-tests.yml",
      ".github/workflows/torchbench.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/torchbench.yml",
      ".github/workflows/trunk-tagging.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/trunk-tagging.yml",
      ".github/workflows/trunk.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/trunk.yml",
      ".github/workflows/trymerge.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/trymerge.yml",
      ".github/workflows/tryrebase.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/tryrebase.yml",
      ".github/workflows/unstable-periodic.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/unstable-periodic.yml",
      ".github/workflows/unstable.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/unstable.yml",
      ".github/workflows/update-viablestrict.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/update-viablestrict.yml",
      ".github/workflows/update_pytorch_labels.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/update_pytorch_labels.yml",
      ".github/workflows/upload-test-stats-while-running.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/upload-test-stats-while-running.yml",
      ".github/workflows/upload-test-stats.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/upload-test-stats.yml",
      ".github/workflows/upload-torch-dynamo-perf-stats.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/upload-torch-dynamo-perf-stats.yml",
      ".github/workflows/upload_test_stats_intermediate.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/upload_test_stats_intermediate.yml",
      ".github/workflows/vllm.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/vllm.yml",
      ".github/workflows/weekly.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/weekly.yml",
      ".github/workflows/win-arm64-build-test.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/win-arm64-build-test.yml",
      ".github/workflows/xpu.yml": "https://raw.githubusercontent.com/pytorch/pytorch/main/.github/workflows/xpu.yml"
    },
    "processed_at": "2025-11-05 23:46:49"
  },
  "fastapi/fastapi": {
    "full_name": "fastapi/fastapi",
    "stargazers_count": 91553,
    "size": 28581,
    "topics": [
      "api",
      "async",
      "asyncio",
      "fastapi",
      "framework",
      "json",
      "json-schema",
      "openapi",
      "openapi3",
      "pydantic",
      "python",
      "python-types",
      "python3",
      "redoc",
      "rest",
      "starlette",
      "swagger",
      "swagger-ui",
      "uvicorn",
      "web"
    ],
    "releases_count": 30,
    "major_releases": [
      {
        "tag_name": "0.121.0",
        "name": "0.121.0",
        "body": "### Features\r\n\r\n* ‚ú® Add support for dependencies with scopes, support `scope=\"request\"` for dependencies with `yield` that exit before the response is sent. PR [#14262](https://github.com/fastapi/fastapi/pull/14262) by [@tiangolo](https://github.com/tiangolo).\r\n    * New docs: [Dependencies with `yield` - Early exit and `scope`](https://fastapi.tiangolo.com/tutorial/dependencies/dependencies-with-yield/#early-exit-and-scope).\r\n\r\n### Internal\r\n\r\n* üë• Update FastAPI People - Contributors and Translators. PR [#14273](https://github.com/fastapi/fastapi/pull/14273) by [@tiangolo](https://github.com/tiangolo).\r\n* üë• Update FastAPI People - Sponsors. PR [#14274](https://github.com/fastapi/fastapi/pull/14274) by [@tiangolo](https://github.com/tiangolo).\r\n* üë• Update FastAPI GitHub topic repositories. PR [#14280](https://github.com/fastapi/fastapi/pull/14280) by [@tiangolo](https://github.com/tiangolo).\r\n* ‚¨Ü Bump mkdocs-macros-plugin from 1.4.0 to 1.4.1. PR [#14277](https://github.com/fastapi/fastapi/pull/14277) by [@dependabot[bot]](https://github.com/apps/dependabot).\r\n* ‚¨Ü Bump mkdocstrings[python] from 0.26.1 to 0.30.1. PR [#14279](https://github.com/fastapi/fastapi/pull/14279) by [@dependabot[bot]](https://github.com/apps/dependabot).\r\n",
        "published_at": "2025-11-03T10:25:21Z",
        "target_commitish": "master",
        "version_tuple": [
          0,
          121,
          0
        ],
        "version_key": "0.121.0"
      },
      {
        "tag_name": "0.120.4",
        "name": "0.120.4",
        "body": "### Fixes\r\n\r\n* üêõ Fix security schemes in OpenAPI when added at the top level app. PR [#14266](https://github.com/fastapi/fastapi/pull/14266) by [@YuriiMotov](https://github.com/YuriiMotov).",
        "published_at": "2025-10-31T18:36:42Z",
        "target_commitish": "master",
        "version_tuple": [
          0,
          120,
          4
        ],
        "version_key": "0.120.4"
      },
      {
        "tag_name": "0.120.3",
        "name": "0.120.3",
        "body": "### Refactors\r\n\r\n* ‚ôªÔ∏è Reduce internal cyclic recursion in dependencies, from 2 functions calling each other to 1 calling itself. PR [#14256](https://github.com/fastapi/fastapi/pull/14256) by [@tiangolo](https://github.com/tiangolo).\r\n* ‚ôªÔ∏è Refactor internals of dependencies, simplify code and remove `get_param_sub_dependant`. PR [#14255](https://github.com/fastapi/fastapi/pull/14255) by [@tiangolo](https://github.com/tiangolo).\r\n* ‚ôªÔ∏è Refactor internals of dependencies, simplify using dataclasses. PR [#14254](https://github.com/fastapi/fastapi/pull/14254) by [@tiangolo](https://github.com/tiangolo).\r\n\r\n### Docs\r\n\r\n* üìù Update note for untranslated pages. PR [#14257](https://github.com/fastapi/fastapi/pull/14257) by [@YuriiMotov](https://github.com/YuriiMotov).",
        "published_at": "2025-10-30T20:41:01Z",
        "target_commitish": "master",
        "version_tuple": [
          0,
          120,
          3
        ],
        "version_key": "0.120.3"
      }
    ],
    "readme_content": "<p align=\"center\">\n  <a href=\"https://fastapi.tiangolo.com\"><img src=\"https://fastapi.tiangolo.com/img/logo-margin/logo-teal.png\" alt=\"FastAPI\"></a>\n</p>\n<p align=\"center\">\n    <em>FastAPI framework, high performance, easy to learn, fast to code, ready for production</em>\n</p>\n<p align=\"center\">\n<a href=\"https://github.com/fastapi/fastapi/actions?query=workflow%3ATest+event%3Apush+branch%3Amaster\" target=\"_blank\">\n    <img src=\"https://github.com/fastapi/fastapi/actions/workflows/test.yml/badge.svg?event=push&branch=master\" alt=\"Test\">\n</a>\n<a href=\"https://coverage-badge.samuelcolvin.workers.dev/redirect/fastapi/fastapi\" target=\"_blank\">\n    <img src=\"https://coverage-badge.samuelcolvin.workers.dev/fastapi/fastapi.svg\" alt=\"Coverage\">\n</a>\n<a href=\"https://pypi.org/project/fastapi\" target=\"_blank\">\n    <img src=\"https://img.shields.io/pypi/v/fastapi?color=%2334D058&label=pypi%20package\" alt=\"Package version\">\n</a>\n<a href=\"https://pypi.org/project/fastapi\" target=\"_blank\">\n    <img src=\"https://img.shields.io/pypi/pyversions/fastapi.svg?color=%2334D058\" alt=\"Supported Python versions\">\n</a>\n</p>\n\n---\n\n**Documentation**: <a href=\"https://fastapi.tiangolo.com\" target=\"_blank\">https://fastapi.tiangolo.com</a>\n\n**Source Code**: <a href=\"https://github.com/fastapi/fastapi\" target=\"_blank\">https://github.com/fastapi/fastapi</a>\n\n---\n\nFastAPI is a modern, fast (high-performance), web framework for building APIs with Python based on standard Python type hints.\n\nThe key features are:\n\n* **Fast**: Very high performance, on par with **NodeJS** and **Go** (thanks to Starlette and Pydantic). [One of the fastest Python frameworks available](#performance).\n* **Fast to code**: Increase the speed to develop features by about 200% to 300%. *\n* **Fewer bugs**: Reduce about 40% of human (developer) induced errors. *\n* **Intuitive**: Great editor support. <abbr title=\"also known as auto-complete, autocompletion, IntelliSense\">Completion</abbr> everywhere. Less time debugging.\n* **Easy**: Designed to be easy to use and learn. Less time reading docs.\n* **Short**: Minimize code duplication. Multiple features from each parameter declaration. Fewer bugs.\n* **Robust**: Get production-ready code. With automatic interactive documentation.\n* **Standards-based**: Based on (and fully compatible with) the open standards for APIs: <a href=\"https://github.com/OAI/OpenAPI-Specification\" class=\"external-link\" target=\"_blank\">OpenAPI</a> (previously known as Swagger) and <a href=\"https://json-schema.org/\" class=\"external-link\" target=\"_blank\">JSON Schema</a>.\n\n<small>* estimation based on tests on an internal development team, building production applications.</small>\n\n## Sponsors\n\n<!-- sponsors -->\n\n<a href=\"https://blockbee.io?ref=fastapi\" target=\"_blank\" title=\"BlockBee Cryptocurrency Payment Gateway\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/blockbee.png\"></a>\n<a href=\"https://github.com/scalar/scalar/?utm_source=fastapi&utm_medium=website&utm_campaign=main-badge\" target=\"_blank\" title=\"Scalar: Beautiful Open-Source API References from Swagger/OpenAPI files\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/scalar.svg\"></a>\n<a href=\"https://www.propelauth.com/?utm_source=fastapi&utm_campaign=1223&utm_medium=mainbadge\" target=\"_blank\" title=\"Auth, user management and more for your B2B product\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/propelauth.png\"></a>\n<a href=\"https://zuplo.link/fastapi-gh\" target=\"_blank\" title=\"Zuplo: Deploy, Secure, Document, and Monetize your FastAPI\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/zuplo.png\"></a>\n<a href=\"https://liblab.com?utm_source=fastapi\" target=\"_blank\" title=\"liblab - Generate SDKs from FastAPI\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/liblab.png\"></a>\n<a href=\"https://docs.render.com/deploy-fastapi?utm_source=deploydoc&utm_medium=referral&utm_campaign=fastapi\" target=\"_blank\" title=\"Deploy & scale any full-stack web app on Render. Focus on building apps, not infra.\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/render.svg\"></a>\n<a href=\"https://www.coderabbit.ai/?utm_source=fastapi&utm_medium=badge&utm_campaign=fastapi\" target=\"_blank\" title=\"Cut Code Review Time & Bugs in Half with CodeRabbit\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/coderabbit.png\"></a>\n<a href=\"https://subtotal.com/?utm_source=fastapi&utm_medium=sponsorship&utm_campaign=open-source\" target=\"_blank\" title=\"The Gold Standard in Retail Account Linking\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/subtotal.svg\"></a>\n<a href=\"https://docs.railway.com/guides/fastapi?utm_medium=integration&utm_source=docs&utm_campaign=fastapi\" target=\"_blank\" title=\"Deploy enterprise applications at startup speed\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/railway.png\"></a>\n<a href=\"https://serpapi.com/?utm_source=fastapi_website\" target=\"_blank\" title=\"SerpApi: Web Search API\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/serpapi.png\"></a>\n<a href=\"https://databento.com/?utm_source=fastapi&utm_medium=sponsor&utm_content=display\" target=\"_blank\" title=\"Pay as you go for market data\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/databento.svg\"></a>\n<a href=\"https://speakeasy.com/editor?utm_source=fastapi+repo&utm_medium=github+sponsorship\" target=\"_blank\" title=\"SDKs for your API | Speakeasy\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/speakeasy.png\"></a>\n<a href=\"https://www.svix.com/\" target=\"_blank\" title=\"Svix - Webhooks as a service\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/svix.svg\"></a>\n<a href=\"https://www.stainlessapi.com/?utm_source=fastapi&utm_medium=referral\" target=\"_blank\" title=\"Stainless | Generate best-in-class SDKs\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/stainless.png\"></a>\n<a href=\"https://www.permit.io/blog/implement-authorization-in-fastapi?utm_source=github&utm_medium=referral&utm_campaign=fastapi\" target=\"_blank\" title=\"Fine-Grained Authorization for FastAPI\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/permit.png\"></a>\n<a href=\"https://www.interviewpal.com/?utm_source=fastapi&utm_medium=open-source&utm_campaign=dev-hiring\" target=\"_blank\" title=\"InterviewPal - AI Interview Coach for Engineers and Devs\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/interviewpal.png\"></a>\n<a href=\"https://dribia.com/en/\" target=\"_blank\" title=\"Dribia - Data Science within your reach\"><img src=\"https://fastapi.tiangolo.com/img/sponsors/dribia.png\"></a>\n\n<!-- /sponsors -->\n\n<a href=\"https://fastapi.tiangolo.com/fastapi-people/#sponsors\" class=\"external-link\" target=\"_blank\">Other sponsors</a>\n\n## Opinions\n\n\"_[...] I'm using **FastAPI** a ton these days. [...] I'm actually planning to use it for all of my team's **ML services at Microsoft**. Some of them are getting integrated into the core **Windows** product and some **Office** products._\"\n\n<div style=\"text-align: right; margin-right: 10%;\">Kabir Khan - <strong>Microsoft</strong> <a href=\"https://github.com/fastapi/fastapi/pull/26\" target=\"_blank\"><small>(ref)</small></a></div>\n\n---\n\n\"_We adopted the **FastAPI** library to spawn a **REST** server that can be queried to obtain **predictions**. [for Ludwig]_\"\n\n<div style=\"text-align: right; margin-right: 10%;\">Piero Molino, Yaroslav Dudin, and Sai Sumanth Miryala - <strong>Uber</strong> <a href=\"https://eng.uber.com/ludwig-v0-2/\" target=\"_blank\"><small>(ref)</small></a></div>\n\n---\n\n\"_**Netflix** is pleased to announce the open-source release of our **crisis management** orchestration framework: **Dispatch**! [built with **FastAPI**]_\"\n\n<div style=\"text-align: right; margin-right: 10%;\">Kevin Glisson, Marc Vilanova, Forest Monsen - <strong>Netflix</strong> <a href=\"https://netflixtechblog.com/introducing-dispatch-da4b8a2a8072\" target=\"_blank\"><small>(ref)</small></a></div>\n\n---\n\n\"_I‚Äôm over the moon excited about **FastAPI**. It‚Äôs so fun!_\"\n\n<div style=\"text-align: right; margin-right: 10%;\">Brian Okken - <strong><a href=\"https://pythonbytes.fm/episodes/show/123/time-to-right-the-py-wrongs?time_in_sec=855\" target=\"_blank\">Python Bytes</a> podcast host</strong> <a href=\"https://x.com/brianokken/status/1112220079972728832\" target=\"_blank\"><small>(ref)</small></a></div>\n\n---\n\n\"_Honestly, what you've built looks super solid and polished. In many ways, it's what I wanted **Hug** to be - it's really inspiring to see someone build that._\"\n\n<div style=\"text-align: right; margin-right: 10%;\">Timothy Crosley - <strong><a href=\"https://github.com/hugapi/hug\" target=\"_blank\">Hug</a> creator</strong> <a href=\"https://news.ycombinator.com/item?id=19455465\" target=\"_blank\"><small>(ref)</small></a></div>\n\n---\n\n\"_If you're looking to learn one **modern framework** for building REST APIs, check out **FastAPI** [...] It's fast, easy to use and easy to learn [...]_\"\n\n\"_We've switched over to **FastAPI** for our **APIs** [...] I think you'll like it [...]_\"\n\n<div style=\"text-align: right; margin-right: 10%;\">Ines Montani - Matthew Honnibal - <strong><a href=\"https://explosion.ai\" target=\"_blank\">Explosion AI</a> founders - <a href=\"https://spacy.io\" target=\"_blank\">spaCy</a> creators</strong> <a href=\"https://x.com/_inesmontani/status/1144173225322143744\" target=\"_blank\"><small>(ref)</small></a> - <a href=\"https://x.com/honnibal/status/1144031421859655680\" target=\"_blank\"><small>(ref)</small></a></div>\n\n---\n\n\"_If anyone is looking to build a production Python API, I would highly recommend **FastAPI**. It is **beautifully designed**, **simple to use** and **highly scalable**, it has become a **key component** in our API first development strategy and is driving many automations and services such as our Virtual TAC Engineer._\"\n\n<div style=\"text-align: right; margin-right: 10%;\">Deon Pillsbury - <strong>Cisco</strong> <a href=\"https://www.linkedin.com/posts/deonpillsbury_cisco-cx-python-activity-6963242628536487936-trAp/\" target=\"_blank\"><small>(ref)</small></a></div>\n\n---\n\n## **Typer**, the FastAPI of CLIs\n\n<a href=\"https://typer.tiangolo.com\" target=\"_blank\"><img src=\"https://typer.tiangolo.com/img/logo-margin/logo-margin-vector.svg\" style=\"width: 20%;\"></a>\n\nIf you are building a <abbr title=\"Command Line Interface\">CLI</abbr> app to be used in the terminal instead of a web API, check out <a href=\"https://typer.tiangolo.com/\" class=\"external-link\" target=\"_blank\">**Typer**</a>.\n\n**Typer** is FastAPI's little sibling. And it's intended to be the **FastAPI of CLIs**. ‚å®Ô∏è üöÄ\n\n## Requirements\n\nFastAPI stands on the shoulders of giants:\n\n* <a href=\"https://www.starlette.dev/\" class=\"external-link\" target=\"_blank\">Starlette</a> for the web parts.\n* <a href=\"https://docs.pydantic.dev/\" class=\"external-link\" target=\"_blank\">Pydantic</a> for the data parts.\n\n## Installation\n\nCreate and activate a <a href=\"https://fastapi.tiangolo.com/virtual-environments/\" class=\"external-link\" target=\"_blank\">virtual environment</a> and then install FastAPI:\n\n<div class=\"termy\">\n\n```console\n$ pip install \"fastapi[standard]\"\n\n---> 100%\n```\n\n</div>\n\n**Note**: Make sure you put `\"fastapi[standard]\"` in quotes to ensure it works in all terminals.\n\n## Example\n\n### Create it\n\nCreate a file `main.py` with:\n\n```Python\nfrom typing import Union\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}\n```\n\n<details markdown=\"1\">\n<summary>Or use <code>async def</code>...</summary>\n\nIf your code uses `async` / `await`, use `async def`:\n\n```Python hl_lines=\"9  14\"\nfrom typing import Union\n\nfrom fastapi import FastAPI\n\napp = FastAPI()\n\n\n@app.get(\"/\")\nasync def read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\nasync def read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}\n```\n\n**Note**:\n\nIf you don't know, check the _\"In a hurry?\"_ section about <a href=\"https://fastapi.tiangolo.com/async/#in-a-hurry\" target=\"_blank\">`async` and `await` in the docs</a>.\n\n</details>\n\n### Run it\n\nRun the server with:\n\n<div class=\"termy\">\n\n```console\n$ fastapi dev main.py\n\n ‚ï≠‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ FastAPI CLI - Development mode ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïÆ\n ‚îÇ                                                     ‚îÇ\n ‚îÇ  Serving at: http://127.0.0.1:8000                  ‚îÇ\n ‚îÇ                                                     ‚îÇ\n ‚îÇ  API docs: http://127.0.0.1:8000/docs               ‚îÇ\n ‚îÇ                                                     ‚îÇ\n ‚îÇ  Running in development mode, for production use:   ‚îÇ\n ‚îÇ                                                     ‚îÇ\n ‚îÇ  fastapi run                                        ‚îÇ\n ‚îÇ                                                     ‚îÇ\n ‚ï∞‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ïØ\n\nINFO:     Will watch for changes in these directories: ['/home/user/code/awesomeapp']\nINFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\nINFO:     Started reloader process [2248755] using WatchFiles\nINFO:     Started server process [2248757]\nINFO:     Waiting for application startup.\nINFO:     Application startup complete.\n```\n\n</div>\n\n<details markdown=\"1\">\n<summary>About the command <code>fastapi dev main.py</code>...</summary>\n\nThe command `fastapi dev` reads your `main.py` file, detects the **FastAPI** app in it, and starts a server using <a href=\"https://www.uvicorn.dev\" class=\"external-link\" target=\"_blank\">Uvicorn</a>.\n\nBy default, `fastapi dev` will start with auto-reload enabled for local development.\n\nYou can read more about it in the <a href=\"https://fastapi.tiangolo.com/fastapi-cli/\" target=\"_blank\">FastAPI CLI docs</a>.\n\n</details>\n\n### Check it\n\nOpen your browser at <a href=\"http://127.0.0.1:8000/items/5?q=somequery\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/items/5?q=somequery</a>.\n\nYou will see the JSON response as:\n\n```JSON\n{\"item_id\": 5, \"q\": \"somequery\"}\n```\n\nYou already created an API that:\n\n* Receives HTTP requests in the _paths_ `/` and `/items/{item_id}`.\n* Both _paths_ take `GET` <em>operations</em> (also known as HTTP _methods_).\n* The _path_ `/items/{item_id}` has a _path parameter_ `item_id` that should be an `int`.\n* The _path_ `/items/{item_id}` has an optional `str` _query parameter_ `q`.\n\n### Interactive API docs\n\nNow go to <a href=\"http://127.0.0.1:8000/docs\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/docs</a>.\n\nYou will see the automatic interactive API documentation (provided by <a href=\"https://github.com/swagger-api/swagger-ui\" class=\"external-link\" target=\"_blank\">Swagger UI</a>):\n\n![Swagger UI](https://fastapi.tiangolo.com/img/index/index-01-swagger-ui-simple.png)\n\n### Alternative API docs\n\nAnd now, go to <a href=\"http://127.0.0.1:8000/redoc\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/redoc</a>.\n\nYou will see the alternative automatic documentation (provided by <a href=\"https://github.com/Rebilly/ReDoc\" class=\"external-link\" target=\"_blank\">ReDoc</a>):\n\n![ReDoc](https://fastapi.tiangolo.com/img/index/index-02-redoc-simple.png)\n\n## Example upgrade\n\nNow modify the file `main.py` to receive a body from a `PUT` request.\n\nDeclare the body using standard Python types, thanks to Pydantic.\n\n```Python hl_lines=\"4  9-12  25-27\"\nfrom typing import Union\n\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI()\n\n\nclass Item(BaseModel):\n    name: str\n    price: float\n    is_offer: Union[bool, None] = None\n\n\n@app.get(\"/\")\ndef read_root():\n    return {\"Hello\": \"World\"}\n\n\n@app.get(\"/items/{item_id}\")\ndef read_item(item_id: int, q: Union[str, None] = None):\n    return {\"item_id\": item_id, \"q\": q}\n\n\n@app.put(\"/items/{item_id}\")\ndef update_item(item_id: int, item: Item):\n    return {\"item_name\": item.name, \"item_id\": item_id}\n```\n\nThe `fastapi dev` server should reload automatically.\n\n### Interactive API docs upgrade\n\nNow go to <a href=\"http://127.0.0.1:8000/docs\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/docs</a>.\n\n* The interactive API documentation will be automatically updated, including the new body:\n\n![Swagger UI](https://fastapi.tiangolo.com/img/index/index-03-swagger-02.png)\n\n* Click on the button \"Try it out\", it allows you to fill the parameters and directly interact with the API:\n\n![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-04-swagger-03.png)\n\n* Then click on the \"Execute\" button, the user interface will communicate with your API, send the parameters, get the results and show them on the screen:\n\n![Swagger UI interaction](https://fastapi.tiangolo.com/img/index/index-05-swagger-04.png)\n\n### Alternative API docs upgrade\n\nAnd now, go to <a href=\"http://127.0.0.1:8000/redoc\" class=\"external-link\" target=\"_blank\">http://127.0.0.1:8000/redoc</a>.\n\n* The alternative documentation will also reflect the new query parameter and body:\n\n![ReDoc](https://fastapi.tiangolo.com/img/index/index-06-redoc-02.png)\n\n### Recap\n\nIn summary, you declare **once** the types of parameters, body, etc. as function parameters.\n\nYou do that with standard modern Python types.\n\nYou don't have to learn a new syntax, the methods or classes of a specific library, etc.\n\nJust standard **Python**.\n\nFor example, for an `int`:\n\n```Python\nitem_id: int\n```\n\nor for a more complex `Item` model:\n\n```Python\nitem: Item\n```\n\n...and with that single declaration you get:\n\n* Editor support, including:\n    * Completion.\n    * Type checks.\n* Validation of data:\n    * Automatic and clear errors when the data is invalid.\n    * Validation even for deeply nested JSON objects.\n* <abbr title=\"also known as: serialization, parsing, marshalling\">Conversion</abbr> of input data: coming from the network to Python data and types. Reading from:\n    * JSON.\n    * Path parameters.\n    * Query parameters.\n    * Cookies.\n    * Headers.\n    * Forms.\n    * Files.\n* <abbr title=\"also known as: serialization, parsing, marshalling\">Conversion</abbr> of output data: converting from Python data and types to network data (as JSON):\n    * Convert Python types (`str`, `int`, `float`, `bool`, `list`, etc).\n    * `datetime` objects.\n    * `UUID` objects.\n    * Database models.\n    * ...and many more.\n* Automatic interactive API documentation, including 2 alternative user interfaces:\n    * Swagger UI.\n    * ReDoc.\n\n---\n\nComing back to the previous code example, **FastAPI** will:\n\n* Validate that there is an `item_id` in the path for `GET` and `PUT` requests.\n* Validate that the `item_id` is of type `int` for `GET` and `PUT` requests.\n    * If it is not, the client will see a useful, clear error.\n* Check if there is an optional query parameter named `q` (as in `http://127.0.0.1:8000/items/foo?q=somequery`) for `GET` requests.\n    * As the `q` parameter is declared with `= None`, it is optional.\n    * Without the `None` it would be required (as is the body in the case with `PUT`).\n* For `PUT` requests to `/items/{item_id}`, read the body as JSON:\n    * Check that it has a required attribute `name` that should be a `str`.\n    * Check that it has a required attribute `price` that has to be a `float`.\n    * Check that it has an optional attribute `is_offer`, that should be a `bool`, if present.\n    * All this would also work for deeply nested JSON objects.\n* Convert from and to JSON automatically.\n* Document everything with OpenAPI, that can be used by:\n    * Interactive documentation systems.\n    * Automatic client code generation systems, for many languages.\n* Provide 2 interactive documentation web interfaces directly.\n\n---\n\nWe just scratched the surface, but you already get the idea of how it all works.\n\nTry changing the line with:\n\n```Python\n    return {\"item_name\": item.name, \"item_id\": item_id}\n```\n\n...from:\n\n```Python\n        ... \"item_name\": item.name ...\n```\n\n...to:\n\n```Python\n        ... \"item_price\": item.price ...\n```\n\n...and see how your editor will auto-complete the attributes and know their types:\n\n![editor support](https://fastapi.tiangolo.com/img/vscode-completion.png)\n\nFor a more complete example including more features, see the <a href=\"https://fastapi.tiangolo.com/tutorial/\">Tutorial - User Guide</a>.\n\n**Spoiler alert**: the tutorial - user guide includes:\n\n* Declaration of **parameters** from other different places as: **headers**, **cookies**, **form fields** and **files**.\n* How to set **validation constraints** as `maximum_length` or `regex`.\n* A very powerful and easy to use **<abbr title=\"also known as components, resources, providers, services, injectables\">Dependency Injection</abbr>** system.\n* Security and authentication, including support for **OAuth2** with **JWT tokens** and **HTTP Basic** auth.\n* More advanced (but equally easy) techniques for declaring **deeply nested JSON models** (thanks to Pydantic).\n* **GraphQL** integration with <a href=\"https://strawberry.rocks\" class=\"external-link\" target=\"_blank\">Strawberry</a> and other libraries.\n* Many extra features (thanks to Starlette) as:\n    * **WebSockets**\n    * extremely easy tests based on HTTPX and `pytest`\n    * **CORS**\n    * **Cookie Sessions**\n    * ...and more.\n\n## Performance\n\nIndependent TechEmpower benchmarks show **FastAPI** applications running under Uvicorn as <a href=\"https://www.techempower.com/benchmarks/#section=test&runid=7464e520-0dc2-473d-bd34-dbdfd7e85911&hw=ph&test=query&l=zijzen-7\" class=\"external-link\" target=\"_blank\">one of the fastest Python frameworks available</a>, only below Starlette and Uvicorn themselves (used internally by FastAPI). (*)\n\nTo understand more about it, see the section <a href=\"https://fastapi.tiangolo.com/benchmarks/\" class=\"internal-link\" target=\"_blank\">Benchmarks</a>.\n\n## Dependencies\n\nFastAPI depends on Pydantic and Starlette.\n\n### `standard` Dependencies\n\nWhen you install FastAPI with `pip install \"fastapi[standard]\"` it comes with the `standard` group of optional dependencies:\n\nUsed by Pydantic:\n\n* <a href=\"https://github.com/JoshData/python-email-validator\" target=\"_blank\"><code>email-validator</code></a> - for email validation.\n\nUsed by Starlette:\n\n* <a href=\"https://www.python-httpx.org\" target=\"_blank\"><code>httpx</code></a> - Required if you want to use the `TestClient`.\n* <a href=\"https://jinja.palletsprojects.com\" target=\"_blank\"><code>jinja2</code></a> - Required if you want to use the default template configuration.\n* <a href=\"https://github.com/Kludex/python-multipart\" target=\"_blank\"><code>python-multipart</code></a> - Required if you want to support form <abbr title=\"converting the string that comes from an HTTP request into Python data\">\"parsing\"</abbr>, with `request.form()`.\n\nUsed by FastAPI:\n\n* <a href=\"https://www.uvicorn.dev\" target=\"_blank\"><code>uvicorn</code></a> - for the server that loads and serves your application. This includes `uvicorn[standard]`, which includes some dependencies (e.g. `uvloop`) needed for high performance serving.\n* `fastapi-cli[standard]` - to provide the `fastapi` command.\n    * This includes `fastapi-cloud-cli`, which allows you to deploy your FastAPI application to <a href=\"https://fastapicloud.com\" class=\"external-link\" target=\"_blank\">FastAPI Cloud</a>.\n\n### Without `standard` Dependencies\n\nIf you don't want to include the `standard` optional dependencies, you can install with `pip install fastapi` instead of `pip install \"fastapi[standard]\"`.\n\n### Without `fastapi-cloud-cli`\n\nIf you want to install FastAPI with the standard dependencies but without the `fastapi-cloud-cli`, you can install with `pip install \"fastapi[standard-no-fastapi-cloud-cli]\"`.\n\n### Additional Optional Dependencies\n\nThere are some additional dependencies you might want to install.\n\nAdditional optional Pydantic dependencies:\n\n* <a href=\"https://docs.pydantic.dev/latest/usage/pydantic_settings/\" target=\"_blank\"><code>pydantic-settings</code></a> - for settings management.\n* <a href=\"https://docs.pydantic.dev/latest/usage/types/extra_types/extra_types/\" target=\"_blank\"><code>pydantic-extra-types</code></a> - for extra types to be used with Pydantic.\n\nAdditional optional FastAPI dependencies:\n\n* <a href=\"https://github.com/ijl/orjson\" target=\"_blank\"><code>orjson</code></a> - Required if you want to use `ORJSONResponse`.\n* <a href=\"https://github.com/esnme/ultrajson\" target=\"_blank\"><code>ujson</code></a> - Required if you want to use `UJSONResponse`.\n\n## License\n\nThis project is licensed under the terms of the MIT license.\n",
    "ci_configs": {
      ".github/workflows/add-to-project.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/add-to-project.yml",
      ".github/workflows/build-docs.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/build-docs.yml",
      ".github/workflows/contributors.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/contributors.yml",
      ".github/workflows/deploy-docs.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/deploy-docs.yml",
      ".github/workflows/detect-conflicts.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/detect-conflicts.yml",
      ".github/workflows/issue-manager.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/issue-manager.yml",
      ".github/workflows/label-approved.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/label-approved.yml",
      ".github/workflows/labeler.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/labeler.yml",
      ".github/workflows/latest-changes.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/latest-changes.yml",
      ".github/workflows/notify-translations.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/notify-translations.yml",
      ".github/workflows/people.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/people.yml",
      ".github/workflows/publish.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/publish.yml",
      ".github/workflows/smokeshow.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/smokeshow.yml",
      ".github/workflows/sponsors.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/sponsors.yml",
      ".github/workflows/test-redistribute.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/test-redistribute.yml",
      ".github/workflows/test.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/test.yml",
      ".github/workflows/topic-repos.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/topic-repos.yml",
      ".github/workflows/translate.yml": "https://raw.githubusercontent.com/fastapi/fastapi/master/.github/workflows/translate.yml"
    },
    "processed_at": "2025-11-06 00:32:49"
  },
  "home-assistant/core": {
    "full_name": "home-assistant/core",
    "stargazers_count": 82380,
    "size": 758165,
    "topics": [
      "asyncio",
      "hacktoberfest",
      "home-automation",
      "internet-of-things",
      "iot",
      "mqtt",
      "python",
      "raspberry-pi"
    ],
    "releases_count": 30,
    "major_releases": [
      {
        "tag_name": "2025.10.3",
        "name": "2025.10.3",
        "body": "- Bump aioasuswrt to 1.5.1 ([@kennedyshead] - [#153209]) ([asuswrt docs]) (dependency)\r\n- PushSafer: Handle empty data section properly ([@LennartC] - [#154109]) ([pushsafer docs])\r\n- Remove redudant state write in Smart Meter Texas ([@srirams] - [#154126]) ([smart_meter_texas docs])\r\n- Fix state class for Overkiz water consumption ([@Yvan13120] - [#154164]) ([overkiz docs])\r\n- Bump frontend 20251001.4 ([@piitaya] - [#154218]) ([frontend docs])\r\n- Bump aioamazondevices to 6.4.1 ([@chemelli74] - [#154228]) ([alexa_devices docs]) (dependency)\r\n- Move URL out of Mealie strings.json ([@andrew-codechimp] - [#154230]) ([mealie docs])\r\n- Move URL out of Mastodon strings.json ([@andrew-codechimp] - [#154231]) ([mastodon docs])\r\n- Move URL out of Switcher strings.json ([@thecode] - [#154240]) ([switcher_kis docs])\r\n- Remove URL from ViCare strings.json ([@CFenner] - [#154243]) ([vicare docs])\r\n- Fix August integration to handle unavailable OAuth implementation at startup ([@bdraco] - [#154244]) ([august docs])\r\n- Fix Yale integration to handle unavailable OAuth implementation at startup ([@bdraco] - [#154245]) ([yale docs])\r\n- Move url like strings to placeholders for nibe ([@elupus] - [#154249]) ([nibe_heatpump docs])\r\n- Add description placeholders in Uptime Kuma config flow ([@tr4nt0r] - [#154252]) ([uptime_kuma docs])\r\n- Add description placeholders to pyLoad config flow ([@tr4nt0r] - [#154254]) ([pyload docs])\r\n- Fix home wiziard total increasing sensors returning 0 ([@jbouwh] - [#154264]) ([homewizard docs])\r\n- Bump pyprobeplus to 1.1.0 ([@pantherale0] - [#154265]) ([probe_plus docs]) (dependency)\r\n- Update Snoo strings.json to include weaning_baseline ([@dschafer] - [#154268]) ([snoo docs])\r\n- Move Electricity Maps url out of strings.json ([@jpbede] - [#154284]) ([co2signal docs])\r\n- Bump aioamazondevices to 6.4.3 ([@chemelli74] - [#154293]) ([alexa_devices docs]) (dependency)\r\n- Move URL out of Overkiz Config Flow descriptions ([@iMicknl] - [#154315]) ([overkiz docs])\r\n- AsusWRT: Pass only online clients to the device list from the API ([@Vaskivskyi] - [#154322]) ([asuswrt docs])\r\n- Move Ecobee authorization URL out of strings.json ([@ogruendel] - [#154332]) ([ecobee docs])\r\n- Move URLs out of SABnzbd strings.json ([@shaiu] - [#154333]) ([sabnzbd docs])\r\n- Move developer url out of strings.json for coinbase setup flow ([@ogruendel] - [#154339]) ([coinbase docs])\r\n- Fix Bluetooth discovery for devices with alternating advertisement names ([@bdraco] - [#154347]) ([bluetooth docs])\r\n- Bump opower to 0.15.7 ([@tronikos] - [#154351]) ([opower docs]) (dependency)\r\n- update pysqueezebox lib to 0.13.0 ([@wollew] - [#154358]) ([squeezebox docs]) (dependency)\r\n- Move URL out of sfr_box strings.json ([@epenet] - [#154364]) ([sfr_box docs])\r\n- Move translatable URLs out of strings.json for huawei lte ([@sonianuj287] - [#154368]) ([huawei_lte docs])\r\n- Bump aioairq to 0.4.7 ([@Sibgatulin] - [#154386]) ([airq docs]) (dependency)\r\n- Bump aiocomelit to 1.1.2 ([@chemelli74] - [#154393]) ([comelit docs]) (dependency)\r\n- Use `async_schedule_reload` instead of `async_reload` for ZHA ([@puddly] - [#154397]) ([zha docs])\r\n- Move igloohome API access URL into constant placeholders ([@DannyS95] - [#154430]) ([igloohome docs])\r\n- Add missing`long_press` entry for trigger_type in strings.json for Hue ([@mvdwetering] - [#154437]) ([hue docs])\r\n- Move translatable URLs out of strings.json for isy994 ([@sonianuj287] - [#154464]) ([isy994 docs])\r\n- OpenUV: Fix update by skipping when protection window is null ([@wbyoung] - [#154487]) ([openuv docs])\r\n- Bump aioamazondevices to 6.4.4 ([@chemelli74] - [#154538]) ([alexa_devices docs]) (dependency)\r\n- Move URL out of Nuheat strings.json ([@tstabrawa] - [#154580]) ([nuheat docs])\r\n- Bump pyvesync version to 3.1.2 ([@cdnninja] - [#154650]) ([vesync docs]) (dependency)\r\n\r\n[#152881]: https://github.com/home-assistant/core/pull/152881\r\n[#153209]: https://github.com/home-assistant/core/pull/153209\r\n[#153582]: https://github.com/home-assistant/core/pull/153582\r\n[#154109]: https://github.com/home-assistant/core/pull/154109\r\n[#154126]: https://github.com/home-assistant/core/pull/154126\r\n[#154164]: https://github.com/home-assistant/core/pull/154164\r\n[#154181]: https://github.com/home-assistant/core/pull/154181\r\n[#154218]: https://github.com/home-assistant/core/pull/154218\r\n[#154228]: https://github.com/home-assistant/core/pull/154228\r\n[#154230]: https://github.com/home-assistant/core/pull/154230\r\n[#154231]: https://github.com/home-assistant/core/pull/154231\r\n[#154240]: https://github.com/home-assistant/core/pull/154240\r\n[#154243]: https://github.com/home-assistant/core/pull/154243\r\n[#154244]: https://github.com/home-assistant/core/pull/154244\r\n[#154245]: https://github.com/home-assistant/core/pull/154245\r\n[#154249]: https://github.com/home-assistant/core/pull/154249\r\n[#154252]: https://github.com/home-assistant/core/pull/154252\r\n[#154254]: https://github.com/home-assistant/core/pull/154254\r\n[#154264]: https://github.com/home-assistant/core/pull/154264\r\n[#154265]: https://github.com/home-assistant/core/pull/154265\r\n[#154268]: https://github.com/home-assistant/core/pull/154268\r\n[#154284]: https://github.com/home-assistant/core/pull/154284\r\n[#154293]: https://github.com/home-assistant/core/pull/154293\r\n[#154315]: https://github.com/home-assistant/core/pull/154315\r\n[#154322]: https://github.com/home-assistant/core/pull/154322\r\n[#154332]: https://github.com/home-assistant/core/pull/154332\r\n[#154333]: https://github.com/home-assistant/core/pull/154333\r\n[#154339]: https://github.com/home-assistant/core/pull/154339\r\n[#154347]: https://github.com/home-assistant/core/pull/154347\r\n[#154351]: https://github.com/home-assistant/core/pull/154351\r\n[#154358]: https://github.com/home-assistant/core/pull/154358\r\n[#154364]: https://github.com/home-assistant/core/pull/154364\r\n[#154368]: https://github.com/home-assistant/core/pull/154368\r\n[#154386]: https://github.com/home-assistant/core/pull/154386\r\n[#154393]: https://github.com/home-assistant/core/pull/154393\r\n[#154397]: https://github.com/home-assistant/core/pull/154397\r\n[#154430]: https://github.com/home-assistant/core/pull/154430\r\n[#154437]: https://github.com/home-assistant/core/pull/154437\r\n[#154464]: https://github.com/home-assistant/core/pull/154464\r\n[#154487]: https://github.com/home-assistant/core/pull/154487\r\n[#154538]: https://github.com/home-assistant/core/pull/154538\r\n[#154580]: https://github.com/home-assistant/core/pull/154580\r\n[#154650]: https://github.com/home-assistant/core/pull/154650\r\n[@CFenner]: https://github.com/CFenner\r\n[@DannyS95]: https://github.com/DannyS95\r\n[@LennartC]: https://github.com/LennartC\r\n[@Sibgatulin]: https://github.com/Sibgatulin\r\n[@Vaskivskyi]: https://github.com/Vaskivskyi\r\n[@Yvan13120]: https://github.com/Yvan13120\r\n[@andrew-codechimp]: https://github.com/andrew-codechimp\r\n[@bdraco]: https://github.com/bdraco\r\n[@cdnninja]: https://github.com/cdnninja\r\n[@chemelli74]: https://github.com/chemelli74\r\n[@dschafer]: https://github.com/dschafer\r\n[@elupus]: https://github.com/elupus\r\n[@epenet]: https://github.com/epenet\r\n[@frenck]: https://github.com/frenck\r\n[@iMicknl]: https://github.com/iMicknl\r\n[@jbouwh]: https://github.com/jbouwh\r\n[@jpbede]: https://github.com/jpbede\r\n[@kennedyshead]: https://github.com/kennedyshead\r\n[@mvdwetering]: https://github.com/mvdwetering\r\n[@ogruendel]: https://github.com/ogruendel\r\n[@pantherale0]: https://github.com/pantherale0\r\n[@piitaya]: https://github.com/piitaya\r\n[@puddly]: https://github.com/puddly\r\n[@shaiu]: https://github.com/shaiu\r\n[@sonianuj287]: https://github.com/sonianuj287\r\n[@srirams]: https://github.com/srirams\r\n[@thecode]: https://github.com/thecode\r\n[@tr4nt0r]: https://github.com/tr4nt0r\r\n[@tronikos]: https://github.com/tronikos\r\n[@tstabrawa]: https://github.com/tstabrawa\r\n[@wbyoung]: https://github.com/wbyoung\r\n[@wollew]: https://github.com/wollew\r\n[airq docs]: https://www.home-assistant.io/integrations/airq/\r\n[alexa_devices docs]: https://www.home-assistant.io/integrations/alexa_devices/\r\n[asuswrt docs]: https://www.home-assistant.io/integrations/asuswrt/\r\n[august docs]: https://www.home-assistant.io/integrations/august/\r\n[bluetooth docs]: https://www.home-assistant.io/integrations/bluetooth/\r\n[co2signal docs]: https://www.home-assistant.io/integrations/co2signal/\r\n[coinbase docs]: https://www.home-assistant.io/integrations/coinbase/\r\n[comelit docs]: https://www.home-assistant.io/integrations/comelit/\r\n[ecobee docs]: https://www.home-assistant.io/integrations/ecobee/\r\n[frontend docs]: https://www.home-assistant.io/integrations/frontend/\r\n[homewizard docs]: https://www.home-assistant.io/integrations/homewizard/\r\n[huawei_lte docs]: https://www.home-assistant.io/integrations/huawei_lte/\r\n[hue docs]: https://www.home-assistant.io/integrations/hue/\r\n[igloohome docs]: https://www.home-assistant.io/integrations/igloohome/\r\n[isy994 docs]: https://www.home-assistant.io/integrations/isy994/\r\n[mastodon docs]: https://www.home-assistant.io/integrations/mastodon/\r\n[mealie docs]: https://www.home-assistant.io/integrations/mealie/\r\n[nibe_heatpump docs]: https://www.home-assistant.io/integrations/nibe_heatpump/\r\n[nuheat docs]: https://www.home-assistant.io/integrations/nuheat/\r\n[openuv docs]: https://www.home-assistant.io/integrations/openuv/\r\n[opower docs]: https://www.home-assistant.io/integrations/opower/\r\n[overkiz docs]: https://www.home-assistant.io/integrations/overkiz/\r\n[probe_plus docs]: https://www.home-assistant.io/integrations/probe_plus/\r\n[pushsafer docs]: https://www.home-assistant.io/integrations/pushsafer/\r\n[pyload docs]: https://www.home-assistant.io/integrations/pyload/\r\n[sabnzbd docs]: https://www.home-assistant.io/integrations/sabnzbd/\r\n[sfr_box docs]: https://www.home-assistant.io/integrations/sfr_box/\r\n[smart_meter_texas docs]: https://www.home-assistant.io/integrations/smart_meter_texas/\r\n[snoo docs]: https://www.home-assistant.io/integrations/snoo/\r\n[squeezebox docs]: https://www.home-assistant.io/integrations/squeezebox/\r\n[switcher_kis docs]: https://www.home-assistant.io/integrations/switcher_kis/\r\n[uptime_kuma docs]: https://www.home-assistant.io/integrations/uptime_kuma/\r\n[vesync docs]: https://www.home-assistant.io/integrations/vesync/\r\n[vicare docs]: https://www.home-assistant.io/integrations/vicare/\r\n[yale docs]: https://www.home-assistant.io/integrations/yale/\r\n[zha docs]: https://www.home-assistant.io/integrations/zha/",
        "published_at": "2025-10-17T21:15:07Z",
        "target_commitish": "master",
        "version_tuple": [
          2025,
          10,
          3
        ],
        "version_key": "2025.10.3"
      },
      {
        "tag_name": "2025.10.2",
        "name": "2025.10.2",
        "body": "- Prevent reloading the ZHA integration while adapter firmware is being updated ([@puddly] - [#152626]) ([zha docs]) ([homeassistant_hardware docs])\r\n- Wallbox fix Rate Limit issue for multiple chargers ([@hesselonline] - [#153074]) ([wallbox docs])\r\n- Fix power device classes for system bridge ([@timmo001] - [#153201]) ([system_bridge docs])\r\n- Bump PyCync to 0.4.1 ([@Kinachi249] - [#153401]) ([cync docs]) (dependency)\r\n- Updated VRM client and accounted for missing forecasts ([@AndyTempel] - [#153464]) ([victron_remote_monitoring docs]) (dependency)\r\n- Bump python-roborock to 2.50.2 ([@Lash-L] - [#153561]) ([roborock docs]) (dependency)\r\n- Bump aioamazondevices to 6.2.8 ([@chemelli74] - [#153592]) ([alexa_devices docs]) (dependency)\r\n- Switch Roborock to v4 of the code login api ([@Lash-L] - [#153593]) ([roborock docs])\r\n- Fix MQTT Lock state reset to unknown when a reset payload is received ([@jbouwh] - [#153647]) ([mqtt docs])\r\n- Gemini: Use default model instead of recommended where applicable ([@Shulyaka] - [#153676]) ([google_generative_ai_conversation docs])\r\n- Fix ViCare pressure sensors missing unit of measurement ([@CFenner] - [#153691]) ([vicare docs])\r\n- Bump pyvesync to 3.1.0 ([@cdnninja] - [#153693]) ([vesync docs]) (dependency)\r\n- Modbus Fix message_wait_milliseconds is no longer applied ([@peetersch] - [#153709]) ([modbus docs])\r\n- Bump opower to 0.15.6 ([@tronikos] - [#153714]) ([opower docs]) (dependency)\r\n- Version bump pydaikin to 2.17.0 ([@fredrike] - [#153718]) ([daikin docs]) (dependency)\r\n- Version bump pydaikin to 2.17.1 ([@fredrike] - [#153726]) ([daikin docs]) (dependency)\r\n- Fix missing google_assistant_sdk.send_text_command ([@tronikos] - [#153735]) ([google_assistant_sdk docs])\r\n- Bump airOS to 0.5.5 using formdata for v6 firmware ([@CoMPaTech] - [#153736]) ([airos docs]) (dependency)\r\n- Align Shelly `presencezone` entity to the new API/firmware ([@bieniu] - [#153737]) ([shelly docs])\r\n- Synology DSM: Don't reinitialize API during configuration ([@oyvindwe] - [#153739]) ([synology_dsm docs])\r\n- Upgrade python-melcloud to 0.1.2 ([@Sander0542] - [#153742]) ([melcloud docs]) (dependency)\r\n- Fix sensors availability check for Alexa Devices ([@chemelli74] - [#153743]) ([alexa_devices docs])\r\n- Bump aioamazondevices to 6.2.9 ([@chemelli74] - [#153756]) ([alexa_devices docs])\r\n- Remove stale entities from Alexa Devices ([@chemelli74] - [#153759]) ([alexa_devices docs])\r\n- vesync correct fan set modes ([@cdnninja] - [#153761]) ([vesync docs])\r\n- Handle ESPHome discoveries with uninitialized Z-Wave antennas ([@balloob] - [#153790]) ([zwave_js docs])\r\n- Fix Tuya cover position when only control is available ([@epenet] - [#153803]) ([tuya docs])\r\n- Bump pySmartThings to 3.3.1 ([@joostlek] - [#153826]) ([smartthings docs]) (dependency)\r\n- Catch update exception in AirGradient ([@joostlek] - [#153828]) ([airgradient docs])\r\n- Add motion presets to SmartThings AC ([@joostlek] - [#153830]) ([smartthings docs])\r\n- Fix delay_on and auto_off with multiple triggers ([@Petro31] - [#153839]) ([template docs])\r\n- Fix PIN validation for Comelit SimpleHome ([@chemelli74] - [#153840]) ([comelit docs])\r\n- Bump aiocomelit to 1.1.1 ([@chemelli74] - [#153843]) ([comelit docs]) (dependency)\r\n- Limit SimpliSafe websocket connection attempts during startup ([@bachya] - [#153853]) ([simplisafe docs])\r\n- Handle timeout errors gracefully in Nord Pool services ([@gjohansson-ST] - [#153856]) ([nordpool docs])\r\n- Add plate_count for Miele KM7575 ([@derytive] - [#153868]) ([miele docs])\r\n- Fix restore cover state for Comelit SimpleHome ([@chemelli74] - [#153887]) ([comelit docs])\r\n- fix typo in icon assignment of AccuWeather integration ([@CFenner] - [#153890]) ([accuweather docs])\r\n- Add missing translation string for Satel Integra subentry type ([@Tommatheussen] - [#153905]) ([satel_integra docs])\r\n- Do not auto-set up ZHA zeroconf discoveries during onboarding ([@TheJulianJES] - [#153914]) ([zha docs])\r\n- `sharkiq` dependency bump to 1.4.2 ([@Freebien] - [#153931]) ([sharkiq docs]) (dependency)\r\n- Fix HA hardware configuration message for Thread without HAOS ([@TheJulianJES] - [#153933]) ([homeassistant_hardware docs])\r\n- Adjust OTBR config entry name for ZBT-2 ([@TheJulianJES] - [#153940]) ([otbr docs])\r\n- Bump pylamarzocco to 2.1.2 ([@zweckj] - [#153950]) ([lamarzocco docs]) (dependency)\r\n- Bump holidays to 0.82 ([@gjohansson-ST] - [#153952]) ([workday docs]) ([holiday docs]) (dependency)\r\n- Fix update interval for AccuWeather hourly forecast ([@bieniu] - [#153957]) ([accuweather docs])\r\n- Bump env-canada to 0.11.3 ([@michaeldavie] - [#153967]) ([environment_canada docs])\r\n- Fix empty llm api list in chat log ([@arturpragacz] - [#153996]) ([conversation docs])\r\n- Don't mark ZHA coordinator as via_device with itself ([@joostlek] - [#154004]) ([zha docs])\r\n- Filter out invalid Renault vehicles ([@epenet] - [#154070]) ([renault docs])\r\n- Bump aioamazondevices to 6.4.0 ([@chemelli74] - [#154071]) ([alexa_devices docs]) (dependency)\r\n- Bump brother to version 5.1.1 ([@bieniu] - [#154080]) ([brother docs]) (dependency)\r\n- Fix for multiple Lyrion Music Server on a single Home Assistant server for Squeezebox ([@peteS-UK] - [#154081]) ([squeezebox docs])\r\n- Z-Wave: ESPHome discovery to update all options ([@balloob] - [#154113]) ([zwave_js docs])\r\n- Add missing entity category and icons for smlight integration ([@piitaya] - [#154131]) ([smlight docs])\r\n- Update frontend to 20251001.2 ([@bramkragten] - [#154143]) ([frontend docs]) (dependency)\r\n- IOmeter bump version v0.2.0 ([@jukrebs] - [#154150]) ([iometer docs]) (dependency)\r\n- Bump deebot-client to 15.1.0 ([@edenhaus] - [#154154]) ([ecovacs docs]) (dependency)\r\n- Fix Shelly RPC cover update when the device is not initialized ([@thecode] - [#154159]) ([shelly docs])\r\n- Fix shelly remove orphaned entities ([@thecode] - [#154182]) ([shelly docs])\r\n\r\n[#152626]: https://github.com/home-assistant/core/pull/152626\r\n[#152881]: https://github.com/home-assistant/core/pull/152881\r\n[#153074]: https://github.com/home-assistant/core/pull/153074\r\n[#153201]: https://github.com/home-assistant/core/pull/153201\r\n[#153401]: https://github.com/home-assistant/core/pull/153401\r\n[#153464]: https://github.com/home-assistant/core/pull/153464\r\n[#153561]: https://github.com/home-assistant/core/pull/153561\r\n[#153582]: https://github.com/home-assistant/core/pull/153582\r\n[#153592]: https://github.com/home-assistant/core/pull/153592\r\n[#153593]: https://github.com/home-assistant/core/pull/153593\r\n[#153647]: https://github.com/home-assistant/core/pull/153647\r\n[#153676]: https://github.com/home-assistant/core/pull/153676\r\n[#153691]: https://github.com/home-assistant/core/pull/153691\r\n[#153693]: https://github.com/home-assistant/core/pull/153693\r\n[#153709]: https://github.com/home-assistant/core/pull/153709\r\n[#153714]: https://github.com/home-assistant/core/pull/153714\r\n[#153718]: https://github.com/home-assistant/core/pull/153718\r\n[#153726]: https://github.com/home-assistant/core/pull/153726\r\n[#153735]: https://github.com/home-assistant/core/pull/153735\r\n[#153736]: https://github.com/home-assistant/core/pull/153736\r\n[#153737]: https://github.com/home-assistant/core/pull/153737\r\n[#153739]: https://github.com/home-assistant/core/pull/153739\r\n[#153742]: https://github.com/home-assistant/core/pull/153742\r\n[#153743]: https://github.com/home-assistant/core/pull/153743\r\n[#153756]: https://github.com/home-assistant/core/pull/153756\r\n[#153759]: https://github.com/home-assistant/core/pull/153759\r\n[#153761]: https://github.com/home-assistant/core/pull/153761\r\n[#153790]: https://github.com/home-assistant/core/pull/153790\r\n[#153803]: https://github.com/home-assistant/core/pull/153803\r\n[#153826]: https://github.com/home-assistant/core/pull/153826\r\n[#153828]: https://github.com/home-assistant/core/pull/153828\r\n[#153830]: https://github.com/home-assistant/core/pull/153830\r\n[#153839]: https://github.com/home-assistant/core/pull/153839\r\n[#153840]: https://github.com/home-assistant/core/pull/153840\r\n[#153843]: https://github.com/home-assistant/core/pull/153843\r\n[#153853]: https://github.com/home-assistant/core/pull/153853\r\n[#153856]: https://github.com/home-assistant/core/pull/153856\r\n[#153868]: https://github.com/home-assistant/core/pull/153868\r\n[#153887]: https://github.com/home-assistant/core/pull/153887\r\n[#153890]: https://github.com/home-assistant/core/pull/153890\r\n[#153905]: https://github.com/home-assistant/core/pull/153905\r\n[#153914]: https://github.com/home-assistant/core/pull/153914\r\n[#153931]: https://github.com/home-assistant/core/pull/153931\r\n[#153933]: https://github.com/home-assistant/core/pull/153933\r\n[#153940]: https://github.com/home-assistant/core/pull/153940\r\n[#153950]: https://github.com/home-assistant/core/pull/153950\r\n[#153952]: https://github.com/home-assistant/core/pull/153952\r\n[#153957]: https://github.com/home-assistant/core/pull/153957\r\n[#153967]: https://github.com/home-assistant/core/pull/153967\r\n[#153996]: https://github.com/home-assistant/core/pull/153996\r\n[#154004]: https://github.com/home-assistant/core/pull/154004\r\n[#154070]: https://github.com/home-assistant/core/pull/154070\r\n[#154071]: https://github.com/home-assistant/core/pull/154071\r\n[#154080]: https://github.com/home-assistant/core/pull/154080\r\n[#154081]: https://github.com/home-assistant/core/pull/154081\r\n[#154113]: https://github.com/home-assistant/core/pull/154113\r\n[#154131]: https://github.com/home-assistant/core/pull/154131\r\n[#154143]: https://github.com/home-assistant/core/pull/154143\r\n[#154150]: https://github.com/home-assistant/core/pull/154150\r\n[#154154]: https://github.com/home-assistant/core/pull/154154\r\n[#154159]: https://github.com/home-assistant/core/pull/154159\r\n[#154182]: https://github.com/home-assistant/core/pull/154182\r\n[@AndyTempel]: https://github.com/AndyTempel\r\n[@CFenner]: https://github.com/CFenner\r\n[@CoMPaTech]: https://github.com/CoMPaTech\r\n[@Freebien]: https://github.com/Freebien\r\n[@Kinachi249]: https://github.com/Kinachi249\r\n[@Lash-L]: https://github.com/Lash-L\r\n[@Petro31]: https://github.com/Petro31\r\n[@Sander0542]: https://github.com/Sander0542\r\n[@Shulyaka]: https://github.com/Shulyaka\r\n[@TheJulianJES]: https://github.com/TheJulianJES\r\n[@Tommatheussen]: https://github.com/Tommatheussen\r\n[@arturpragacz]: https://github.com/arturpragacz\r\n[@bachya]: https://github.com/bachya\r\n[@balloob]: https://github.com/balloob\r\n[@bieniu]: https://github.com/bieniu\r\n[@bramkragten]: https://github.com/bramkragten\r\n[@cdnninja]: https://github.com/cdnninja\r\n[@chemelli74]: https://github.com/chemelli74\r\n[@derytive]: https://github.com/derytive\r\n[@edenhaus]: https://github.com/edenhaus\r\n[@epenet]: https://github.com/epenet\r\n[@fredrike]: https://github.com/fredrike\r\n[@frenck]: https://github.com/frenck\r\n[@gjohansson-ST]: https://github.com/gjohansson-ST\r\n[@hesselonline]: https://github.com/hesselonline\r\n[@jbouwh]: https://github.com/jbouwh\r\n[@joostlek]: https://github.com/joostlek\r\n[@jukrebs]: https://github.com/jukrebs\r\n[@michaeldavie]: https://github.com/michaeldavie\r\n[@oyvindwe]: https://github.com/oyvindwe\r\n[@peetersch]: https://github.com/peetersch\r\n[@peteS-UK]: https://github.com/peteS-UK\r\n[@piitaya]: https://github.com/piitaya\r\n[@puddly]: https://github.com/puddly\r\n[@thecode]: https://github.com/thecode\r\n[@timmo001]: https://github.com/timmo001\r\n[@tronikos]: https://github.com/tronikos\r\n[@zweckj]: https://github.com/zweckj\r\n[accuweather docs]: https://www.home-assistant.io/integrations/accuweather/\r\n[airgradient docs]: https://www.home-assistant.io/integrations/airgradient/\r\n[airos docs]: https://www.home-assistant.io/integrations/airos/\r\n[alexa_devices docs]: https://www.home-assistant.io/integrations/alexa_devices/\r\n[brother docs]: https://www.home-assistant.io/integrations/brother/\r\n[comelit docs]: https://www.home-assistant.io/integrations/comelit/\r\n[conversation docs]: https://www.home-assistant.io/integrations/conversation/\r\n[cync docs]: https://www.home-assistant.io/integrations/cync/\r\n[daikin docs]: https://www.home-assistant.io/integrations/daikin/\r\n[ecovacs docs]: https://www.home-assistant.io/integrations/ecovacs/\r\n[environment_canada docs]: https://www.home-assistant.io/integrations/environment_canada/\r\n[frontend docs]: https://www.home-assistant.io/integrations/frontend/\r\n[google_assistant_sdk docs]: https://www.home-assistant.io/integrations/google_assistant_sdk/\r\n[google_generative_ai_conversation docs]: https://www.home-assistant.io/integrations/google_generative_ai_conversation/\r\n[holiday docs]: https://www.home-assistant.io/integrations/holiday/\r\n[homeassistant_hardware docs]: https://www.home-assistant.io/integrations/homeassistant_hardware/\r\n[iometer docs]: https://www.home-assistant.io/integrations/iometer/\r\n[lamarzocco docs]: https://www.home-assistant.io/integrations/lamarzocco/\r\n[melcloud docs]: https://www.home-assistant.io/integrations/melcloud/\r\n[miele docs]: https://www.home-assistant.io/integrations/miele/\r\n[modbus docs]: https://www.home-assistant.io/integrations/modbus/\r\n[mqtt docs]: https://www.home-assistant.io/integrations/mqtt/\r\n[nordpool docs]: https://www.home-assistant.io/integrations/nordpool/\r\n[opower docs]: https://www.home-assistant.io/integrations/opower/\r\n[otbr docs]: https://www.home-assistant.io/integrations/otbr/\r\n[renault docs]: https://www.home-assistant.io/integrations/renault/\r\n[roborock docs]: https://www.home-assistant.io/integrations/roborock/\r\n[satel_integra docs]: https://www.home-assistant.io/integrations/satel_integra/\r\n[sharkiq docs]: https://www.home-assistant.io/integrations/sharkiq/\r\n[shelly docs]: https://www.home-assistant.io/integrations/shelly/\r\n[simplisafe docs]: https://www.home-assistant.io/integrations/simplisafe/\r\n[smartthings docs]: https://www.home-assistant.io/integrations/smartthings/\r\n[smlight docs]: https://www.home-assistant.io/integrations/smlight/\r\n[squeezebox docs]: https://www.home-assistant.io/integrations/squeezebox/\r\n[synology_dsm docs]: https://www.home-assistant.io/integrations/synology_dsm/\r\n[system_bridge docs]: https://www.home-assistant.io/integrations/system_bridge/\r\n[template docs]: https://www.home-assistant.io/integrations/template/\r\n[tuya docs]: https://www.home-assistant.io/integrations/tuya/\r\n[vesync docs]: https://www.home-assistant.io/integrations/vesync/\r\n[vicare docs]: https://www.home-assistant.io/integrations/vicare/\r\n[victron_remote_monitoring docs]: https://www.home-assistant.io/integrations/victron_remote_monitoring/\r\n[wallbox docs]: https://www.home-assistant.io/integrations/wallbox/\r\n[workday docs]: https://www.home-assistant.io/integrations/workday/\r\n[zha docs]: https://www.home-assistant.io/integrations/zha/\r\n[zwave_js docs]: https://www.home-assistant.io/integrations/zwave_js/",
        "published_at": "2025-10-10T21:20:11Z",
        "target_commitish": "master",
        "version_tuple": [
          2025,
          10,
          2
        ],
        "version_key": "2025.10.2"
      },
      {
        "tag_name": "2025.10.1",
        "name": "2025.10.1",
        "body": "- Bump airOS dependency ([@CoMPaTech] - [#153065]) ([airos docs]) (dependency)\r\n- Bump airOS module for alternative login url ([@CoMPaTech] - [#153317]) ([airos docs]) (dependency)\r\n- Bump aiohasupervisor to 0.3.3 ([@agners] - [#153344]) ([hassio docs]) (dependency)\r\n- Do not reset the adapter twice during ZHA options flow migration ([@puddly] - [#153345]) ([zha docs])\r\n- Fix Nord Pool 15 minute interval ([@gjohansson-ST] - [#153350]) ([nordpool docs])\r\n- Explicitly check for None in raw value processing of modbus ([@alengwenus] - [#153352]) ([modbus docs])\r\n- Set config entry to None in ProxmoxVE ([@mib1185] - [#153357]) ([proxmoxve docs])\r\n- Explicit pass in the config entry to coordinator in airtouch4 ([@mib1185] - [#153361]) ([airtouch4 docs])\r\n- Add Roborock mop intensity translations ([@starkillerOG] - [#153380]) ([roborock docs])\r\n- Correct blocking update in ToGrill with lack of notifications ([@elupus] - [#153387]) ([togrill docs])\r\n- Bump python-roborock to 2.49.1 ([@Lash-L] - [#153396]) ([roborock docs]) (dependency)\r\n- Pushover: Handle empty data section properly ([@linuxkidd] - [#153397]) ([pushover docs])\r\n- Increase onedrive upload chunk size ([@zweckj] - [#153406]) ([onedrive docs])\r\n- Bump pyportainer 1.0.2 ([@erwindouna] - [#153326]) ([portainer docs]) (dependency)\r\n- Bump pyportainer 1.0.3 ([@erwindouna] - [#153413]) ([portainer docs]) (dependency)\r\n- Disable thinking for unsupported gemini models ([@Shulyaka] - [#153415]) ([google_generative_ai_conversation docs])\r\n- Fix Satel Integra creating new binary sensors on YAML import ([@Tommatheussen] - [#153419]) ([satel_integra docs])\r\n- Update `markdown` field description in ntfy integration ([@tr4nt0r] - [#153421]) ([ntfy docs])\r\n- Fix Z-Wave RGB light turn on causing rare `ZeroDivisionError` ([@TheJulianJES] - [#153422]) ([zwave_js docs])\r\n- Bump aiohomekit to 3.2.19 ([@bdraco] - [#153423]) ([homekit_controller docs]) (dependency)\r\n- Fix sentence-casing in user-facing strings of `slack` ([@NoRi2909] - [#153427]) ([slack docs])\r\n- Add missing translation for media browser default title ([@timmo001] - [#153430]) ([media_source docs])\r\n- Fix missing powerconsumptionreport in Smartthings ([@joostlek] - [#153438]) ([smartthings docs])\r\n- Update Home Assistant base image to 2025.10.0 ([@agners] - [#153441]) (dependency)\r\n- Disable baudrate bootloader reset for ZBT-2 ([@puddly] - [#153443]) ([homeassistant_connect_zbt2 docs])\r\n- Add translation for turbo fan mode in SmartThings ([@joostlek] - [#153445]) ([smartthings docs])\r\n- Fix next event in workday calendar ([@gjohansson-ST] - [#153465]) ([workday docs])\r\n- Update OVOEnergy to 3.0.1 ([@timmo001] - [#153476]) ([ovo_energy docs]) (dependency)\r\n- Fix missing parameter pass in onedrive ([@zweckj] - [#153478]) ([onedrive docs])\r\n- Bump pyTibber to 0.32.2 ([@Danielhiversen] - [#153484]) ([tibber docs]) (dependency)\r\n- Bump reolink-aio to 0.16.1 ([@starkillerOG] - [#153489]) ([reolink docs]) (dependency)\r\n- Fix VeSync zero fan speed handling ([@cdnninja] - [#153493]) ([vesync docs])\r\n- Bump universal-silabs-flasher to 0.0.35 ([@puddly] - [#153500]) ([homeassistant_hardware docs]) (dependency)\r\n- Debounce updates in Idasen Desk ([@abmantis] - [#153503]) ([idasen_desk docs])\r\n- Z-Wave to support migrating from USB to socket with same home ID ([@balloob] - [#153522]) ([zwave_js docs])\r\n- When discovering a Z-Wave adapter, always configure add-on in config flow ([@balloob] - [#153575]) ([zwave_js docs])\r\n\r\n[#152881]: https://github.com/home-assistant/core/pull/152881\r\n[#153065]: https://github.com/home-assistant/core/pull/153065\r\n[#153317]: https://github.com/home-assistant/core/pull/153317\r\n[#153326]: https://github.com/home-assistant/core/pull/153326\r\n[#153344]: https://github.com/home-assistant/core/pull/153344\r\n[#153345]: https://github.com/home-assistant/core/pull/153345\r\n[#153350]: https://github.com/home-assistant/core/pull/153350\r\n[#153352]: https://github.com/home-assistant/core/pull/153352\r\n[#153357]: https://github.com/home-assistant/core/pull/153357\r\n[#153361]: https://github.com/home-assistant/core/pull/153361\r\n[#153380]: https://github.com/home-assistant/core/pull/153380\r\n[#153387]: https://github.com/home-assistant/core/pull/153387\r\n[#153396]: https://github.com/home-assistant/core/pull/153396\r\n[#153397]: https://github.com/home-assistant/core/pull/153397\r\n[#153406]: https://github.com/home-assistant/core/pull/153406\r\n[#153413]: https://github.com/home-assistant/core/pull/153413\r\n[#153415]: https://github.com/home-assistant/core/pull/153415\r\n[#153419]: https://github.com/home-assistant/core/pull/153419\r\n[#153421]: https://github.com/home-assistant/core/pull/153421\r\n[#153422]: https://github.com/home-assistant/core/pull/153422\r\n[#153423]: https://github.com/home-assistant/core/pull/153423\r\n[#153427]: https://github.com/home-assistant/core/pull/153427\r\n[#153430]: https://github.com/home-assistant/core/pull/153430\r\n[#153438]: https://github.com/home-assistant/core/pull/153438\r\n[#153441]: https://github.com/home-assistant/core/pull/153441\r\n[#153443]: https://github.com/home-assistant/core/pull/153443\r\n[#153445]: https://github.com/home-assistant/core/pull/153445\r\n[#153465]: https://github.com/home-assistant/core/pull/153465\r\n[#153476]: https://github.com/home-assistant/core/pull/153476\r\n[#153478]: https://github.com/home-assistant/core/pull/153478\r\n[#153484]: https://github.com/home-assistant/core/pull/153484\r\n[#153489]: https://github.com/home-assistant/core/pull/153489\r\n[#153493]: https://github.com/home-assistant/core/pull/153493\r\n[#153500]: https://github.com/home-assistant/core/pull/153500\r\n[#153503]: https://github.com/home-assistant/core/pull/153503\r\n[#153522]: https://github.com/home-assistant/core/pull/153522\r\n[#153575]: https://github.com/home-assistant/core/pull/153575\r\n[@CoMPaTech]: https://github.com/CoMPaTech\r\n[@Danielhiversen]: https://github.com/Danielhiversen\r\n[@Lash-L]: https://github.com/Lash-L\r\n[@NoRi2909]: https://github.com/NoRi2909\r\n[@Shulyaka]: https://github.com/Shulyaka\r\n[@TheJulianJES]: https://github.com/TheJulianJES\r\n[@Tommatheussen]: https://github.com/Tommatheussen\r\n[@abmantis]: https://github.com/abmantis\r\n[@agners]: https://github.com/agners\r\n[@alengwenus]: https://github.com/alengwenus\r\n[@balloob]: https://github.com/balloob\r\n[@bdraco]: https://github.com/bdraco\r\n[@cdnninja]: https://github.com/cdnninja\r\n[@elupus]: https://github.com/elupus\r\n[@erwindouna]: https://github.com/erwindouna\r\n[@frenck]: https://github.com/frenck\r\n[@gjohansson-ST]: https://github.com/gjohansson-ST\r\n[@joostlek]: https://github.com/joostlek\r\n[@linuxkidd]: https://github.com/linuxkidd\r\n[@mib1185]: https://github.com/mib1185\r\n[@puddly]: https://github.com/puddly\r\n[@starkillerOG]: https://github.com/starkillerOG\r\n[@timmo001]: https://github.com/timmo001\r\n[@tr4nt0r]: https://github.com/tr4nt0r\r\n[@zweckj]: https://github.com/zweckj\r\n[airos docs]: https://www.home-assistant.io/integrations/airos/\r\n[airtouch4 docs]: https://www.home-assistant.io/integrations/airtouch4/\r\n[google_generative_ai_conversation docs]: https://www.home-assistant.io/integrations/google_generative_ai_conversation/\r\n[hassio docs]: https://www.home-assistant.io/integrations/hassio/\r\n[homeassistant_connect_zbt2 docs]: https://www.home-assistant.io/integrations/homeassistant_connect_zbt2/\r\n[homeassistant_hardware docs]: https://www.home-assistant.io/integrations/homeassistant_hardware/\r\n[homekit_controller docs]: https://www.home-assistant.io/integrations/homekit_controller/\r\n[idasen_desk docs]: https://www.home-assistant.io/integrations/idasen_desk/\r\n[media_source docs]: https://www.home-assistant.io/integrations/media_source/\r\n[modbus docs]: https://www.home-assistant.io/integrations/modbus/\r\n[nordpool docs]: https://www.home-assistant.io/integrations/nordpool/\r\n[ntfy docs]: https://www.home-assistant.io/integrations/ntfy/\r\n[onedrive docs]: https://www.home-assistant.io/integrations/onedrive/\r\n[ovo_energy docs]: https://www.home-assistant.io/integrations/ovo_energy/\r\n[portainer docs]: https://www.home-assistant.io/integrations/portainer/\r\n[proxmoxve docs]: https://www.home-assistant.io/integrations/proxmoxve/\r\n[pushover docs]: https://www.home-assistant.io/integrations/pushover/\r\n[reolink docs]: https://www.home-assistant.io/integrations/reolink/\r\n[roborock docs]: https://www.home-assistant.io/integrations/roborock/\r\n[satel_integra docs]: https://www.home-assistant.io/integrations/satel_integra/\r\n[slack docs]: https://www.home-assistant.io/integrations/slack/\r\n[smartthings docs]: https://www.home-assistant.io/integrations/smartthings/\r\n[tibber docs]: https://www.home-assistant.io/integrations/tibber/\r\n[togrill docs]: https://www.home-assistant.io/integrations/togrill/\r\n[vesync docs]: https://www.home-assistant.io/integrations/vesync/\r\n[workday docs]: https://www.home-assistant.io/integrations/workday/\r\n[zha docs]: https://www.home-assistant.io/integrations/zha/\r\n[zwave_js docs]: https://www.home-assistant.io/integrations/zwave_js/",
        "published_at": "2025-10-03T18:10:59Z",
        "target_commitish": "master",
        "version_tuple": [
          2025,
          10,
          1
        ],
        "version_key": "2025.10.1"
      }
    ],
    "readme_content": "Home Assistant |Chat Status|\n=================================================================================\n\nOpen source home automation that puts local control and privacy first. Powered by a worldwide community of tinkerers and DIY enthusiasts. Perfect to run on a Raspberry Pi or a local server.\n\nCheck out `home-assistant.io <https://home-assistant.io>`__ for `a\ndemo <https://demo.home-assistant.io>`__, `installation instructions <https://home-assistant.io/getting-started/>`__,\n`tutorials <https://home-assistant.io/getting-started/automation/>`__ and `documentation <https://home-assistant.io/docs/>`__.\n\n|screenshot-states|\n\nFeatured integrations\n---------------------\n\n|screenshot-integrations|\n\nThe system is built using a modular approach so support for other devices or actions can be implemented easily. See also the `section on architecture <https://developers.home-assistant.io/docs/architecture_index/>`__ and the `section on creating your own\ncomponents <https://developers.home-assistant.io/docs/creating_component_index/>`__.\n\nIf you run into issues while using Home Assistant or during development\nof a component, check the `Home Assistant help section <https://home-assistant.io/help/>`__ of our website for further help and information.\n\n|ohf-logo|\n\n.. |Chat Status| image:: https://img.shields.io/discord/330944238910963714.svg\n   :target: https://www.home-assistant.io/join-chat/\n.. |screenshot-states| image:: https://raw.githubusercontent.com/home-assistant/core/dev/.github/assets/screenshot-states.png\n   :target: https://demo.home-assistant.io\n.. |screenshot-integrations| image:: https://raw.githubusercontent.com/home-assistant/core/dev/.github/assets/screenshot-integrations.png\n   :target: https://home-assistant.io/integrations/\n.. |ohf-logo| image:: https://www.openhomefoundation.org/badges/home-assistant.png\n   :alt: Home Assistant - A project from the Open Home Foundation\n   :target: https://www.openhomefoundation.org/\n",
    "ci_configs": {
      ".github/workflows/builder.yml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/builder.yml",
      ".github/workflows/ci.yaml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/ci.yaml",
      ".github/workflows/codeql.yml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/codeql.yml",
      ".github/workflows/detect-duplicate-issues.yml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/detect-duplicate-issues.yml",
      ".github/workflows/detect-non-english-issues.yml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/detect-non-english-issues.yml",
      ".github/workflows/lock.yml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/lock.yml",
      ".github/workflows/restrict-task-creation.yml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/restrict-task-creation.yml",
      ".github/workflows/stale.yml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/stale.yml",
      ".github/workflows/translations.yml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/translations.yml",
      ".github/workflows/wheels.yml": "https://raw.githubusercontent.com/home-assistant/core/dev/.github/workflows/wheels.yml"
    },
    "processed_at": "2025-11-06 00:33:03"
  }
}